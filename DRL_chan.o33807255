IP Head: 10.19.5.10:8579
Starting HEAD at sh03-05n10
Starting WORKER 1 at sh03-05n12
Starting WORKER 2 at sh03-05n15
Starting WORKER 3 at sh03-05n17
Starting WORKER 4 at sh03-05n30
Starting WORKER 5 at sh03-05n54
Starting WORKER 6 at sh03-05n57
== Status ==
Memory usage on this node: 8.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 PENDING)
+------------------------------+----------+-------+
| Trial name                   | status   | loc   |
|------------------------------+----------+-------|
| PPO_ReservoirEnv_a1429_00000 | PENDING  |       |
+------------------------------+----------+-------+


== Status ==
Memory usage on this node: 8.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 PENDING)
+------------------------------+----------+-------+
| Trial name                   | status   | loc   |
|------------------------------+----------+-------|
| PPO_ReservoirEnv_a1429_00000 | PENDING  |       |
+------------------------------+----------+-------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 1890
  custom_metrics: {}
  date: 2021-09-18_17-26-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.375449816168958
  episode_reward_mean: 4.325423060028553
  episode_reward_min: 1.044484050646622
  episodes_this_iter: 270
  episodes_total: 270
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 0.20000000298023224
          cur_lr: 9.999999747378752e-05
          entropy: 12.973909378051758
          entropy_coeff: 0.0005000000237487257
          kl: 0.10269016027450562
          model: {}
          policy_loss: -0.041490547358989716
          total_loss: 0.1828366219997406
          vf_explained_var: 0.9288635849952698
          vf_loss: 0.21027608215808868
    num_agent_steps_sampled: 1890
    num_agent_steps_trained: 1890
    num_steps_sampled: 1890
    num_steps_trained: 1890
  iterations_since_restore: 1
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.21174270755422
    ram_util_percent: 5.697980553477937
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11308622948917342
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4111.462900491409
    mean_inference_ms: 21.3783288885046
    mean_raw_obs_processing_ms: 674.8680825881016
  time_since_restore: 1908.7739658355713
  time_this_iter_s: 1908.7739658355713
  time_total_s: 1908.7739658355713
  timers:
    learn_throughput: 509.429
    learn_time_ms: 3710.037
    load_throughput: 10954.833
    load_time_ms: 172.527
    sample_throughput: 0.992
    sample_time_ms: 1904439.039
    update_time_ms: 30.005
  timestamp: 1632011203
  timesteps_since_restore: 0
  timesteps_total: 1890
  training_iteration: 1
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |      1 |          1908.77 | 1890 |  4.32542 |              7.37545 |              1.04448 |                  7 |
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 3780
  custom_metrics: {}
  date: 2021-09-18_17-51-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.643730889729045
  episode_reward_mean: 4.406116993315637
  episode_reward_min: 0.9537463176888954
  episodes_this_iter: 270
  episodes_total: 540
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 0.30000001192092896
          cur_lr: 9.981000039260834e-05
          entropy: 12.90930461883545
          entropy_coeff: 0.000499000190757215
          kl: 0.025934454053640366
          model: {}
          policy_loss: -0.09652598202228546
          total_loss: 0.07403367012739182
          vf_explained_var: 0.9451296925544739
          vf_loss: 0.16922105848789215
    num_agent_steps_sampled: 3780
    num_agent_steps_trained: 3780
    num_steps_sampled: 3780
    num_steps_trained: 3780
  iterations_since_restore: 2
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.10550950755729
    ram_util_percent: 5.625646026328622
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10877562969884957
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4120.561991493059
    mean_inference_ms: 12.21713727461424
    mean_raw_obs_processing_ms: 478.7766773277469
  time_since_restore: 3375.5632066726685
  time_this_iter_s: 1466.7892408370972
  time_total_s: 3375.5632066726685
  timers:
    learn_throughput: 596.726
    learn_time_ms: 3167.285
    load_throughput: 20637.471
    load_time_ms: 91.581
    sample_throughput: 1.122
    sample_time_ms: 1684216.471
    update_time_ms: 31.301
  timestamp: 1632012670
  timesteps_since_restore: 0
  timesteps_total: 3780
  training_iteration: 2
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |      2 |          3375.56 | 3780 |  4.40612 |              7.64373 |             0.953746 |                  7 |
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 5670
  custom_metrics: {}
  date: 2021-09-18_18-05-29
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.466168576646159
  episode_reward_mean: 4.399128642642114
  episode_reward_min: 1.5401396206256466
  episodes_this_iter: 270
  episodes_total: 810
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 0.44999998807907104
          cur_lr: 9.962000331142917e-05
          entropy: 12.960113525390625
          entropy_coeff: 0.0004980004159733653
          kl: 0.028500869870185852
          model: {}
          policy_loss: -0.08898622542619705
          total_loss: 0.050864219665527344
          vf_explained_var: 0.9561024904251099
          vf_loss: 0.13347919285297394
    num_agent_steps_sampled: 5670
    num_agent_steps_trained: 5670
    num_steps_sampled: 5670
    num_steps_trained: 5670
  iterations_since_restore: 3
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.195
    ram_util_percent: 5.700000000000002
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10650143553530933
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4041.952790523992
    mean_inference_ms: 8.9521220796176
    mean_raw_obs_processing_ms: 839.9728662489202
  time_since_restore: 4234.82857465744
  time_this_iter_s: 859.2653679847717
  time_total_s: 4234.82857465744
  timers:
    learn_throughput: 632.723
    learn_time_ms: 2987.087
    load_throughput: 29239.991
    load_time_ms: 64.638
    sample_throughput: 1.342
    sample_time_ms: 1408302.077
    update_time_ms: 29.961
  timestamp: 1632013529
  timesteps_since_restore: 0
  timesteps_total: 5670
  training_iteration: 3
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |      3 |          4234.83 | 5670 |  4.39913 |              7.46617 |              1.54014 |                  7 |
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 7560
  custom_metrics: {}
  date: 2021-09-18_18-24-35
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.388504149143735
  episode_reward_mean: 4.330282948590868
  episode_reward_min: 0.8944232771908379
  episodes_this_iter: 270
  episodes_total: 1080
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 0.675000011920929
          cur_lr: 9.942999895429239e-05
          entropy: 13.062110900878906
          entropy_coeff: 0.0004970005829818547
          kl: 0.025842200964689255
          model: {}
          policy_loss: -0.13502837717533112
          total_loss: -0.006650306284427643
          vf_explained_var: 0.9590896368026733
          vf_loss: 0.11742645502090454
    num_agent_steps_sampled: 7560
    num_agent_steps_trained: 7560
    num_steps_sampled: 7560
    num_steps_trained: 7560
  iterations_since_restore: 4
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.1779375
    ram_util_percent: 5.700000000000002
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10526020701943163
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4029.067379999192
    mean_inference_ms: 7.3083744953792555
    mean_raw_obs_processing_ms: 800.2936031076333
  time_since_restore: 5380.529753684998
  time_this_iter_s: 1145.7011790275574
  time_total_s: 5380.529753684998
  timers:
    learn_throughput: 651.483
    learn_time_ms: 2901.072
    load_throughput: 37004.375
    load_time_ms: 51.075
    sample_throughput: 1.408
    sample_time_ms: 1341949.36
    update_time_ms: 29.435
  timestamp: 1632014675
  timesteps_since_restore: 0
  timesteps_total: 7560
  training_iteration: 4
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |      4 |          5380.53 | 7560 |  4.33028 |               7.3885 |             0.894423 |                  7 |
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 9450
  custom_metrics: {}
  date: 2021-09-18_18-56-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.587978077652378
  episode_reward_mean: 4.381517134253265
  episode_reward_min: 1.6334726716495456
  episodes_this_iter: 270
  episodes_total: 1350
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.0125000476837158
          cur_lr: 9.924000187311321e-05
          entropy: 13.096328735351562
          entropy_coeff: 0.000496000808198005
          kl: 0.019554689526557922
          model: {}
          policy_loss: -0.11758042871952057
          total_loss: -0.00329960766248405
          vf_explained_var: 0.965944230556488
          vf_loss: 0.10097750276327133
    num_agent_steps_sampled: 9450
    num_agent_steps_trained: 9450
    num_steps_sampled: 9450
    num_steps_trained: 9450
  iterations_since_restore: 5
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.35011086474501
    ram_util_percent: 5.761492978566149
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1040149244966144
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4344.45413520825
    mean_inference_ms: 6.293645350337339
    mean_raw_obs_processing_ms: 771.1867165304313
  time_since_restore: 7317.850120306015
  time_this_iter_s: 1937.3203666210175
  time_total_s: 7317.850120306015
  timers:
    learn_throughput: 661.079
    learn_time_ms: 2858.962
    load_throughput: 43982.428
    load_time_ms: 42.972
    sample_throughput: 1.294
    sample_time_ms: 1460450.795
    update_time_ms: 29.43
  timestamp: 1632016612
  timesteps_since_restore: 0
  timesteps_total: 9450
  training_iteration: 5
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |      5 |          7317.85 | 9450 |  4.38152 |              7.58798 |              1.63347 |                  7 |
+------------------------------+----------+------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 11340
  custom_metrics: {}
  date: 2021-09-18_19-23-21
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.567611774415293
  episode_reward_mean: 4.434024065230546
  episode_reward_min: 1.816236694626016
  episodes_this_iter: 270
  episodes_total: 1620
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.0125000476837158
          cur_lr: 9.904999751597643e-05
          entropy: 13.13963508605957
          entropy_coeff: 0.0004950009752064943
          kl: 0.022779900580644608
          model: {}
          policy_loss: -0.1390969455242157
          total_loss: -0.029093917459249496
          vf_explained_var: 0.9697039723396301
          vf_loss: 0.09344251453876495
    num_agent_steps_sampled: 11340
    num_agent_steps_trained: 11340
    num_steps_sampled: 11340
    num_steps_trained: 11340
  iterations_since_restore: 6
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.955605583070685
    ram_util_percent: 5.779108509680324
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10359425430464071
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4192.648464610115
    mean_inference_ms: 5.622591504863665
    mean_raw_obs_processing_ms: 678.2896439546074
  time_since_restore: 8906.381916284561
  time_this_iter_s: 1588.5317959785461
  time_total_s: 8906.381916284561
  timers:
    learn_throughput: 667.844
    learn_time_ms: 2830.002
    load_throughput: 50408.995
    load_time_ms: 37.493
    sample_throughput: 1.276
    sample_time_ms: 1481322.492
    update_time_ms: 29.217
  timestamp: 1632018201
  timesteps_since_restore: 0
  timesteps_total: 11340
  training_iteration: 6
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |      6 |          8906.38 | 11340 |  4.43402 |              7.56761 |              1.81624 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 13230
  custom_metrics: {}
  date: 2021-09-18_19-40-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.668554242015901
  episode_reward_mean: 4.383192265902584
  episode_reward_min: 1.6768384361012567
  episodes_this_iter: 270
  episodes_total: 1890
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 9.886000043479726e-05
          entropy: 13.089826583862305
          entropy_coeff: 0.0004940012004226446
          kl: 0.019496509805321693
          model: {}
          policy_loss: -0.1180933341383934
          total_loss: -0.002938502700999379
          vf_explained_var: 0.9684848785400391
          vf_loss: 0.09201090037822723
    num_agent_steps_sampled: 13230
    num_agent_steps_trained: 13230
    num_steps_sampled: 13230
    num_steps_trained: 13230
  iterations_since_restore: 7
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.14104323953329
    ram_util_percent: 5.746602608098833
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10329975066807578
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4280.890493574902
    mean_inference_ms: 5.141586121039314
    mean_raw_obs_processing_ms: 611.9439844687333
  time_since_restore: 9948.93619465828
  time_this_iter_s: 1042.5542783737183
  time_total_s: 9948.93619465828
  timers:
    learn_throughput: 674.172
    learn_time_ms: 2803.437
    load_throughput: 55995.667
    load_time_ms: 33.753
    sample_throughput: 1.333
    sample_time_ms: 1418237.649
    update_time_ms: 30.764
  timestamp: 1632019243
  timesteps_since_restore: 0
  timesteps_total: 13230
  training_iteration: 7
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |      7 |          9948.94 | 13230 |  4.38319 |              7.66855 |              1.67684 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 15120
  custom_metrics: {}
  date: 2021-09-18_20-06-02
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.809692674290245
  episode_reward_mean: 4.430503305445199
  episode_reward_min: 1.8491521326373954
  episodes_this_iter: 270
  episodes_total: 2160
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 9.867000335361809e-05
          entropy: 13.08138370513916
          entropy_coeff: 0.0004930014256387949
          kl: 0.018102023750543594
          model: {}
          policy_loss: -0.13598431646823883
          total_loss: -0.03699829801917076
          vf_explained_var: 0.9734496474266052
          vf_loss: 0.07794272154569626
    num_agent_steps_sampled: 15120
    num_agent_steps_trained: 15120
    num_steps_sampled: 15120
    num_steps_trained: 15120
  iterations_since_restore: 8
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.69193396226415
    ram_util_percent: 5.728254716981131
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10297075719842749
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4213.1819295242385
    mean_inference_ms: 4.77814669688777
    mean_raw_obs_processing_ms: 561.597724009326
  time_since_restore: 11467.108310699463
  time_this_iter_s: 1518.1721160411835
  time_total_s: 11467.108310699463
  timers:
    learn_throughput: 678.297
    learn_time_ms: 2786.389
    load_throughput: 61347.402
    load_time_ms: 30.808
    sample_throughput: 1.321
    sample_time_ms: 1430375.084
    update_time_ms: 30.525
  timestamp: 1632020762
  timesteps_since_restore: 0
  timesteps_total: 15120
  training_iteration: 8
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |      8 |          11467.1 | 15120 |   4.4305 |              7.80969 |              1.84915 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 17010
  custom_metrics: {}
  date: 2021-09-18_20-16-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.725966768084025
  episode_reward_mean: 4.527929382457697
  episode_reward_min: 1.8575902000495204
  episodes_this_iter: 270
  episodes_total: 2430
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 9.84799989964813e-05
          entropy: 13.09325122833252
          entropy_coeff: 0.0004920015926472843
          kl: 0.01963694952428341
          model: {}
          policy_loss: -0.1284562647342682
          total_loss: -0.01848040334880352
          vf_explained_var: 0.9725998044013977
          vf_loss: 0.08659414947032928
    num_agent_steps_sampled: 17010
    num_agent_steps_trained: 17010
    num_steps_sampled: 17010
    num_steps_trained: 17010
  iterations_since_restore: 9
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.279405034324945
    ram_util_percent: 5.685469107551487
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10268460838604891
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4016.8800496123354
    mean_inference_ms: 4.495585531422194
    mean_raw_obs_processing_ms: 522.3639637292665
  time_since_restore: 12092.832514047623
  time_this_iter_s: 625.7242033481598
  time_total_s: 12092.832514047623
  timers:
    learn_throughput: 680.636
    learn_time_ms: 2776.813
    load_throughput: 66219.521
    load_time_ms: 28.541
    sample_throughput: 1.41
    sample_time_ms: 1340650.694
    update_time_ms: 30.273
  timestamp: 1632021387
  timesteps_since_restore: 0
  timesteps_total: 17010
  training_iteration: 9
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |      9 |          12092.8 | 17010 |  4.52793 |              7.72597 |              1.85759 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 18900
  custom_metrics: {}
  date: 2021-09-18_20-42-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.125037396633986
  episode_reward_mean: 4.3811426412145
  episode_reward_min: 1.3854203846567403
  episodes_this_iter: 270
  episodes_total: 2700
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 9.829000191530213e-05
          entropy: 13.163680076599121
          entropy_coeff: 0.0004910018178634346
          kl: 0.01932710036635399
          model: {}
          policy_loss: -0.14157931506633759
          total_loss: -0.051264334470033646
          vf_explained_var: 0.977263867855072
          vf_loss: 0.06742534786462784
    num_agent_steps_sampled: 18900
    num_agent_steps_trained: 18900
    num_steps_sampled: 18900
    num_steps_trained: 18900
  iterations_since_restore: 10
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.753200181570595
    ram_util_percent: 5.542623694961417
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10256421575403475
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4012.8972147091617
    mean_inference_ms: 4.269698125219483
    mean_raw_obs_processing_ms: 490.86021882265857
  time_since_restore: 13671.743948936462
  time_this_iter_s: 1578.9114348888397
  time_total_s: 13671.743948936462
  timers:
    learn_throughput: 683.729
    learn_time_ms: 2764.252
    load_throughput: 70553.989
    load_time_ms: 26.788
    sample_throughput: 1.385
    sample_time_ms: 1364194.753
    update_time_ms: 29.984
  timestamp: 1632022966
  timesteps_since_restore: 0
  timesteps_total: 18900
  training_iteration: 10
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     10 |          13671.7 | 18900 |  4.38114 |              7.12504 |              1.38542 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 20790
  custom_metrics: {}
  date: 2021-09-18_21-04-32
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.165819173436866
  episode_reward_mean: 4.393465662704805
  episode_reward_min: 1.0351939959245111
  episodes_this_iter: 270
  episodes_total: 2970
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 9.809999755816534e-05
          entropy: 13.139244079589844
          entropy_coeff: 0.0004900019848719239
          kl: 0.020118271932005882
          model: {}
          policy_loss: -0.14339987933635712
          total_loss: -0.05357806757092476
          vf_explained_var: 0.9776710271835327
          vf_loss: 0.06570545583963394
    num_agent_steps_sampled: 20790
    num_agent_steps_trained: 20790
    num_steps_sampled: 20790
    num_steps_trained: 20790
  iterations_since_restore: 11
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.95987925356751
    ram_util_percent: 5.697310647639956
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10229089069252516
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3982.0103588354914
    mean_inference_ms: 4.093960804181572
    mean_raw_obs_processing_ms: 464.9237712053415
  time_since_restore: 14976.665551185608
  time_this_iter_s: 1304.9216022491455
  time_total_s: 14976.665551185608
  timers:
    learn_throughput: 711.018
    learn_time_ms: 2658.162
    load_throughput: 178604.877
    load_time_ms: 10.582
    sample_throughput: 1.449
    sample_time_ms: 1303961.236
    update_time_ms: 29.841
  timestamp: 1632024272
  timesteps_since_restore: 0
  timesteps_total: 20790
  training_iteration: 11
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     11 |          14976.7 | 20790 |  4.39347 |              7.16582 |              1.03519 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 22680
  custom_metrics: {}
  date: 2021-09-18_21-27-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.589986335471992
  episode_reward_mean: 4.46099159645416
  episode_reward_min: 1.8479726103392038
  episodes_this_iter: 270
  episodes_total: 3240
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.791000047698617e-05
          entropy: 13.153157234191895
          entropy_coeff: 0.0004890022100880742
          kl: 0.015335164032876492
          model: {}
          policy_loss: -0.16442111134529114
          total_loss: -0.07695509493350983
          vf_explained_var: 0.9805058836936951
          vf_loss: 0.058962512761354446
    num_agent_steps_sampled: 22680
    num_agent_steps_trained: 22680
    num_steps_sampled: 22680
    num_steps_trained: 22680
  iterations_since_restore: 12
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.64699228791774
    ram_util_percent: 5.610591259640103
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10212216491164175
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3968.06394661463
    mean_inference_ms: 3.9459754572660803
    mean_raw_obs_processing_ms: 443.09975498474506
  time_since_restore: 16370.597654342651
  time_this_iter_s: 1393.9321031570435
  time_total_s: 16370.597654342651
  timers:
    learn_throughput: 710.873
    learn_time_ms: 2658.703
    load_throughput: 178728.501
    load_time_ms: 10.575
    sample_throughput: 1.458
    sample_time_ms: 1296675.372
    update_time_ms: 29.621
  timestamp: 1632025665
  timesteps_since_restore: 0
  timesteps_total: 22680
  training_iteration: 12
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     12 |          16370.6 | 22680 |  4.46099 |              7.58999 |              1.84797 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 24570
  custom_metrics: {}
  date: 2021-09-18_21-38-51
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.652681491446018
  episode_reward_mean: 4.479272087480208
  episode_reward_min: 1.0877970485026034
  episodes_this_iter: 270
  episodes_total: 3510
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.7720003395807e-05
          entropy: 13.136894226074219
          entropy_coeff: 0.00048800240620039403
          kl: 0.016063755378127098
          model: {}
          policy_loss: -0.16196461021900177
          total_loss: -0.07696937024593353
          vf_explained_var: 0.9813657402992249
          vf_loss: 0.05481084808707237
    num_agent_steps_sampled: 24570
    num_agent_steps_trained: 24570
    num_steps_sampled: 24570
    num_steps_trained: 24570
  iterations_since_restore: 13
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.207650862068974
    ram_util_percent: 5.632435344827588
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10224762920232178
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3870.684460728932
    mean_inference_ms: 3.8250995632898133
    mean_raw_obs_processing_ms: 425.0070814129988
  time_since_restore: 17035.721345186234
  time_this_iter_s: 665.1236908435822
  time_total_s: 17035.721345186234
  timers:
    learn_throughput: 709.759
    learn_time_ms: 2662.875
    load_throughput: 179119.423
    load_time_ms: 10.552
    sample_throughput: 1.48
    sample_time_ms: 1277256.558
    update_time_ms: 29.852
  timestamp: 1632026331
  timesteps_since_restore: 0
  timesteps_total: 24570
  training_iteration: 13
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     13 |          17035.7 | 24570 |  4.47927 |              7.65268 |               1.0878 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 26460
  custom_metrics: {}
  date: 2021-09-18_22-51-28
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.671267382492384
  episode_reward_mean: 4.490860378685575
  episode_reward_min: 1.8321654722387377
  episodes_this_iter: 270
  episodes_total: 3780
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.752999903867021e-05
          entropy: 13.143017768859863
          entropy_coeff: 0.00048700260231271386
          kl: 0.01489044725894928
          model: {}
          policy_loss: -0.17319844663143158
          total_loss: -0.09626336395740509
          vf_explained_var: 0.9834766983985901
          vf_loss: 0.04941347986459732
    num_agent_steps_sampled: 26460
    num_agent_steps_trained: 26460
    num_steps_sampled: 26460
    num_steps_trained: 26460
  iterations_since_restore: 14
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.41974679381783
    ram_util_percent: 5.586402499177903
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10230391965501402
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4169.040618684198
    mean_inference_ms: 3.7173008241920593
    mean_raw_obs_processing_ms: 409.42768275143845
  time_since_restore: 21392.98498415947
  time_this_iter_s: 4357.263638973236
  time_total_s: 21392.98498415947
  timers:
    learn_throughput: 710.236
    learn_time_ms: 2661.088
    load_throughput: 178569.07
    load_time_ms: 10.584
    sample_throughput: 1.182
    sample_time_ms: 1598413.912
    update_time_ms: 30.057
  timestamp: 1632030688
  timesteps_since_restore: 0
  timesteps_total: 26460
  training_iteration: 14
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     14 |            21393 | 26460 |  4.49086 |              7.67127 |              1.83217 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 28350
  custom_metrics: {}
  date: 2021-09-18_23-17-11
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.625468255218463
  episode_reward_mean: 4.514472273413563
  episode_reward_min: 1.793574931005294
  episodes_this_iter: 270
  episodes_total: 4050
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.734000195749104e-05
          entropy: 13.11373233795166
          entropy_coeff: 0.0004860027984250337
          kl: 0.015413953922688961
          model: {}
          policy_loss: -0.1422284096479416
          total_loss: -0.06528132408857346
          vf_explained_var: 0.9836698770523071
          vf_loss: 0.04820548743009567
    num_agent_steps_sampled: 28350
    num_agent_steps_trained: 28350
    num_steps_sampled: 28350
    num_steps_trained: 28350
  iterations_since_restore: 15
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 5.8503258845437625
    ram_util_percent: 4.90754189944134
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10230823715716177
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4163.176794941014
    mean_inference_ms: 3.6280820845553228
    mean_raw_obs_processing_ms: 396.0838974160397
  time_since_restore: 22935.528041362762
  time_this_iter_s: 1542.5430572032928
  time_total_s: 22935.528041362762
  timers:
    learn_throughput: 712.834
    learn_time_ms: 2651.389
    load_throughput: 178612.523
    load_time_ms: 10.582
    sample_throughput: 1.212
    sample_time_ms: 1558945.797
    update_time_ms: 29.993
  timestamp: 1632032231
  timesteps_since_restore: 0
  timesteps_total: 28350
  training_iteration: 15
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     15 |          22935.5 | 28350 |  4.51447 |              7.62547 |              1.79357 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 30240
  custom_metrics: {}
  date: 2021-09-18_23-48-56
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.541027610060892
  episode_reward_mean: 4.486545856814963
  episode_reward_min: 1.5597395405165684
  episodes_this_iter: 270
  episodes_total: 4320
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.714999760035425e-05
          entropy: 13.080103874206543
          entropy_coeff: 0.0004850029945373535
          kl: 0.01608314923942089
          model: {}
          policy_loss: -0.16074833273887634
          total_loss: -0.08006700128316879
          vf_explained_var: 0.9834800362586975
          vf_loss: 0.050385795533657074
    num_agent_steps_sampled: 30240
    num_agent_steps_trained: 30240
    num_steps_sampled: 30240
    num_steps_trained: 30240
  iterations_since_restore: 16
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.033571966842502
    ram_util_percent: 4.80101733232856
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.102356530020757
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4132.04757724966
    mean_inference_ms: 3.553372363494747
    mean_raw_obs_processing_ms: 384.68514944790815
  time_since_restore: 24841.2515437603
  time_this_iter_s: 1905.7235023975372
  time_total_s: 24841.2515437603
  timers:
    learn_throughput: 716.353
    learn_time_ms: 2638.364
    load_throughput: 176767.276
    load_time_ms: 10.692
    sample_throughput: 1.188
    sample_time_ms: 1590677.203
    update_time_ms: 30.449
  timestamp: 1632034136
  timesteps_since_restore: 0
  timesteps_total: 30240
  training_iteration: 16
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     16 |          24841.3 | 30240 |  4.48655 |              7.54103 |              1.55974 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 32130
  custom_metrics: {}
  date: 2021-09-19_00-03-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.406274549601819
  episode_reward_mean: 4.474701848208693
  episode_reward_min: 1.7122788572025454
  episodes_this_iter: 270
  episodes_total: 4590
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.696000051917508e-05
          entropy: 13.101839065551758
          entropy_coeff: 0.00048400319064967334
          kl: 0.016873763874173164
          model: {}
          policy_loss: -0.16824550926685333
          total_loss: -0.07446056604385376
          vf_explained_var: 0.9805823564529419
          vf_loss: 0.06168573349714279
    num_agent_steps_sampled: 32130
    num_agent_steps_trained: 32130
    num_steps_sampled: 32130
    num_steps_trained: 32130
  iterations_since_restore: 17
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.9398178807947017
    ram_util_percent: 4.800827814569536
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10214942602266662
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4079.828101270018
    mean_inference_ms: 3.4830539628884116
    mean_raw_obs_processing_ms: 374.25698776605714
  time_since_restore: 25708.758852481842
  time_this_iter_s: 867.5073087215424
  time_total_s: 25708.758852481842
  timers:
    learn_throughput: 718.474
    learn_time_ms: 2630.574
    load_throughput: 176447.397
    load_time_ms: 10.711
    sample_throughput: 1.201
    sample_time_ms: 1573181.287
    update_time_ms: 29.273
  timestamp: 1632035004
  timesteps_since_restore: 0
  timesteps_total: 32130
  training_iteration: 17
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     17 |          25708.8 | 32130 |   4.4747 |              7.40627 |              1.71228 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 34020
  custom_metrics: {}
  date: 2021-09-19_00-22-14
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.736735148539017
  episode_reward_mean: 4.5634694475434285
  episode_reward_min: 1.8440315392848259
  episodes_this_iter: 270
  episodes_total: 4860
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.677000343799591e-05
          entropy: 13.039473533630371
          entropy_coeff: 0.00048300338676199317
          kl: 0.015264056622982025
          model: {}
          policy_loss: -0.15431520342826843
          total_loss: -0.07329922914505005
          vf_explained_var: 0.9827775955200195
          vf_loss: 0.05254065617918968
    num_agent_steps_sampled: 34020
    num_agent_steps_trained: 34020
    num_steps_sampled: 34020
    num_steps_trained: 34020
  iterations_since_restore: 18
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.5105530832803558
    ram_util_percent: 4.801271455816909
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10198131868979221
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3979.570662896781
    mean_inference_ms: 3.4212808241835098
    mean_raw_obs_processing_ms: 364.9902004288852
  time_since_restore: 26838.497749567032
  time_this_iter_s: 1129.7388970851898
  time_total_s: 26838.497749567032
  timers:
    learn_throughput: 720.949
    learn_time_ms: 2621.544
    load_throughput: 175627.251
    load_time_ms: 10.761
    sample_throughput: 1.232
    sample_time_ms: 1534346.359
    update_time_ms: 29.434
  timestamp: 1632036134
  timesteps_since_restore: 0
  timesteps_total: 34020
  training_iteration: 18
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     18 |          26838.5 | 34020 |  4.56347 |              7.73674 |              1.84403 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 35910
  custom_metrics: {}
  date: 2021-09-19_00-43-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.515951579091744
  episode_reward_mean: 4.527011787613152
  episode_reward_min: 1.681566515700744
  episodes_this_iter: 270
  episodes_total: 5130
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.657999908085912e-05
          entropy: 13.027313232421875
          entropy_coeff: 0.00048200361197814345
          kl: 0.014173617586493492
          model: {}
          policy_loss: -0.15481172502040863
          total_loss: -0.08593256026506424
          vf_explained_var: 0.9860357046127319
          vf_loss: 0.04286911338567734
    num_agent_steps_sampled: 35910
    num_agent_steps_trained: 35910
    num_steps_sampled: 35910
    num_steps_trained: 35910
  iterations_since_restore: 19
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 2.209236725663717
    ram_util_percent: 4.801050884955751
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1019083555668431
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4010.1503259151077
    mean_inference_ms: 3.365648568362795
    mean_raw_obs_processing_ms: 356.8192514074809
  time_since_restore: 28137.07140493393
  time_this_iter_s: 1298.5736553668976
  time_total_s: 28137.07140493393
  timers:
    learn_throughput: 724.54
    learn_time_ms: 2608.551
    load_throughput: 175435.246
    load_time_ms: 10.773
    sample_throughput: 1.18
    sample_time_ms: 1601644.11
    update_time_ms: 29.571
  timestamp: 1632037432
  timesteps_since_restore: 0
  timesteps_total: 35910
  training_iteration: 19
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     19 |          28137.1 | 35910 |  4.52701 |              7.51595 |              1.68157 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 37800
  custom_metrics: {}
  date: 2021-09-19_01-01-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.7253696110086105
  episode_reward_mean: 4.48409120660408
  episode_reward_min: 1.4411957495416994
  episodes_this_iter: 270
  episodes_total: 5400
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.639000199967995e-05
          entropy: 13.066930770874023
          entropy_coeff: 0.0004810038080904633
          kl: 0.016270780935883522
          model: {}
          policy_loss: -0.16660340130329132
          total_loss: -0.09106368571519852
          vf_explained_var: 0.9848304986953735
          vf_loss: 0.04475807771086693
    num_agent_steps_sampled: 37800
    num_agent_steps_trained: 37800
    num_steps_sampled: 37800
    num_steps_trained: 37800
  iterations_since_restore: 20
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.55191661062542
    ram_util_percent: 4.801210490921317
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10212738240238108
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3966.9366400311296
    mean_inference_ms: 3.320444558815977
    mean_raw_obs_processing_ms: 349.40996234778663
  time_since_restore: 29207.12651324272
  time_this_iter_s: 1070.0551083087921
  time_total_s: 29207.12651324272
  timers:
    learn_throughput: 727.277
    learn_time_ms: 2598.735
    load_throughput: 176543.278
    load_time_ms: 10.706
    sample_throughput: 1.219
    sample_time_ms: 1550767.813
    update_time_ms: 30.497
  timestamp: 1632038502
  timesteps_since_restore: 0
  timesteps_total: 37800
  training_iteration: 20
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     20 |          29207.1 | 37800 |  4.48409 |              7.72537 |               1.4412 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 39690
  custom_metrics: {}
  date: 2021-09-19_01-19-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.570133641681512
  episode_reward_mean: 4.591777807021478
  episode_reward_min: 1.7742647648156666
  episodes_this_iter: 270
  episodes_total: 5670
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.619999764254317e-05
          entropy: 13.005393981933594
          entropy_coeff: 0.0004800040042027831
          kl: 0.01598089188337326
          model: {}
          policy_loss: -0.1490112990140915
          total_loss: -0.07275526970624924
          vf_explained_var: 0.9857034683227539
          vf_loss: 0.046092208474874496
    num_agent_steps_sampled: 39690
    num_agent_steps_trained: 39690
    num_steps_sampled: 39690
    num_steps_trained: 39690
  iterations_since_restore: 21
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.1409333333333334
    ram_util_percent: 4.801599999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10208283739874217
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3907.8376271630414
    mean_inference_ms: 3.2739695137727955
    mean_raw_obs_processing_ms: 342.7226874189263
  time_since_restore: 30287.217089176178
  time_this_iter_s: 1080.0905759334564
  time_total_s: 30287.217089176178
  timers:
    learn_throughput: 728.934
    learn_time_ms: 2592.826
    load_throughput: 176377.909
    load_time_ms: 10.716
    sample_throughput: 1.237
    sample_time_ms: 1528289.798
    update_time_ms: 31.02
  timestamp: 1632039583
  timesteps_since_restore: 0
  timesteps_total: 39690
  training_iteration: 21
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     21 |          30287.2 | 39690 |  4.59178 |              7.57013 |              1.77426 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 41580
  custom_metrics: {}
  date: 2021-09-19_01-34-39
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.775499306965626
  episode_reward_mean: 4.528168864519496
  episode_reward_min: 0.8390752169384021
  episodes_this_iter: 270
  episodes_total: 5940
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.6010000561364e-05
          entropy: 12.981271743774414
          entropy_coeff: 0.00047900420031510293
          kl: 0.015012712217867374
          model: {}
          policy_loss: -0.15399381518363953
          total_loss: -0.08061723411083221
          vf_explained_var: 0.9849743247032166
          vf_loss: 0.04539385065436363
    num_agent_steps_sampled: 41580
    num_agent_steps_trained: 41580
    num_steps_sampled: 41580
    num_steps_trained: 41580
  iterations_since_restore: 22
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 5.254136546184738
    ram_util_percent: 4.8001606425702805
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10217934450371065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3846.477640731064
    mean_inference_ms: 3.2345202705578084
    mean_raw_obs_processing_ms: 336.7275731277512
  time_since_restore: 31183.967776060104
  time_this_iter_s: 896.7506868839264
  time_total_s: 31183.967776060104
  timers:
    learn_throughput: 729.892
    learn_time_ms: 2589.425
    load_throughput: 173759.794
    load_time_ms: 10.877
    sample_throughput: 1.278
    sample_time_ms: 1478574.495
    update_time_ms: 30.899
  timestamp: 1632040479
  timesteps_since_restore: 0
  timesteps_total: 41580
  training_iteration: 22
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     22 |            31184 | 41580 |  4.52817 |               7.7755 |             0.839075 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 43470
  custom_metrics: {}
  date: 2021-09-19_02-13-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.5064904606033425
  episode_reward_mean: 4.50077523374716
  episode_reward_min: 1.4965162965816043
  episodes_this_iter: 270
  episodes_total: 6210
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.582000348018482e-05
          entropy: 12.95590877532959
          entropy_coeff: 0.00047800439642742276
          kl: 0.013405339792370796
          model: {}
          policy_loss: -0.18926049768924713
          total_loss: -0.12526865303516388
          vf_explained_var: 0.9872727990150452
          vf_loss: 0.039645809680223465
    num_agent_steps_sampled: 43470
    num_agent_steps_trained: 43470
    num_steps_sampled: 43470
    num_steps_trained: 43470
  iterations_since_restore: 23
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 0.9144533662754786
    ram_util_percent: 4.8125694873378615
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.102105647540806
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3906.7311037544973
    mean_inference_ms: 3.1982990077241493
    mean_raw_obs_processing_ms: 331.24045004143164
  time_since_restore: 33515.63997077942
  time_this_iter_s: 2331.6721947193146
  time_total_s: 33515.63997077942
  timers:
    learn_throughput: 731.236
    learn_time_ms: 2584.664
    load_throughput: 171521.678
    load_time_ms: 11.019
    sample_throughput: 1.149
    sample_time_ms: 1645233.743
    update_time_ms: 31.005
  timestamp: 1632042811
  timesteps_since_restore: 0
  timesteps_total: 43470
  training_iteration: 23
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     23 |          33515.6 | 43470 |  4.50078 |              7.50649 |              1.49652 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 45360
  custom_metrics: {}
  date: 2021-09-19_02-24-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.529101375871851
  episode_reward_mean: 4.609505895005403
  episode_reward_min: 2.1037494713641793
  episodes_this_iter: 270
  episodes_total: 6480
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.562999912304804e-05
          entropy: 12.915799140930176
          entropy_coeff: 0.0004770045925397426
          kl: 0.014031997881829739
          model: {}
          policy_loss: -0.1722879707813263
          total_loss: -0.11273825913667679
          vf_explained_var: 0.9889079928398132
          vf_loss: 0.0337439589202404
    num_agent_steps_sampled: 45360
    num_agent_steps_trained: 45360
    num_steps_sampled: 45360
    num_steps_trained: 45360
  iterations_since_restore: 24
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.1899249732047157
    ram_util_percent: 4.836870310825293
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10213987134763049
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3828.290314689575
    mean_inference_ms: 3.165397728939792
    mean_raw_obs_processing_ms: 326.12519504762196
  time_since_restore: 34187.77043175697
  time_this_iter_s: 672.1304609775543
  time_total_s: 34187.77043175697
  timers:
    learn_throughput: 731.376
    learn_time_ms: 2584.17
    load_throughput: 171888.387
    load_time_ms: 10.996
    sample_throughput: 1.48
    sample_time_ms: 1276721.042
    update_time_ms: 31.012
  timestamp: 1632043483
  timesteps_since_restore: 0
  timesteps_total: 45360
  training_iteration: 24
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     24 |          34187.8 | 45360 |  4.60951 |               7.5291 |              2.10375 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 47250
  custom_metrics: {}
  date: 2021-09-19_02-30-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.881184468323903
  episode_reward_mean: 4.573347831184163
  episode_reward_min: 1.8022723810330086
  episodes_this_iter: 270
  episodes_total: 6750
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.544000204186887e-05
          entropy: 12.941949844360352
          entropy_coeff: 0.0004760047886520624
          kl: 0.015209983102977276
          model: {}
          policy_loss: -0.1673220843076706
          total_loss: -0.09599768370389938
          vf_explained_var: 0.9864153861999512
          vf_loss: 0.04283461719751358
    num_agent_steps_sampled: 47250
    num_agent_steps_trained: 47250
    num_steps_sampled: 47250
    num_steps_trained: 47250
  iterations_since_restore: 25
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 5.438241308793455
    ram_util_percent: 4.804498977505112
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10202106851382738
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3762.595024961226
    mean_inference_ms: 3.134173029671871
    mean_raw_obs_processing_ms: 321.560168605924
  time_since_restore: 34540.016824007034
  time_this_iter_s: 352.24639225006104
  time_total_s: 34540.016824007034
  timers:
    learn_throughput: 731.041
    learn_time_ms: 2585.353
    load_throughput: 171670.627
    load_time_ms: 11.009
    sample_throughput: 1.633
    sample_time_ms: 1157690.353
    update_time_ms: 31.412
  timestamp: 1632043836
  timesteps_since_restore: 0
  timesteps_total: 47250
  training_iteration: 25
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     25 |            34540 | 47250 |  4.57335 |              7.88118 |              1.80227 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 49140
  custom_metrics: {}
  date: 2021-09-19_02-40-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.181657875147383
  episode_reward_mean: 4.537161838915828
  episode_reward_min: 1.799090358651889
  episodes_this_iter: 270
  episodes_total: 7020
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.524999768473208e-05
          entropy: 12.952177047729492
          entropy_coeff: 0.0004750050138682127
          kl: 0.015415256842970848
          model: {}
          policy_loss: -0.16882872581481934
          total_loss: -0.10198966413736343
          vf_explained_var: 0.9881448149681091
          vf_loss: 0.03787354379892349
    num_agent_steps_sampled: 49140
    num_agent_steps_trained: 49140
    num_steps_sampled: 49140
    num_steps_trained: 49140
  iterations_since_restore: 26
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 4.088304093567252
    ram_util_percent: 4.899999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10201764796241052
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3709.4405230672633
    mean_inference_ms: 3.1087177698531163
    mean_raw_obs_processing_ms: 317.154396919355
  time_since_restore: 35155.98452305794
  time_this_iter_s: 615.9676990509033
  time_total_s: 35155.98452305794
  timers:
    learn_throughput: 729.858
    learn_time_ms: 2589.545
    load_throughput: 172379.152
    load_time_ms: 10.964
    sample_throughput: 1.837
    sample_time_ms: 1028711.06
    update_time_ms: 31.194
  timestamp: 1632044452
  timesteps_since_restore: 0
  timesteps_total: 49140
  training_iteration: 26
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 13.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     26 |            35156 | 49140 |  4.53716 |              7.18166 |              1.79909 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 51030
  custom_metrics: {}
  date: 2021-09-19_02-51-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.462280643999909
  episode_reward_mean: 4.657117068620737
  episode_reward_min: 1.9143452667329066
  episodes_this_iter: 270
  episodes_total: 7290
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.506000060355291e-05
          entropy: 12.939499855041504
          entropy_coeff: 0.0004740052099805325
          kl: 0.015841105952858925
          model: {}
          policy_loss: -0.14877016842365265
          total_loss: -0.08291707932949066
          vf_explained_var: 0.9887354969978333
          vf_loss: 0.03589845821261406
    num_agent_steps_sampled: 51030
    num_agent_steps_trained: 51030
    num_steps_sampled: 51030
    num_steps_trained: 51030
  iterations_since_restore: 27
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 13.60394144144144
    ram_util_percent: 5.175337837837838
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10202338033135798
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3661.7249191116052
    mean_inference_ms: 3.0821325305292686
    mean_raw_obs_processing_ms: 313.29188331951434
  time_since_restore: 35795.10578370094
  time_this_iter_s: 639.1212606430054
  time_total_s: 35795.10578370094
  timers:
    learn_throughput: 725.857
    learn_time_ms: 2603.82
    load_throughput: 173031.588
    load_time_ms: 10.923
    sample_throughput: 1.879
    sample_time_ms: 1005856.72
    update_time_ms: 32.052
  timestamp: 1632045091
  timesteps_since_restore: 0
  timesteps_total: 51030
  training_iteration: 27
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     27 |          35795.1 | 51030 |  4.65712 |              7.46228 |              1.91435 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 52920
  custom_metrics: {}
  date: 2021-09-19_03-10-23
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.584502569678728
  episode_reward_mean: 4.566606422645718
  episode_reward_min: 2.0671766415187984
  episodes_this_iter: 270
  episodes_total: 7560
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.487000352237374e-05
          entropy: 12.933537483215332
          entropy_coeff: 0.00047300540609285235
          kl: 0.01533413864672184
          model: {}
          policy_loss: -0.15451237559318542
          total_loss: -0.0893564373254776
          vf_explained_var: 0.987795352935791
          vf_loss: 0.03634047880768776
    num_agent_steps_sampled: 52920
    num_agent_steps_trained: 52920
    num_steps_sampled: 52920
    num_steps_trained: 52920
  iterations_since_restore: 28
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.126683608640406
    ram_util_percent: 5.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10209418794637172
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3662.728850779283
    mean_inference_ms: 3.051080417030888
    mean_raw_obs_processing_ms: 309.4140555157354
  time_since_restore: 36927.76183652878
  time_this_iter_s: 1132.656052827835
  time_total_s: 36927.76183652878
  timers:
    learn_throughput: 721.503
    learn_time_ms: 2619.534
    load_throughput: 172446.65
    load_time_ms: 10.96
    sample_throughput: 1.878
    sample_time_ms: 1006132.867
    update_time_ms: 32.283
  timestamp: 1632046223
  timesteps_since_restore: 0
  timesteps_total: 52920
  training_iteration: 28
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     28 |          36927.8 | 52920 |  4.56661 |               7.5845 |              2.06718 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 54810
  custom_metrics: {}
  date: 2021-09-19_03-35-02
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.470177409310458
  episode_reward_mean: 4.5673637641568225
  episode_reward_min: 1.2942867662657767
  episodes_this_iter: 270
  episodes_total: 7830
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.467999916523695e-05
          entropy: 12.932708740234375
          entropy_coeff: 0.0004720056022051722
          kl: 0.015697525814175606
          model: {}
          policy_loss: -0.17353582382202148
          total_loss: -0.10634664446115494
          vf_explained_var: 0.9877690672874451
          vf_loss: 0.03753255680203438
    num_agent_steps_sampled: 54810
    num_agent_steps_trained: 54810
    num_steps_sampled: 54810
    num_steps_trained: 54810
  iterations_since_restore: 29
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.551265822784806
    ram_util_percent: 5.799999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10203506788399061
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3635.0424442747744
    mean_inference_ms: 3.0233169645572753
    mean_raw_obs_processing_ms: 305.8049764458742
  time_since_restore: 38406.04518485069
  time_this_iter_s: 1478.2833483219147
  time_total_s: 38406.04518485069
  timers:
    learn_throughput: 717.352
    learn_time_ms: 2634.691
    load_throughput: 171898.45
    load_time_ms: 10.995
    sample_throughput: 1.846
    sample_time_ms: 1024088.369
    update_time_ms: 32.838
  timestamp: 1632047702
  timesteps_since_restore: 0
  timesteps_total: 54810
  training_iteration: 29
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     29 |            38406 | 54810 |  4.56736 |              7.47018 |              1.29429 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 56700
  custom_metrics: {}
  date: 2021-09-19_03-42-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.562847922005129
  episode_reward_mean: 4.562908375300565
  episode_reward_min: 1.9777941491969735
  episodes_this_iter: 270
  episodes_total: 8100
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.449000208405778e-05
          entropy: 12.881210327148438
          entropy_coeff: 0.000471005798317492
          kl: 0.01517428271472454
          model: {}
          policy_loss: -0.17602477967739105
          total_loss: -0.11250574886798859
          vf_explained_var: 0.9883518815040588
          vf_loss: 0.035017229616642
    num_agent_steps_sampled: 56700
    num_agent_steps_trained: 56700
    num_steps_sampled: 56700
    num_steps_trained: 56700
  iterations_since_restore: 30
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.27133956386293
    ram_util_percent: 5.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10202931699184815
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3590.8557620727684
    mean_inference_ms: 2.9967505625414956
    mean_raw_obs_processing_ms: 302.35422125925663
  time_since_restore: 38868.334601163864
  time_this_iter_s: 462.2894163131714
  time_total_s: 38868.334601163864
  timers:
    learn_throughput: 713.323
    learn_time_ms: 2649.571
    load_throughput: 156811.299
    load_time_ms: 12.053
    sample_throughput: 1.962
    sample_time_ms: 963296.492
    update_time_ms: 31.948
  timestamp: 1632048164
  timesteps_since_restore: 0
  timesteps_total: 56700
  training_iteration: 30
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     30 |          38868.3 | 56700 |  4.56291 |              7.56285 |              1.97779 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 58590
  custom_metrics: {}
  date: 2021-09-19_03-56-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.9298110731187474
  episode_reward_mean: 4.617919144100677
  episode_reward_min: 1.2388916340082492
  episodes_this_iter: 270
  episodes_total: 8370
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.429999772692099e-05
          entropy: 12.910677909851074
          entropy_coeff: 0.00047000599442981184
          kl: 0.014484480954706669
          model: {}
          policy_loss: -0.17765845358371735
          total_loss: -0.1189492791891098
          vf_explained_var: 0.989767849445343
          vf_loss: 0.03177984431385994
    num_agent_steps_sampled: 58590
    num_agent_steps_trained: 58590
    num_steps_sampled: 58590
    num_steps_trained: 58590
  iterations_since_restore: 31
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.33419642857143
    ram_util_percent: 5.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10197980904284817
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3573.22243966971
    mean_inference_ms: 2.972430862127948
    mean_raw_obs_processing_ms: 299.17985249905445
  time_since_restore: 39674.408141851425
  time_this_iter_s: 806.073540687561
  time_total_s: 39674.408141851425
  timers:
    learn_throughput: 709.626
    learn_time_ms: 2663.376
    load_throughput: 156361.263
    load_time_ms: 12.087
    sample_throughput: 2.019
    sample_time_ms: 935881.809
    update_time_ms: 31.588
  timestamp: 1632048970
  timesteps_since_restore: 0
  timesteps_total: 58590
  training_iteration: 31
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     31 |          39674.4 | 58590 |  4.61792 |              7.92981 |              1.23889 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 60480
  custom_metrics: {}
  date: 2021-09-19_04-43-15
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.374770693608389
  episode_reward_mean: 4.52993021041616
  episode_reward_min: 0.7621491776350242
  episodes_this_iter: 270
  episodes_total: 8640
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.411000064574182e-05
          entropy: 12.879768371582031
          entropy_coeff: 0.00046900619054213166
          kl: 0.014709415845572948
          model: {}
          policy_loss: -0.16117675602436066
          total_loss: -0.10031548887491226
          vf_explained_var: 0.9889501333236694
          vf_loss: 0.033392053097486496
    num_agent_steps_sampled: 60480
    num_agent_steps_trained: 60480
    num_steps_sampled: 60480
    num_steps_trained: 60480
  iterations_since_restore: 32
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.56059602649007
    ram_util_percent: 5.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10194835082282822
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3633.6288034783174
    mean_inference_ms: 2.949787462582675
    mean_raw_obs_processing_ms: 296.1749146384472
  time_since_restore: 42499.23802089691
  time_this_iter_s: 2824.8298790454865
  time_total_s: 42499.23802089691
  timers:
    learn_throughput: 705.795
    learn_time_ms: 2677.832
    load_throughput: 158180.875
    load_time_ms: 11.948
    sample_throughput: 1.675
    sample_time_ms: 1128675.172
    update_time_ms: 31.578
  timestamp: 1632051795
  timesteps_since_restore: 0
  timesteps_total: 60480
  training_iteration: 32
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     32 |          42499.2 | 60480 |  4.52993 |              7.37477 |             0.762149 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 62370
  custom_metrics: {}
  date: 2021-09-19_04-52-25
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.255693843388514
  episode_reward_mean: 4.579976071459721
  episode_reward_min: 1.8102003425691573
  episodes_this_iter: 270
  episodes_total: 8910
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.392000356456265e-05
          entropy: 12.911772727966309
          entropy_coeff: 0.0004680063866544515
          kl: 0.01563146524131298
          model: {}
          policy_loss: -0.1653321087360382
          total_loss: -0.10257703810930252
          vf_explained_var: 0.989358127117157
          vf_loss: 0.033187415450811386
    num_agent_steps_sampled: 62370
    num_agent_steps_trained: 62370
    num_steps_sampled: 62370
    num_steps_trained: 62370
  iterations_since_restore: 33
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.97068062827225
    ram_util_percent: 5.799999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10197754812549564
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3600.455820677748
    mean_inference_ms: 2.92873934129459
    mean_raw_obs_processing_ms: 293.3878311356795
  time_since_restore: 43049.39272069931
  time_this_iter_s: 550.1546998023987
  time_total_s: 43049.39272069931
  timers:
    learn_throughput: 701.842
    learn_time_ms: 2692.912
    load_throughput: 159398.429
    load_time_ms: 11.857
    sample_throughput: 1.988
    sample_time_ms: 950508.465
    update_time_ms: 31.371
  timestamp: 1632052345
  timesteps_since_restore: 0
  timesteps_total: 62370
  training_iteration: 33
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     33 |          43049.4 | 62370 |  4.57998 |              7.25569 |               1.8102 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 64260
  custom_metrics: {}
  date: 2021-09-19_05-01-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.242822479190682
  episode_reward_mean: 4.5931207850204645
  episode_reward_min: 1.6433949518446571
  episodes_this_iter: 270
  episodes_total: 9180
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.372999920742586e-05
          entropy: 12.889429092407227
          entropy_coeff: 0.0004670066118706018
          kl: 0.014348330907523632
          model: {}
          policy_loss: -0.16120527684688568
          total_loss: -0.1025138646364212
          vf_explained_var: 0.9900078177452087
          vf_loss: 0.03202356398105621
    num_agent_steps_sampled: 64260
    num_agent_steps_trained: 64260
    num_steps_sampled: 64260
    num_steps_trained: 64260
  iterations_since_restore: 34
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.265582655826556
    ram_util_percent: 5.8
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10199219888574135
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3546.3378869262638
    mean_inference_ms: 2.9087931540212475
    mean_raw_obs_processing_ms: 290.7194284486035
  time_since_restore: 43580.81385731697
  time_this_iter_s: 531.4211366176605
  time_total_s: 43580.81385731697
  timers:
    learn_throughput: 699.14
    learn_time_ms: 2703.323
    load_throughput: 158587.193
    load_time_ms: 11.918
    sample_throughput: 2.018
    sample_time_ms: 936427.618
    update_time_ms: 31.111
  timestamp: 1632052877
  timesteps_since_restore: 0
  timesteps_total: 64260
  training_iteration: 34
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     34 |          43580.8 | 64260 |  4.59312 |              7.24282 |              1.64339 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 66150
  custom_metrics: {}
  date: 2021-09-19_06-23-38
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.2442067180161
  episode_reward_mean: 4.557048493902217
  episode_reward_min: 1.168369405852792
  episodes_this_iter: 270
  episodes_total: 9450
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.354000212624669e-05
          entropy: 12.863743782043457
          entropy_coeff: 0.0004660068079829216
          kl: 0.014748496003448963
          model: {}
          policy_loss: -0.15087559819221497
          total_loss: -0.091895692050457
          vf_explained_var: 0.9897183775901794
          vf_loss: 0.031375590711832047
    num_agent_steps_sampled: 66150
    num_agent_steps_trained: 66150
    num_steps_sampled: 66150
    num_steps_trained: 66150
  iterations_since_restore: 35
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.26409845616079
    ram_util_percent: 5.799999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10194840668247161
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3652.9004719675127
    mean_inference_ms: 2.889194705776385
    mean_raw_obs_processing_ms: 288.2391791614533
  time_since_restore: 48521.74427843094
  time_this_iter_s: 4940.930421113968
  time_total_s: 48521.74427843094
  timers:
    learn_throughput: 696.525
    learn_time_ms: 2713.471
    load_throughput: 157752.144
    load_time_ms: 11.981
    sample_throughput: 1.355
    sample_time_ms: 1395285.438
    update_time_ms: 31.334
  timestamp: 1632057818
  timesteps_since_restore: 0
  timesteps_total: 66150
  training_iteration: 35
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     35 |          48521.7 | 66150 |  4.55705 |              7.24421 |              1.16837 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 68040
  custom_metrics: {}
  date: 2021-09-19_06-40-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.040499311030848
  episode_reward_mean: 4.564454741206509
  episode_reward_min: 1.7314822821291527
  episodes_this_iter: 270
  episodes_total: 9720
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.33499977691099e-05
          entropy: 12.912359237670898
          entropy_coeff: 0.00046500700409524143
          kl: 0.015753138810396194
          model: {}
          policy_loss: -0.1797800064086914
          total_loss: -0.1170983836054802
          vf_explained_var: 0.9891117811203003
          vf_loss: 0.032798342406749725
    num_agent_steps_sampled: 68040
    num_agent_steps_trained: 68040
    num_steps_sampled: 68040
    num_steps_trained: 68040
  iterations_since_restore: 36
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.92273684210526
    ram_util_percent: 5.799999999999997
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10200228409580053
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3626.566690508594
    mean_inference_ms: 2.8715105514890076
    mean_raw_obs_processing_ms: 285.87725480922114
  time_since_restore: 49547.068276166916
  time_this_iter_s: 1025.3239977359772
  time_total_s: 49547.068276166916
  timers:
    learn_throughput: 694.258
    learn_time_ms: 2722.331
    load_throughput: 157934.118
    load_time_ms: 11.967
    sample_throughput: 1.316
    sample_time_ms: 1436211.825
    update_time_ms: 31.191
  timestamp: 1632058843
  timesteps_since_restore: 0
  timesteps_total: 68040
  training_iteration: 36
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     36 |          49547.1 | 68040 |  4.56445 |               8.0405 |              1.73148 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 69930
  custom_metrics: {}
  date: 2021-09-19_07-17-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.383381352489242
  episode_reward_mean: 4.505560186089739
  episode_reward_min: 2.0150893248188853
  episodes_this_iter: 270
  episodes_total: 9990
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.316000068793073e-05
          entropy: 12.881827354431152
          entropy_coeff: 0.00046400720020756125
          kl: 0.015591884031891823
          model: {}
          policy_loss: -0.16681911051273346
          total_loss: -0.10426385700702667
          vf_explained_var: 0.9887896180152893
          vf_loss: 0.03301223739981651
    num_agent_steps_sampled: 69930
    num_agent_steps_trained: 69930
    num_steps_sampled: 69930
    num_steps_trained: 69930
  iterations_since_restore: 37
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.05526060213662
    ram_util_percent: 5.786662350275169
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10198973098522947
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3630.1086306920747
    mean_inference_ms: 2.854084704848349
    mean_raw_obs_processing_ms: 283.6277231996221
  time_since_restore: 51769.424589157104
  time_this_iter_s: 2222.3563129901886
  time_total_s: 51769.424589157104
  timers:
    learn_throughput: 695.97
    learn_time_ms: 2715.634
    load_throughput: 158497.142
    load_time_ms: 11.925
    sample_throughput: 1.185
    sample_time_ms: 1594543.098
    update_time_ms: 30.935
  timestamp: 1632061065
  timesteps_since_restore: 0
  timesteps_total: 69930
  training_iteration: 37
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     37 |          51769.4 | 69930 |  4.50556 |              7.38338 |              2.01509 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 71820
  custom_metrics: {}
  date: 2021-09-19_07-38-02
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.841615599422244
  episode_reward_mean: 4.653460343600415
  episode_reward_min: 1.9065919229237203
  episodes_this_iter: 270
  episodes_total: 10260
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.297000360675156e-05
          entropy: 12.866601943969727
          entropy_coeff: 0.0004630073963198811
          kl: 0.01432772260159254
          model: {}
          policy_loss: -0.17917828261852264
          total_loss: -0.11879241466522217
          vf_explained_var: 0.9894523024559021
          vf_loss: 0.03370285406708717
    num_agent_steps_sampled: 71820
    num_agent_steps_trained: 71820
    num_steps_sampled: 71820
    num_steps_trained: 71820
  iterations_since_restore: 38
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.93934911242604
    ram_util_percent: 5.938579881656805
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10197683335278349
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3638.436804135177
    mean_inference_ms: 2.8411395554247316
    mean_raw_obs_processing_ms: 281.5878120359011
  time_since_restore: 52985.74881148338
  time_this_iter_s: 1216.3242223262787
  time_total_s: 52985.74881148338
  timers:
    learn_throughput: 697.042
    learn_time_ms: 2711.458
    load_throughput: 157439.153
    load_time_ms: 12.005
    sample_throughput: 1.179
    sample_time_ms: 1602913.954
    update_time_ms: 31.036
  timestamp: 1632062282
  timesteps_since_restore: 0
  timesteps_total: 71820
  training_iteration: 38
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     38 |          52985.7 | 71820 |  4.65346 |              7.84162 |              1.90659 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 73710
  custom_metrics: {}
  date: 2021-09-19_09-46-21
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.733728143471463
  episode_reward_mean: 4.601403349925296
  episode_reward_min: 1.562996242303957
  episodes_this_iter: 270
  episodes_total: 10530
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.277999924961478e-05
          entropy: 12.87957763671875
          entropy_coeff: 0.0004620075924322009
          kl: 0.015276740305125713
          model: {}
          policy_loss: -0.18257705867290497
          total_loss: -0.12054014950990677
          vf_explained_var: 0.9893172979354858
          vf_loss: 0.03318502753973007
    num_agent_steps_sampled: 73710
    num_agent_steps_trained: 73710
    num_steps_sampled: 73710
    num_steps_trained: 73710
  iterations_since_restore: 39
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.87244020926756
    ram_util_percent: 5.993675261584455
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1019650576230724
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3707.8335677530536
    mean_inference_ms: 2.826174321401595
    mean_raw_obs_processing_ms: 279.62972690841343
  time_since_restore: 60685.25180268288
  time_this_iter_s: 7699.502991199493
  time_total_s: 60685.25180268288
  timers:
    learn_throughput: 696.845
    learn_time_ms: 2712.226
    load_throughput: 158014.395
    load_time_ms: 11.961
    sample_throughput: 0.849
    sample_time_ms: 2225034.713
    update_time_ms: 30.771
  timestamp: 1632069981
  timesteps_since_restore: 0
  timesteps_total: 73710
  training_iteration: 39
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     39 |          60685.3 | 73710 |   4.6014 |              7.73373 |                1.563 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 75600
  custom_metrics: {}
  date: 2021-09-19_10-05-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.674940711496761
  episode_reward_mean: 4.569011547531686
  episode_reward_min: 1.71965262522209
  episodes_this_iter: 270
  episodes_total: 10800
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.25900021684356e-05
          entropy: 12.78171443939209
          entropy_coeff: 0.00046100778854452074
          kl: 0.015248960815370083
          model: {}
          policy_loss: -0.16039589047431946
          total_loss: -0.10229556262493134
          vf_explained_var: 0.9903885722160339
          vf_loss: 0.0292537659406662
    num_agent_steps_sampled: 75600
    num_agent_steps_trained: 75600
    num_steps_sampled: 75600
    num_steps_trained: 75600
  iterations_since_restore: 40
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.88660826032541
    ram_util_percent: 6.001376720901127
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10193223613885838
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3697.0301186282095
    mean_inference_ms: 2.812971448303331
    mean_raw_obs_processing_ms: 277.8296813182865
  time_since_restore: 61834.72071099281
  time_this_iter_s: 1149.4689083099365
  time_total_s: 61834.72071099281
  timers:
    learn_throughput: 696.108
    learn_time_ms: 2715.094
    load_throughput: 172394.147
    load_time_ms: 10.963
    sample_throughput: 0.824
    sample_time_ms: 2293750.546
    update_time_ms: 31.216
  timestamp: 1632071131
  timesteps_since_restore: 0
  timesteps_total: 75600
  training_iteration: 40
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     40 |          61834.7 | 75600 |  4.56901 |              7.67494 |              1.71965 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 77490
  custom_metrics: {}
  date: 2021-09-19_10-55-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.635587736863234
  episode_reward_mean: 4.557117372545862
  episode_reward_min: 1.3437060436186823
  episodes_this_iter: 270
  episodes_total: 11070
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.239999781129882e-05
          entropy: 12.747865676879883
          entropy_coeff: 0.000460008013760671
          kl: 0.015118755400180817
          model: {}
          policy_loss: -0.19995813071727753
          total_loss: -0.14009526371955872
          vf_explained_var: 0.9897421598434448
          vf_loss: 0.03128459304571152
    num_agent_steps_sampled: 77490
    num_agent_steps_trained: 77490
    num_steps_sampled: 77490
    num_steps_trained: 77490
  iterations_since_restore: 41
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.724802678784975
    ram_util_percent: 6.000023917723034
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10187199111913905
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3717.9414237969927
    mean_inference_ms: 2.8001711687220086
    mean_raw_obs_processing_ms: 276.1279234187422
  time_since_restore: 64843.615148067474
  time_this_iter_s: 3008.8944370746613
  time_total_s: 64843.615148067474
  timers:
    learn_throughput: 697.255
    learn_time_ms: 2710.629
    load_throughput: 170295.05
    load_time_ms: 11.098
    sample_throughput: 0.752
    sample_time_ms: 2514036.666
    update_time_ms: 31.449
  timestamp: 1632074140
  timesteps_since_restore: 0
  timesteps_total: 77490
  training_iteration: 41
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     41 |          64843.6 | 77490 |  4.55712 |              7.63559 |              1.34371 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 79380
  custom_metrics: {}
  date: 2021-09-19_11-11-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.040478256538098
  episode_reward_mean: 4.657963826571873
  episode_reward_min: 2.075124507648859
  episodes_this_iter: 270
  episodes_total: 11340
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.221000073011965e-05
          entropy: 12.770552635192871
          entropy_coeff: 0.00045900820987299085
          kl: 0.014703535474836826
          model: {}
          policy_loss: -0.17581455409526825
          total_loss: -0.11737173050642014
          vf_explained_var: 0.9901736974716187
          vf_loss: 0.030808132141828537
    num_agent_steps_sampled: 79380
    num_agent_steps_trained: 79380
    num_steps_sampled: 79380
    num_steps_trained: 79380
  iterations_since_restore: 42
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.532370820668696
    ram_util_percent: 5.993844984802432
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1018231555080216
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3700.1683388687056
    mean_inference_ms: 2.787354186909053
    mean_raw_obs_processing_ms: 274.43398971464063
  time_since_restore: 65790.98793458939
  time_this_iter_s: 947.3727865219116
  time_total_s: 65790.98793458939
  timers:
    learn_throughput: 699.815
    learn_time_ms: 2700.713
    load_throughput: 169671.039
    load_time_ms: 11.139
    sample_throughput: 0.812
    sample_time_ms: 2326301.424
    update_time_ms: 31.323
  timestamp: 1632075087
  timesteps_since_restore: 0
  timesteps_total: 79380
  training_iteration: 42
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     42 |            65791 | 79380 |  4.65796 |              8.04048 |              2.07512 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 81270
  custom_metrics: {}
  date: 2021-09-19_11-32-49
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.288718397902067
  episode_reward_mean: 4.573950181985103
  episode_reward_min: 1.4459824290563823
  episodes_this_iter: 270
  episodes_total: 11610
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.201999637298286e-05
          entropy: 12.727059364318848
          entropy_coeff: 0.0004580084059853107
          kl: 0.015065132640302181
          model: {}
          policy_loss: -0.18400879204273224
          total_loss: -0.12256370484828949
          vf_explained_var: 0.989294707775116
          vf_loss: 0.03295394405722618
    num_agent_steps_sampled: 81270
    num_agent_steps_trained: 81270
    num_steps_sampled: 81270
    num_steps_trained: 81270
  iterations_since_restore: 43
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.0351290684624
    ram_util_percent: 6.091245791245791
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10174739918012628
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3690.20201899951
    mean_inference_ms: 2.7769478626343775
    mean_raw_obs_processing_ms: 272.8668427395591
  time_since_restore: 67073.07796382904
  time_this_iter_s: 1282.0900292396545
  time_total_s: 67073.07796382904
  timers:
    learn_throughput: 703.92
    learn_time_ms: 2684.963
    load_throughput: 169272.506
    load_time_ms: 11.165
    sample_throughput: 0.788
    sample_time_ms: 2399510.661
    update_time_ms: 31.294
  timestamp: 1632076369
  timesteps_since_restore: 0
  timesteps_total: 81270
  training_iteration: 43
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     43 |          67073.1 | 81270 |  4.57395 |              7.28872 |              1.44598 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 83160
  custom_metrics: {}
  date: 2021-09-19_11-51-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.927420558137271
  episode_reward_mean: 4.653950786347818
  episode_reward_min: 1.9295867376654223
  episodes_this_iter: 270
  episodes_total: 11880
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.182999929180369e-05
          entropy: 12.733434677124023
          entropy_coeff: 0.0004570086020976305
          kl: 0.01479458250105381
          model: {}
          policy_loss: -0.18520161509513855
          total_loss: -0.12894338369369507
          vf_explained_var: 0.9910717606544495
          vf_loss: 0.028373587876558304
    num_agent_steps_sampled: 83160
    num_agent_steps_trained: 83160
    num_steps_sampled: 83160
    num_steps_trained: 83160
  iterations_since_restore: 44
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.365379581151835
    ram_util_percent: 6.154384816753928
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1017990200464748
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3665.6062453897553
    mean_inference_ms: 2.7677091145917596
    mean_raw_obs_processing_ms: 271.3664298827812
  time_since_restore: 68173.34538197517
  time_this_iter_s: 1100.2674181461334
  time_total_s: 68173.34538197517
  timers:
    learn_throughput: 706.676
    learn_time_ms: 2674.491
    load_throughput: 167147.788
    load_time_ms: 11.307
    sample_throughput: 0.769
    sample_time_ms: 2456405.306
    update_time_ms: 31.355
  timestamp: 1632077470
  timesteps_since_restore: 0
  timesteps_total: 83160
  training_iteration: 44
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     44 |          68173.3 | 83160 |  4.65395 |              7.92742 |              1.92959 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 85050
  custom_metrics: {}
  date: 2021-09-19_12-28-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.825682850599297
  episode_reward_mean: 4.499102510241203
  episode_reward_min: 1.238528322151786
  episodes_this_iter: 270
  episodes_total: 12150
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.164000221062452e-05
          entropy: 12.755613327026367
          entropy_coeff: 0.00045600879820995033
          kl: 0.015449585393071175
          model: {}
          policy_loss: -0.17506840825080872
          total_loss: -0.11696209013462067
          vf_explained_var: 0.9905586838722229
          vf_loss: 0.028726886957883835
    num_agent_steps_sampled: 85050
    num_agent_steps_trained: 85050
    num_steps_sampled: 85050
    num_steps_trained: 85050
  iterations_since_restore: 45
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.783094098883574
    ram_util_percent: 5.910781499202553
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10174351440472677
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3667.8748643824406
    mean_inference_ms: 2.759716265455003
    mean_raw_obs_processing_ms: 269.91965531561164
  time_since_restore: 70429.96029305458
  time_this_iter_s: 2256.6149110794067
  time_total_s: 70429.96029305458
  timers:
    learn_throughput: 707.01
    learn_time_ms: 2673.228
    load_throughput: 168656.312
    load_time_ms: 11.206
    sample_throughput: 0.864
    sample_time_ms: 2187975.02
    update_time_ms: 31.064
  timestamp: 1632079726
  timesteps_since_restore: 0
  timesteps_total: 85050
  training_iteration: 45
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     45 |            70430 | 85050 |   4.4991 |              7.82568 |              1.23853 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 86940
  custom_metrics: {}
  date: 2021-09-19_12-34-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.622937076738485
  episode_reward_mean: 4.538451163280824
  episode_reward_min: 1.1232464563884963
  episodes_this_iter: 270
  episodes_total: 12420
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.144999785348773e-05
          entropy: 12.726941108703613
          entropy_coeff: 0.00045500899432227015
          kl: 0.01633210852742195
          model: {}
          policy_loss: -0.17174027860164642
          total_loss: -0.1056085154414177
          vf_explained_var: 0.9894903302192688
          vf_loss: 0.034716036170721054
    num_agent_steps_sampled: 86940
    num_agent_steps_trained: 86940
    num_steps_sampled: 86940
    num_steps_trained: 86940
  iterations_since_restore: 46
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.863983050847455
    ram_util_percent: 5.777118644067798
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1017635196507268
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3630.4147778767615
    mean_inference_ms: 2.7525555782882156
    mean_raw_obs_processing_ms: 268.6269299476748
  time_since_restore: 70770.09211468697
  time_this_iter_s: 340.13182163238525
  time_total_s: 70770.09211468697
  timers:
    learn_throughput: 707.522
    learn_time_ms: 2671.294
    load_throughput: 168880.876
    load_time_ms: 11.191
    sample_throughput: 0.892
    sample_time_ms: 2119458.165
    update_time_ms: 30.88
  timestamp: 1632080067
  timesteps_since_restore: 0
  timesteps_total: 86940
  training_iteration: 46
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     46 |          70770.1 | 86940 |  4.53845 |              7.62294 |              1.12325 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 88830
  custom_metrics: {}
  date: 2021-09-19_12-48-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.7333575344242975
  episode_reward_mean: 4.666145138147305
  episode_reward_min: 2.170504234977631
  episodes_this_iter: 270
  episodes_total: 12690
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.126000077230856e-05
          entropy: 12.699563026428223
          entropy_coeff: 0.00045400919043459
          kl: 0.015848102048039436
          model: {}
          policy_loss: -0.15895695984363556
          total_loss: -0.09970016032457352
          vf_explained_var: 0.990989089012146
          vf_loss: 0.028918562456965446
    num_agent_steps_sampled: 88830
    num_agent_steps_trained: 88830
    num_steps_sampled: 88830
    num_steps_trained: 88830
  iterations_since_restore: 47
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.949156829679595
    ram_util_percent: 6.212394603709949
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10171847755347659
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3594.804114608816
    mean_inference_ms: 2.7437960780612243
    mean_raw_obs_processing_ms: 267.35298184019814
  time_since_restore: 71623.91296625137
  time_this_iter_s: 853.8208515644073
  time_total_s: 71623.91296625137
  timers:
    learn_throughput: 706.485
    learn_time_ms: 2675.215
    load_throughput: 168899.227
    load_time_ms: 11.19
    sample_throughput: 0.953
    sample_time_ms: 1982600.786
    update_time_ms: 30.445
  timestamp: 1632080920
  timesteps_since_restore: 0
  timesteps_total: 88830
  training_iteration: 47
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     47 |          71623.9 | 88830 |  4.66615 |              7.73336 |               2.1705 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 90720
  custom_metrics: {}
  date: 2021-09-19_13-16-20
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.794075168041574
  episode_reward_mean: 4.604281811875918
  episode_reward_min: 1.041334278594967
  episodes_this_iter: 270
  episodes_total: 12960
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.106999641517177e-05
          entropy: 12.687578201293945
          entropy_coeff: 0.0004530093865469098
          kl: 0.01581226848065853
          model: {}
          policy_loss: -0.17302323877811432
          total_loss: -0.1116359755396843
          vf_explained_var: 0.9901971220970154
          vf_loss: 0.031112518161535263
    num_agent_steps_sampled: 90720
    num_agent_steps_trained: 90720
    num_steps_sampled: 90720
    num_steps_trained: 90720
  iterations_since_restore: 48
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.154596704249787
    ram_util_percent: 5.759713790112751
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1016889498352014
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3592.7110544620064
    mean_inference_ms: 2.734580359338999
    mean_raw_obs_processing_ms: 266.0636970503645
  time_since_restore: 73283.42008686066
  time_this_iter_s: 1659.5071206092834
  time_total_s: 73283.42008686066
  timers:
    learn_throughput: 707.296
    learn_time_ms: 2672.15
    load_throughput: 167573.909
    load_time_ms: 11.279
    sample_throughput: 0.932
    sample_time_ms: 2026922.191
    update_time_ms: 30.211
  timestamp: 1632082580
  timesteps_since_restore: 0
  timesteps_total: 90720
  training_iteration: 48
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     48 |          73283.4 | 90720 |  4.60428 |              7.79408 |              1.04133 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 92610
  custom_metrics: {}
  date: 2021-09-19_13-37-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.4141451900908715
  episode_reward_mean: 4.551607339019636
  episode_reward_min: 1.5275711819670363
  episodes_this_iter: 270
  episodes_total: 13230
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.08799993339926e-05
          entropy: 12.685400009155273
          entropy_coeff: 0.0004520096117630601
          kl: 0.015064314939081669
          model: {}
          policy_loss: -0.1937766820192337
          total_loss: -0.13510288298130035
          vf_explained_var: 0.9902932047843933
          vf_loss: 0.030089322477579117
    num_agent_steps_sampled: 92610
    num_agent_steps_trained: 92610
    num_steps_sampled: 92610
    num_steps_trained: 92610
  iterations_since_restore: 49
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.92129629629629
    ram_util_percent: 6.075173611111112
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10164001121810971
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3583.2498058179453
    mean_inference_ms: 2.72713861244889
    mean_raw_obs_processing_ms: 264.9006493387598
  time_since_restore: 74527.27615189552
  time_this_iter_s: 1243.8560650348663
  time_total_s: 74527.27615189552
  timers:
    learn_throughput: 709.508
    learn_time_ms: 2663.817
    load_throughput: 167758.668
    load_time_ms: 11.266
    sample_throughput: 1.368
    sample_time_ms: 1381366.367
    update_time_ms: 30.258
  timestamp: 1632083824
  timesteps_since_restore: 0
  timesteps_total: 92610
  training_iteration: 49
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     49 |          74527.3 | 92610 |  4.55161 |              7.41415 |              1.52757 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 94500
  custom_metrics: {}
  date: 2021-09-19_14-13-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.697148509999315
  episode_reward_mean: 4.7005411618319615
  episode_reward_min: 2.0758976877391095
  episodes_this_iter: 270
  episodes_total: 13500
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.069000225281343e-05
          entropy: 12.67437744140625
          entropy_coeff: 0.0004510098078753799
          kl: 0.014687744900584221
          model: {}
          policy_loss: -0.18756040930747986
          total_loss: -0.13301388919353485
          vf_explained_var: 0.9916689991950989
          vf_loss: 0.02680225484073162
    num_agent_steps_sampled: 94500
    num_agent_steps_trained: 94500
    num_steps_sampled: 94500
    num_steps_trained: 94500
  iterations_since_restore: 50
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.464757853403135
    ram_util_percent: 6.15625
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10161861386019234
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3618.3806575597387
    mean_inference_ms: 2.7208185525957553
    mean_raw_obs_processing_ms: 263.73516593334915
  time_since_restore: 76726.43774294853
  time_this_iter_s: 2199.161591053009
  time_total_s: 76726.43774294853
  timers:
    learn_throughput: 712.654
    learn_time_ms: 2652.058
    load_throughput: 168833.399
    load_time_ms: 11.194
    sample_throughput: 1.272
    sample_time_ms: 1486347.176
    update_time_ms: 30.813
  timestamp: 1632086023
  timesteps_since_restore: 0
  timesteps_total: 94500
  training_iteration: 50
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     50 |          76726.4 | 94500 |  4.70054 |              7.69715 |               2.0759 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 96390
  custom_metrics: {}
  date: 2021-09-19_14-55-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.596772366943209
  episode_reward_mean: 4.615367523123645
  episode_reward_min: 1.4619484417906878
  episodes_this_iter: 270
  episodes_total: 13770
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.049999789567664e-05
          entropy: 12.725523948669434
          entropy_coeff: 0.00045001000398769975
          kl: 0.014873703010380268
          model: {}
          policy_loss: -0.19440217316150665
          total_loss: -0.13643495738506317
          vf_explained_var: 0.9904248118400574
          vf_loss: 0.029809659346938133
    num_agent_steps_sampled: 96390
    num_agent_steps_trained: 96390
    num_steps_sampled: 96390
    num_steps_trained: 96390
  iterations_since_restore: 51
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.649239598278335
    ram_util_percent: 6.124533715925395
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10167190306731931
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3643.7161659694216
    mean_inference_ms: 2.7146087326369917
    mean_raw_obs_processing_ms: 262.57878406527624
  time_since_restore: 79234.04946923256
  time_this_iter_s: 2507.611726284027
  time_total_s: 79234.04946923256
  timers:
    learn_throughput: 713.35
    learn_time_ms: 2649.47
    load_throughput: 171372.617
    load_time_ms: 11.029
    sample_throughput: 1.316
    sample_time_ms: 1436221.604
    update_time_ms: 31.213
  timestamp: 1632088531
  timesteps_since_restore: 0
  timesteps_total: 96390
  training_iteration: 51
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     51 |            79234 | 96390 |  4.61537 |              7.59677 |              1.46195 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 98280
  custom_metrics: {}
  date: 2021-09-19_15-24-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.374598645665668
  episode_reward_mean: 4.617351671617125
  episode_reward_min: 1.6833616974170198
  episodes_this_iter: 270
  episodes_total: 14040
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.031000081449747e-05
          entropy: 12.712335586547852
          entropy_coeff: 0.0004490102001000196
          kl: 0.014660757966339588
          model: {}
          policy_loss: -0.1745297759771347
          total_loss: -0.11808903515338898
          vf_explained_var: 0.9914625287055969
          vf_loss: 0.028749672695994377
    num_agent_steps_sampled: 98280
    num_agent_steps_trained: 98280
    num_steps_sampled: 98280
    num_steps_trained: 98280
  iterations_since_restore: 52
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.67908633696564
    ram_util_percent: 6.250628667225482
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10166093355052958
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3657.9143563440007
    mean_inference_ms: 2.706934844539677
    mean_raw_obs_processing_ms: 261.4711736740029
  time_since_restore: 80951.58063983917
  time_this_iter_s: 1717.5311706066132
  time_total_s: 80951.58063983917
  timers:
    learn_throughput: 713.556
    learn_time_ms: 2648.707
    load_throughput: 169468.64
    load_time_ms: 11.153
    sample_throughput: 1.249
    sample_time_ms: 1513237.88
    update_time_ms: 31.424
  timestamp: 1632090248
  timesteps_since_restore: 0
  timesteps_total: 98280
  training_iteration: 52
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     52 |          80951.6 | 98280 |  4.61735 |               7.3746 |              1.68336 |                  7 |
+------------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 100170
  custom_metrics: {}
  date: 2021-09-19_15-38-09
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.832120734843506
  episode_reward_mean: 4.697337158325468
  episode_reward_min: 1.5922817468912003
  episodes_this_iter: 270
  episodes_total: 14310
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.011999645736068e-05
          entropy: 12.626420974731445
          entropy_coeff: 0.0004480103962123394
          kl: 0.014728493988513947
          model: {}
          policy_loss: -0.18213807046413422
          total_loss: -0.12479909509420395
          vf_explained_var: 0.9909005165100098
          vf_loss: 0.02944241091609001
    num_agent_steps_sampled: 100170
    num_agent_steps_trained: 100170
    num_steps_sampled: 100170
    num_steps_trained: 100170
  iterations_since_restore: 53
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.884160958904104
    ram_util_percent: 6.297431506849315
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10159732658302942
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3640.6057499477242
    mean_inference_ms: 2.700953811361036
    mean_raw_obs_processing_ms: 260.4826705654611
  time_since_restore: 81791.7517580986
  time_this_iter_s: 840.1711182594299
  time_total_s: 81791.7517580986
  timers:
    learn_throughput: 713.925
    learn_time_ms: 2647.338
    load_throughput: 169724.803
    load_time_ms: 11.136
    sample_throughput: 1.287
    sample_time_ms: 1469047.252
    update_time_ms: 31.32
  timestamp: 1632091089
  timesteps_since_restore: 0
  timesteps_total: 100170
  training_iteration: 53
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     53 |          81791.8 | 100170 |  4.69734 |              7.83212 |              1.59228 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 102060
  custom_metrics: {}
  date: 2021-09-19_15-53-00
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.485868666910008
  episode_reward_mean: 4.637584486061474
  episode_reward_min: 1.7933918016020847
  episodes_this_iter: 270
  episodes_total: 14580
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.992999937618151e-05
          entropy: 12.602248191833496
          entropy_coeff: 0.00044701059232465923
          kl: 0.014493366703391075
          model: {}
          policy_loss: -0.16856305301189423
          total_loss: -0.11495105177164078
          vf_explained_var: 0.9916489720344543
          vf_loss: 0.02622762694954872
    num_agent_steps_sampled: 102060
    num_agent_steps_trained: 102060
    num_steps_sampled: 102060
    num_steps_trained: 102060
  iterations_since_restore: 54
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.76348949919225
    ram_util_percent: 6.262843295638126
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10163136742645654
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3622.1298424358547
    mean_inference_ms: 2.6943869445881408
    mean_raw_obs_processing_ms: 259.4590780045505
  time_since_restore: 82683.2337217331
  time_this_iter_s: 891.481963634491
  time_total_s: 82683.2337217331
  timers:
    learn_throughput: 713.769
    learn_time_ms: 2647.917
    load_throughput: 172133.233
    load_time_ms: 10.98
    sample_throughput: 1.305
    sample_time_ms: 1448168.267
    update_time_ms: 31.52
  timestamp: 1632091980
  timesteps_since_restore: 0
  timesteps_total: 102060
  training_iteration: 54
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     54 |          82683.2 | 102060 |  4.63758 |              7.48587 |              1.79339 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 103950
  custom_metrics: {}
  date: 2021-09-19_16-17-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.7429450125811305
  episode_reward_mean: 4.6428621238614864
  episode_reward_min: 1.4291228749051061
  episodes_this_iter: 270
  episodes_total: 14850
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.974000229500234e-05
          entropy: 12.65595531463623
          entropy_coeff: 0.00044601078843697906
          kl: 0.01568666286766529
          model: {}
          policy_loss: -0.1594114601612091
          total_loss: -0.09915731102228165
          vf_explained_var: 0.9908940196037292
          vf_loss: 0.030162658542394638
    num_agent_steps_sampled: 103950
    num_agent_steps_trained: 103950
    num_steps_sampled: 103950
    num_steps_trained: 103950
  iterations_since_restore: 55
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.060299251870326
    ram_util_percent: 6.243192019950126
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10162343574359368
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3638.1960670788953
    mean_inference_ms: 2.6873064652731684
    mean_raw_obs_processing_ms: 258.4554876472373
  time_since_restore: 84127.21187067032
  time_this_iter_s: 1443.9781489372253
  time_total_s: 84127.21187067032
  timers:
    learn_throughput: 714.788
    learn_time_ms: 2644.142
    load_throughput: 171262.286
    load_time_ms: 11.036
    sample_throughput: 1.383
    sample_time_ms: 1366909.413
    update_time_ms: 31.108
  timestamp: 1632093424
  timesteps_since_restore: 0
  timesteps_total: 103950
  training_iteration: 55
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     55 |          84127.2 | 103950 |  4.64286 |              7.74295 |              1.42912 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 105840
  custom_metrics: {}
  date: 2021-09-19_16-29-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.547433067542189
  episode_reward_mean: 4.663946358018364
  episode_reward_min: 1.7832056856853513
  episodes_this_iter: 270
  episodes_total: 15120
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.954999793786556e-05
          entropy: 12.623085975646973
          entropy_coeff: 0.00044501101365312934
          kl: 0.015977727249264717
          model: {}
          policy_loss: -0.16235360503196716
          total_loss: -0.10056924819946289
          vf_explained_var: 0.9904050230979919
          vf_loss: 0.031002480536699295
    num_agent_steps_sampled: 105840
    num_agent_steps_trained: 105840
    num_steps_sampled: 105840
    num_steps_trained: 105840
  iterations_since_restore: 56
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.53301707779887
    ram_util_percent: 6.2637571157495255
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10165205764500537
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3619.7246047454387
    mean_inference_ms: 2.6810283147942826
    mean_raw_obs_processing_ms: 257.55445169429504
  time_since_restore: 84885.8542983532
  time_this_iter_s: 758.6424276828766
  time_total_s: 84885.8542983532
  timers:
    learn_throughput: 713.784
    learn_time_ms: 2647.86
    load_throughput: 170276.028
    load_time_ms: 11.1
    sample_throughput: 1.342
    sample_time_ms: 1408756.798
    update_time_ms: 31.471
  timestamp: 1632094183
  timesteps_since_restore: 0
  timesteps_total: 105840
  training_iteration: 56
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     56 |          84885.9 | 105840 |  4.66395 |              7.54743 |              1.78321 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 107730
  custom_metrics: {}
  date: 2021-09-19_17-34-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.879000347682806
  episode_reward_mean: 4.653828122676486
  episode_reward_min: 1.7912941047509459
  episodes_this_iter: 270
  episodes_total: 15390
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.936000085668638e-05
          entropy: 12.624856948852539
          entropy_coeff: 0.00044401120976544917
          kl: 0.0158888790756464
          model: {}
          policy_loss: -0.18860432505607605
          total_loss: -0.13115128874778748
          vf_explained_var: 0.991514265537262
          vf_loss: 0.02686174027621746
    num_agent_steps_sampled: 107730
    num_agent_steps_trained: 107730
    num_steps_sampled: 107730
    num_steps_trained: 107730
  iterations_since_restore: 57
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.99109349068782
    ram_util_percent: 6.267103079476304
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10163159273830402
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3689.8809033596644
    mean_inference_ms: 2.6751832007610306
    mean_raw_obs_processing_ms: 256.63461431654395
  time_since_restore: 88788.36590313911
  time_this_iter_s: 3902.511604785919
  time_total_s: 88788.36590313911
  timers:
    learn_throughput: 713.368
    learn_time_ms: 2649.406
    load_throughput: 169589.006
    load_time_ms: 11.145
    sample_throughput: 1.103
    sample_time_ms: 1713624.172
    update_time_ms: 31.526
  timestamp: 1632098085
  timesteps_since_restore: 0
  timesteps_total: 107730
  training_iteration: 57
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     57 |          88788.4 | 107730 |  4.65383 |                7.879 |              1.79129 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 109620
  custom_metrics: {}
  date: 2021-09-19_18-09-56
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.796142798257283
  episode_reward_mean: 4.679374368474014
  episode_reward_min: 1.4637279352563457
  episodes_this_iter: 270
  episodes_total: 15660
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.91699964995496e-05
          entropy: 12.613532066345215
          entropy_coeff: 0.000443011405877769
          kl: 0.015032337978482246
          model: {}
          policy_loss: -0.19379279017448425
          total_loss: -0.13850335776805878
          vf_explained_var: 0.9915677309036255
          vf_loss: 0.026631832122802734
    num_agent_steps_sampled: 109620
    num_agent_steps_trained: 109620
    num_steps_sampled: 109620
    num_steps_trained: 109620
  iterations_since_restore: 58
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.31950886766712
    ram_util_percent: 5.977967257844475
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10171185962203794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3696.7417387684072
    mean_inference_ms: 2.6687282781916806
    mean_raw_obs_processing_ms: 255.72583380931982
  time_since_restore: 90899.3208539486
  time_this_iter_s: 2110.9549508094788
  time_total_s: 90899.3208539486
  timers:
    learn_throughput: 713.332
    learn_time_ms: 2649.539
    load_throughput: 171045.74
    load_time_ms: 11.05
    sample_throughput: 1.075
    sample_time_ms: 1758768.87
    update_time_ms: 31.46
  timestamp: 1632100196
  timesteps_since_restore: 0
  timesteps_total: 109620
  training_iteration: 58
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     58 |          90899.3 | 109620 |  4.67937 |              7.79614 |              1.46373 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 111510
  custom_metrics: {}
  date: 2021-09-19_18-40-41
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.352696612003451
  episode_reward_mean: 4.711358389882886
  episode_reward_min: 1.638700327194077
  episodes_this_iter: 270
  episodes_total: 15930
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.897999941837043e-05
          entropy: 12.575844764709473
          entropy_coeff: 0.0004420116019900888
          kl: 0.015769660472869873
          model: {}
          policy_loss: -0.16753332316875458
          total_loss: -0.10973882675170898
          vf_explained_var: 0.9914771318435669
          vf_loss: 0.027427896857261658
    num_agent_steps_sampled: 111510
    num_agent_steps_trained: 111510
    num_steps_sampled: 111510
    num_steps_trained: 111510
  iterations_since_restore: 59
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.67331252438549
    ram_util_percent: 6.130198985563792
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10166986752380704
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3681.8379499027205
    mean_inference_ms: 2.6616815564197736
    mean_raw_obs_processing_ms: 254.87199357761537
  time_since_restore: 92743.75077342987
  time_this_iter_s: 1844.4299194812775
  time_total_s: 92743.75077342987
  timers:
    learn_throughput: 711.848
    learn_time_ms: 2655.062
    load_throughput: 170876.875
    load_time_ms: 11.061
    sample_throughput: 1.039
    sample_time_ms: 1818820.844
    update_time_ms: 31.029
  timestamp: 1632102041
  timesteps_since_restore: 0
  timesteps_total: 111510
  training_iteration: 59
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     59 |          92743.8 | 111510 |  4.71136 |               7.3527 |               1.6387 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 113400
  custom_metrics: {}
  date: 2021-09-19_18-50-38
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.005419746539834
  episode_reward_mean: 4.730820552637453
  episode_reward_min: 1.7225454261707813
  episodes_this_iter: 270
  episodes_total: 16200
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.879000233719125e-05
          entropy: 12.624068260192871
          entropy_coeff: 0.00044101179810240865
          kl: 0.014468912966549397
          model: {}
          policy_loss: -0.17807206511497498
          total_loss: -0.12569035589694977
          vf_explained_var: 0.9923184514045715
          vf_loss: 0.024987082928419113
    num_agent_steps_sampled: 113400
    num_agent_steps_trained: 113400
    num_steps_sampled: 113400
    num_steps_trained: 113400
  iterations_since_restore: 60
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.71831325301204
    ram_util_percent: 6.418192771084338
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10164908238896167
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3650.2374040795403
    mean_inference_ms: 2.654176693472516
    mean_raw_obs_processing_ms: 253.99299210536512
  time_since_restore: 93340.86313295364
  time_this_iter_s: 597.1123595237732
  time_total_s: 93340.86313295364
  timers:
    learn_throughput: 710.477
    learn_time_ms: 2660.184
    load_throughput: 170059.779
    load_time_ms: 11.114
    sample_throughput: 1.14
    sample_time_ms: 1658611.7
    update_time_ms: 29.859
  timestamp: 1632102638
  timesteps_since_restore: 0
  timesteps_total: 113400
  training_iteration: 60
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     60 |          93340.9 | 113400 |  4.73082 |              8.00542 |              1.72255 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 115290
  custom_metrics: {}
  date: 2021-09-19_19-20-11
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.486037406645691
  episode_reward_mean: 4.708788213039982
  episode_reward_min: 1.6663034434026458
  episodes_this_iter: 270
  episodes_total: 16470
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.859999798005447e-05
          entropy: 12.583513259887695
          entropy_coeff: 0.0004400119942147285
          kl: 0.015230854041874409
          model: {}
          policy_loss: -0.1702326387166977
          total_loss: -0.11401079595088959
          vf_explained_var: 0.9915542006492615
          vf_loss: 0.027060940861701965
    num_agent_steps_sampled: 115290
    num_agent_steps_trained: 115290
    num_steps_sampled: 115290
    num_steps_trained: 115290
  iterations_since_restore: 61
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.80856678846935
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10160297239294086
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3672.376513221911
    mean_inference_ms: 2.6467291927193055
    mean_raw_obs_processing_ms: 253.14528130441872
  time_since_restore: 95113.84061551094
  time_this_iter_s: 1772.9774825572968
  time_total_s: 95113.84061551094
  timers:
    learn_throughput: 710.525
    learn_time_ms: 2660.004
    load_throughput: 168892.39
    load_time_ms: 11.191
    sample_throughput: 1.192
    sample_time_ms: 1585148.921
    update_time_ms: 28.79
  timestamp: 1632104411
  timesteps_since_restore: 0
  timesteps_total: 115290
  training_iteration: 61
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     61 |          95113.8 | 115290 |  4.70879 |              7.48604 |               1.6663 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 117180
  custom_metrics: {}
  date: 2021-09-19_19-29-55
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.70008674011165
  episode_reward_mean: 4.724983973698717
  episode_reward_min: 1.4652306418361127
  episodes_this_iter: 270
  episodes_total: 16740
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.84100008988753e-05
          entropy: 12.667407035827637
          entropy_coeff: 0.0004390121903270483
          kl: 0.01579209417104721
          model: {}
          policy_loss: -0.16150502860546112
          total_loss: -0.10191213339567184
          vf_explained_var: 0.991227924823761
          vf_loss: 0.02917768992483616
    num_agent_steps_sampled: 117180
    num_agent_steps_trained: 117180
    num_steps_sampled: 117180
    num_steps_trained: 117180
  iterations_since_restore: 62
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.736172839506175
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10157875030469453
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3653.8032351270563
    mean_inference_ms: 2.639746213769159
    mean_raw_obs_processing_ms: 252.31681056620744
  time_since_restore: 95697.6794013977
  time_this_iter_s: 583.8387858867645
  time_total_s: 95697.6794013977
  timers:
    learn_throughput: 709.141
    learn_time_ms: 2665.196
    load_throughput: 171207.174
    load_time_ms: 11.039
    sample_throughput: 1.284
    sample_time_ms: 1471774.773
    update_time_ms: 28.736
  timestamp: 1632104995
  timesteps_since_restore: 0
  timesteps_total: 117180
  training_iteration: 62
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     62 |          95697.7 | 117180 |  4.72498 |              7.70009 |              1.46523 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 119070
  custom_metrics: {}
  date: 2021-09-19_19-46-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.778134783836207
  episode_reward_mean: 4.574933879812821
  episode_reward_min: 1.8877603300253292
  episodes_this_iter: 270
  episodes_total: 17010
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.821999654173851e-05
          entropy: 12.67469310760498
          entropy_coeff: 0.00043801238643936813
          kl: 0.014916068874299526
          model: {}
          policy_loss: -0.19596552848815918
          total_loss: -0.14218902587890625
          vf_explained_var: 0.9917563796043396
          vf_loss: 0.025347502902150154
    num_agent_steps_sampled: 119070
    num_agent_steps_trained: 119070
    num_steps_sampled: 119070
    num_steps_trained: 119070
  iterations_since_restore: 63
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.3479686386315
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10157107286848062
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3644.304109690729
    mean_inference_ms: 2.6327886445816016
    mean_raw_obs_processing_ms: 251.52549024626475
  time_since_restore: 96707.34774565697
  time_this_iter_s: 1009.6683442592621
  time_total_s: 96707.34774565697
  timers:
    learn_throughput: 706.147
    learn_time_ms: 2676.496
    load_throughput: 170834.895
    load_time_ms: 11.063
    sample_throughput: 1.27
    sample_time_ms: 1488713.811
    update_time_ms: 28.892
  timestamp: 1632106005
  timesteps_since_restore: 0
  timesteps_total: 119070
  training_iteration: 63
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     63 |          96707.3 | 119070 |  4.57493 |              7.77813 |              1.88776 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 120960
  custom_metrics: {}
  date: 2021-09-19_19-56-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.780380189440498
  episode_reward_mean: 4.717114837273757
  episode_reward_min: 2.248234035894265
  episodes_this_iter: 270
  episodes_total: 17280
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.802999946055934e-05
          entropy: 12.627745628356934
          entropy_coeff: 0.0004370126116555184
          kl: 0.014883860014379025
          model: {}
          policy_loss: -0.1625674068927765
          total_loss: -0.10692363232374191
          vf_explained_var: 0.9916814565658569
          vf_loss: 0.02725495770573616
    num_agent_steps_sampled: 120960
    num_agent_steps_trained: 120960
    num_steps_sampled: 120960
    num_steps_trained: 120960
  iterations_since_restore: 64
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.960189573459715
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10157326475783154
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3621.0881683177618
    mean_inference_ms: 2.6261939070778895
    mean_raw_obs_processing_ms: 250.73870660016448
  time_since_restore: 97314.42593407631
  time_this_iter_s: 607.078188419342
  time_total_s: 97314.42593407631
  timers:
    learn_throughput: 704.295
    learn_time_ms: 2683.535
    load_throughput: 170830.846
    load_time_ms: 11.064
    sample_throughput: 1.294
    sample_time_ms: 1460266.343
    update_time_ms: 29.283
  timestamp: 1632106612
  timesteps_since_restore: 0
  timesteps_total: 120960
  training_iteration: 64
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     64 |          97314.4 | 120960 |  4.71711 |              7.78038 |              2.24823 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 122850
  custom_metrics: {}
  date: 2021-09-19_20-37-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.885058199280795
  episode_reward_mean: 4.695100138952651
  episode_reward_min: 1.381888171436935
  episodes_this_iter: 270
  episodes_total: 17550
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.784000237938017e-05
          entropy: 12.626996040344238
          entropy_coeff: 0.00043601280776783824
          kl: 0.01662670448422432
          model: {}
          policy_loss: -0.17616891860961914
          total_loss: -0.11646433174610138
          vf_explained_var: 0.9915903210639954
          vf_loss: 0.027332399040460587
    num_agent_steps_sampled: 122850
    num_agent_steps_trained: 122850
    num_steps_sampled: 122850
    num_steps_trained: 122850
  iterations_since_restore: 65
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.35792269105931
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1015240706898078
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3646.158096033
    mean_inference_ms: 2.61947891453095
    mean_raw_obs_processing_ms: 249.97663239509578
  time_since_restore: 99753.6371011734
  time_this_iter_s: 2439.2111670970917
  time_total_s: 99753.6371011734
  timers:
    learn_throughput: 702.936
    learn_time_ms: 2688.723
    load_throughput: 169192.662
    load_time_ms: 11.171
    sample_throughput: 1.212
    sample_time_ms: 1559783.793
    update_time_ms: 29.869
  timestamp: 1632109051
  timesteps_since_restore: 0
  timesteps_total: 122850
  training_iteration: 65
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     65 |          99753.6 | 122850 |   4.6951 |              7.88506 |              1.38189 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 124740
  custom_metrics: {}
  date: 2021-09-19_20-47-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.776545334369775
  episode_reward_mean: 4.67988279437859
  episode_reward_min: 1.4930650104279655
  episodes_this_iter: 270
  episodes_total: 17820
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.764999802224338e-05
          entropy: 12.57252025604248
          entropy_coeff: 0.00043501300388015807
          kl: 0.015103457495570183
          model: {}
          policy_loss: -0.17497192323207855
          total_loss: -0.11923348158597946
          vf_explained_var: 0.9916548132896423
          vf_loss: 0.026800062507390976
    num_agent_steps_sampled: 124740
    num_agent_steps_trained: 124740
    num_steps_sampled: 124740
    num_steps_trained: 124740
  iterations_since_restore: 66
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.86517967781908
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1015315146536918
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3624.695805088536
    mean_inference_ms: 2.613116020912881
    mean_raw_obs_processing_ms: 249.2464728904319
  time_since_restore: 100334.35441708565
  time_this_iter_s: 580.7173159122467
  time_total_s: 100334.35441708565
  timers:
    learn_throughput: 703.706
    learn_time_ms: 2685.779
    load_throughput: 168836.994
    load_time_ms: 11.194
    sample_throughput: 1.226
    sample_time_ms: 1541994.231
    update_time_ms: 29.537
  timestamp: 1632109632
  timesteps_since_restore: 0
  timesteps_total: 124740
  training_iteration: 66
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     66 |           100334 | 124740 |  4.67988 |              7.77655 |              1.49307 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 126630
  custom_metrics: {}
  date: 2021-09-19_21-05-49
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.535245147522832
  episode_reward_mean: 4.641785073274424
  episode_reward_min: 1.794124957448143
  episodes_this_iter: 270
  episodes_total: 18090
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.746000094106421e-05
          entropy: 12.586915016174316
          entropy_coeff: 0.0004340131999924779
          kl: 0.015309341251850128
          model: {}
          policy_loss: -0.17352744936943054
          total_loss: -0.11525942385196686
          vf_explained_var: 0.9911580681800842
          vf_loss: 0.02885432355105877
    num_agent_steps_sampled: 126630
    num_agent_steps_trained: 126630
    num_steps_sampled: 126630
    num_steps_trained: 126630
  iterations_since_restore: 67
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.7583762886598
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10149743257648296
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3617.386772447059
    mean_inference_ms: 2.608107375105575
    mean_raw_obs_processing_ms: 248.52909441648399
  time_since_restore: 101451.74478197098
  time_this_iter_s: 1117.3903648853302
  time_total_s: 101451.74478197098
  timers:
    learn_throughput: 704.295
    learn_time_ms: 2683.536
    load_throughput: 169987.21
    load_time_ms: 11.118
    sample_throughput: 1.496
    sample_time_ms: 1263485.176
    update_time_ms: 29.609
  timestamp: 1632110749
  timesteps_since_restore: 0
  timesteps_total: 126630
  training_iteration: 67
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     67 |           101452 | 126630 |  4.64179 |              7.53525 |              1.79412 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 128520
  custom_metrics: {}
  date: 2021-09-19_21-13-28
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.280336137139321
  episode_reward_mean: 4.636588369181512
  episode_reward_min: 0.9307091673568382
  episodes_this_iter: 270
  episodes_total: 18360
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.726999658392742e-05
          entropy: 12.506512641906738
          entropy_coeff: 0.0004330133961047977
          kl: 0.015318485908210278
          model: {}
          policy_loss: -0.17748214304447174
          total_loss: -0.12231159955263138
          vf_explained_var: 0.9919163584709167
          vf_loss: 0.025688599795103073
    num_agent_steps_sampled: 128520
    num_agent_steps_trained: 128520
    num_steps_sampled: 128520
    num_steps_trained: 128520
  iterations_since_restore: 68
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.168445839874416
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10146782654052915
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3597.1774554429608
    mean_inference_ms: 2.6026957117808394
    mean_raw_obs_processing_ms: 247.83835545426032
  time_since_restore: 101911.01854801178
  time_this_iter_s: 459.273766040802
  time_total_s: 101911.01854801178
  timers:
    learn_throughput: 703.87
    learn_time_ms: 2685.155
    load_throughput: 172031.627
    load_time_ms: 10.986
    sample_throughput: 1.721
    sample_time_ms: 1098315.744
    update_time_ms: 29.549
  timestamp: 1632111208
  timesteps_since_restore: 0
  timesteps_total: 128520
  training_iteration: 68
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     68 |           101911 | 128520 |  4.63659 |              7.28034 |             0.930709 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 130410
  custom_metrics: {}
  date: 2021-09-19_21-35-51
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.512193329574657
  episode_reward_mean: 4.68192327238674
  episode_reward_min: 2.05520363186264
  episodes_this_iter: 270
  episodes_total: 18630
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.707999950274825e-05
          entropy: 12.549787521362305
          entropy_coeff: 0.00043201359221711755
          kl: 0.014296025037765503
          model: {}
          policy_loss: -0.17987583577632904
          total_loss: -0.12754282355308533
          vf_explained_var: 0.9919847846031189
          vf_loss: 0.02518654055893421
    num_agent_steps_sampled: 130410
    num_agent_steps_trained: 130410
    num_steps_sampled: 130410
    num_steps_trained: 130410
  iterations_since_restore: 69
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.48236870310826
    ram_util_percent: 6.599249732047158
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10146987338210199
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3584.736206141694
    mean_inference_ms: 2.597765223243664
    mean_raw_obs_processing_ms: 247.1442615658501
  time_since_restore: 103253.64000225067
  time_this_iter_s: 1342.6214542388916
  time_total_s: 103253.64000225067
  timers:
    learn_throughput: 703.899
    learn_time_ms: 2685.045
    load_throughput: 171311.14
    load_time_ms: 11.033
    sample_throughput: 1.803
    sample_time_ms: 1048135.131
    update_time_ms: 29.734
  timestamp: 1632112551
  timesteps_since_restore: 0
  timesteps_total: 130410
  training_iteration: 69
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     69 |           103254 | 130410 |  4.68192 |              7.51219 |               2.0552 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 132300
  custom_metrics: {}
  date: 2021-09-19_21-48-07
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.8480457183229815
  episode_reward_mean: 4.695706551079889
  episode_reward_min: 1.998525995697627
  episodes_this_iter: 270
  episodes_total: 18900
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.689000242156908e-05
          entropy: 12.532017707824707
          entropy_coeff: 0.0004310137883294374
          kl: 0.014921597205102444
          model: {}
          policy_loss: -0.1809220314025879
          total_loss: -0.12820535898208618
          vf_explained_var: 0.9924187064170837
          vf_loss: 0.02412487007677555
    num_agent_steps_sampled: 132300
    num_agent_steps_trained: 132300
    num_steps_sampled: 132300
    num_steps_trained: 132300
  iterations_since_restore: 70
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.78170254403131
    ram_util_percent: 6.599999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1014623437817308
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3571.3550381209134
    mean_inference_ms: 2.5926026265699584
    mean_raw_obs_processing_ms: 246.479371387313
  time_since_restore: 103989.1533010006
  time_this_iter_s: 735.5132987499237
  time_total_s: 103989.1533010006
  timers:
    learn_throughput: 703.887
    learn_time_ms: 2685.091
    load_throughput: 169093.415
    load_time_ms: 11.177
    sample_throughput: 1.78
    sample_time_ms: 1061973.859
    update_time_ms: 30.395
  timestamp: 1632113287
  timesteps_since_restore: 0
  timesteps_total: 132300
  training_iteration: 70
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     70 |           103989 | 132300 |  4.69571 |              7.84805 |              1.99853 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 134190
  custom_metrics: {}
  date: 2021-09-19_22-16-29
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.855408323459684
  episode_reward_mean: 4.728407021178361
  episode_reward_min: 1.5871313908260452
  episodes_this_iter: 270
  episodes_total: 19170
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.669999806443229e-05
          entropy: 12.556503295898438
          entropy_coeff: 0.00043001401354558766
          kl: 0.01596609130501747
          model: {}
          policy_loss: -0.17966921627521515
          total_loss: -0.1213398352265358
          vf_explained_var: 0.9917851090431213
          vf_loss: 0.027356097474694252
    num_agent_steps_sampled: 134190
    num_agent_steps_trained: 134190
    num_steps_sampled: 134190
    num_steps_trained: 134190
  iterations_since_restore: 71
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.283805496828755
    ram_util_percent: 6.6000000000000005
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10145115137943314
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3575.0256008856036
    mean_inference_ms: 2.588169313377893
    mean_raw_obs_processing_ms: 245.82965935849617
  time_since_restore: 105691.47462677956
  time_this_iter_s: 1702.3213257789612
  time_total_s: 105691.47462677956
  timers:
    learn_throughput: 703.429
    learn_time_ms: 2686.838
    load_throughput: 170034.61
    load_time_ms: 11.115
    sample_throughput: 1.792
    sample_time_ms: 1054906.392
    update_time_ms: 30.549
  timestamp: 1632114989
  timesteps_since_restore: 0
  timesteps_total: 134190
  training_iteration: 71
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     71 |           105691 | 134190 |  4.72841 |              7.85541 |              1.58713 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 136080
  custom_metrics: {}
  date: 2021-09-20_00-08-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.406839335262414
  episode_reward_mean: 4.717593719472619
  episode_reward_min: 1.6971087114648182
  episodes_this_iter: 270
  episodes_total: 19440
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.651000098325312e-05
          entropy: 12.624353408813477
          entropy_coeff: 0.0004290142096579075
          kl: 0.015859799459576607
          model: {}
          policy_loss: -0.1769140213727951
          total_loss: -0.12277071177959442
          vf_explained_var: 0.9932141304016113
          vf_loss: 0.023428739979863167
    num_agent_steps_sampled: 136080
    num_agent_steps_trained: 136080
    num_steps_sampled: 136080
    num_steps_trained: 136080
  iterations_since_restore: 72
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.94911943643931
    ram_util_percent: 6.599999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1014204735544658
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3685.370005727432
    mean_inference_ms: 2.582943768979642
    mean_raw_obs_processing_ms: 245.22320744189923
  time_since_restore: 112428.72668290138
  time_this_iter_s: 6737.252056121826
  time_total_s: 112428.72668290138
  timers:
    learn_throughput: 703.69
    learn_time_ms: 2685.843
    load_throughput: 171080.809
    load_time_ms: 11.047
    sample_throughput: 1.132
    sample_time_ms: 1670248.57
    update_time_ms: 30.596
  timestamp: 1632121726
  timesteps_since_restore: 0
  timesteps_total: 136080
  training_iteration: 72
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     72 |           112429 | 136080 |  4.71759 |              7.40684 |              1.69711 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 137970
  custom_metrics: {}
  date: 2021-09-20_00-20-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.492627839131389
  episode_reward_mean: 4.593700265306557
  episode_reward_min: 1.5721777411775792
  episodes_this_iter: 270
  episodes_total: 19710
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.631999662611634e-05
          entropy: 12.667580604553223
          entropy_coeff: 0.0004280144057702273
          kl: 0.014893543906509876
          model: {}
          policy_loss: -0.19343219697475433
          total_loss: -0.1402459591627121
          vf_explained_var: 0.9921236038208008
          vf_loss: 0.024678749963641167
    num_agent_steps_sampled: 137970
    num_agent_steps_trained: 137970
    num_steps_sampled: 137970
    num_steps_trained: 137970
  iterations_since_restore: 73
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.47206349206349
    ram_util_percent: 6.599999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1014251159312201
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3675.8523649423755
    mean_inference_ms: 2.5781134693141197
    mean_raw_obs_processing_ms: 244.62897071390907
  time_since_restore: 113108.16955208778
  time_this_iter_s: 679.4428691864014
  time_total_s: 113108.16955208778
  timers:
    learn_throughput: 704.578
    learn_time_ms: 2682.457
    load_throughput: 172343.175
    load_time_ms: 10.966
    sample_throughput: 1.154
    sample_time_ms: 1637229.275
    update_time_ms: 30.863
  timestamp: 1632122406
  timesteps_since_restore: 0
  timesteps_total: 137970
  training_iteration: 73
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     73 |           113108 | 137970 |   4.5937 |              7.49263 |              1.57218 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 139860
  custom_metrics: {}
  date: 2021-09-20_00-34-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.363612453912035
  episode_reward_mean: 4.655782180859582
  episode_reward_min: 1.725989561629784
  episodes_this_iter: 270
  episodes_total: 19980
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.612999954493716e-05
          entropy: 12.649744987487793
          entropy_coeff: 0.00042701460188254714
          kl: 0.01453848835080862
          model: {}
          policy_loss: -0.17690758407115936
          total_loss: -0.12411533296108246
          vf_explained_var: 0.992207944393158
          vf_loss: 0.025073427706956863
    num_agent_steps_sampled: 139860
    num_agent_steps_trained: 139860
    num_steps_sampled: 139860
    num_steps_trained: 139860
  iterations_since_restore: 74
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.074539363484085
    ram_util_percent: 6.6000000000000005
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10137969440441266
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3673.4188891082176
    mean_inference_ms: 2.572786326476338
    mean_raw_obs_processing_ms: 244.02794456298003
  time_since_restore: 113966.55602383614
  time_this_iter_s: 858.386471748352
  time_total_s: 113966.55602383614
  timers:
    learn_throughput: 704.785
    learn_time_ms: 2681.668
    load_throughput: 171274.497
    load_time_ms: 11.035
    sample_throughput: 1.137
    sample_time_ms: 1662360.95
    update_time_ms: 30.545
  timestamp: 1632123264
  timesteps_since_restore: 0
  timesteps_total: 139860
  training_iteration: 74
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     74 |           113967 | 139860 |  4.65578 |              7.36361 |              1.72599 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 141750
  custom_metrics: {}
  date: 2021-09-20_00-39-09
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.026539177189537
  episode_reward_mean: 4.714897440012594
  episode_reward_min: 1.5988566668702515
  episodes_this_iter: 270
  episodes_total: 20250
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.594000246375799e-05
          entropy: 12.628061294555664
          entropy_coeff: 0.00042601479799486697
          kl: 0.014909184537827969
          model: {}
          policy_loss: -0.18174493312835693
          total_loss: -0.12944276630878448
          vf_explained_var: 0.9928188920021057
          vf_loss: 0.02371690794825554
    num_agent_steps_sampled: 141750
    num_agent_steps_trained: 141750
    num_steps_sampled: 141750
    num_steps_trained: 141750
  iterations_since_restore: 75
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 44.8138888888889
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10134185659006155
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3646.9354786076206
    mean_inference_ms: 2.568416442209702
    mean_raw_obs_processing_ms: 243.4577919780929
  time_since_restore: 114250.9671998024
  time_this_iter_s: 284.4111759662628
  time_total_s: 114250.9671998024
  timers:
    learn_throughput: 705.339
    learn_time_ms: 2679.562
    load_throughput: 173334.264
    load_time_ms: 10.904
    sample_throughput: 1.306
    sample_time_ms: 1446883.509
    update_time_ms: 30.171
  timestamp: 1632123549
  timesteps_since_restore: 0
  timesteps_total: 141750
  training_iteration: 75
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     75 |           114251 | 141750 |   4.7149 |              8.02654 |              1.59886 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 143640
  custom_metrics: {}
  date: 2021-09-20_00-51-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.679580423843022
  episode_reward_mean: 4.769484744345762
  episode_reward_min: 1.8988596795681665
  episodes_this_iter: 270
  episodes_total: 20520
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.57499981066212e-05
          entropy: 12.692221641540527
          entropy_coeff: 0.0004250149941071868
          kl: 0.014833343215286732
          model: {}
          policy_loss: -0.17753982543945312
          total_loss: -0.12457878887653351
          vf_explained_var: 0.9924405217170715
          vf_loss: 0.024563206359744072
    num_agent_steps_sampled: 143640
    num_agent_steps_trained: 143640
    num_steps_sampled: 143640
    num_steps_trained: 143640
  iterations_since_restore: 76
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.91901408450703
    ram_util_percent: 6.599999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10131349731084471
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3632.4764124884064
    mean_inference_ms: 2.5640425269986373
    mean_raw_obs_processing_ms: 242.88942221454317
  time_since_restore: 114965.72920131683
  time_this_iter_s: 714.7620015144348
  time_total_s: 114965.72920131683
  timers:
    learn_throughput: 704.639
    learn_time_ms: 2682.225
    load_throughput: 175243.66
    load_time_ms: 10.785
    sample_throughput: 1.294
    sample_time_ms: 1460285.267
    update_time_ms: 30.188
  timestamp: 1632124264
  timesteps_since_restore: 0
  timesteps_total: 143640
  training_iteration: 76
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     76 |           114966 | 143640 |  4.76948 |              7.67958 |              1.89886 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 145530
  custom_metrics: {}
  date: 2021-09-20_01-11-51
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.944641342909359
  episode_reward_mean: 4.724003654585517
  episode_reward_min: 1.6620786954160542
  episodes_this_iter: 270
  episodes_total: 20790
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.556000102544203e-05
          entropy: 12.64245319366455
          entropy_coeff: 0.0004240151902195066
          kl: 0.014709895476698875
          model: {}
          policy_loss: -0.18059368431568146
          total_loss: -0.12292131036520004
          vf_explained_var: 0.99119633436203
          vf_loss: 0.02952197566628456
    num_agent_steps_sampled: 145530
    num_agent_steps_trained: 145530
    num_steps_sampled: 145530
    num_steps_trained: 145530
  iterations_since_restore: 77
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.329188255613126
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10125872665979842
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3628.477019785411
    mean_inference_ms: 2.5597823667455866
    mean_raw_obs_processing_ms: 242.33341424745805
  time_since_restore: 116213.22154164314
  time_this_iter_s: 1247.4923403263092
  time_total_s: 116213.22154164314
  timers:
    learn_throughput: 704.176
    learn_time_ms: 2683.989
    load_throughput: 175412.73
    load_time_ms: 10.775
    sample_throughput: 1.283
    sample_time_ms: 1473293.551
    update_time_ms: 30.363
  timestamp: 1632125511
  timesteps_since_restore: 0
  timesteps_total: 145530
  training_iteration: 77
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     77 |           116213 | 145530 |    4.724 |              7.94464 |              1.66208 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 147420
  custom_metrics: {}
  date: 2021-09-20_01-20-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.684132977683525
  episode_reward_mean: 4.683896959203887
  episode_reward_min: 1.6365905769021858
  episodes_this_iter: 270
  episodes_total: 21060
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.536999666830525e-05
          entropy: 12.641242980957031
          entropy_coeff: 0.00042301538633182645
          kl: 0.014458718709647655
          model: {}
          policy_loss: -0.1572389304637909
          total_loss: -0.10475663095712662
          vf_explained_var: 0.9922671914100647
          vf_loss: 0.024890972301363945
    num_agent_steps_sampled: 147420
    num_agent_steps_trained: 147420
    num_steps_sampled: 147420
    num_steps_trained: 147420
  iterations_since_restore: 78
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.08175287356322
    ram_util_percent: 6.6
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10119313722883026
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3603.7681741654046
    mean_inference_ms: 2.5551514552676977
    mean_raw_obs_processing_ms: 241.79138835114466
  time_since_restore: 116713.81764006615
  time_this_iter_s: 500.59609842300415
  time_total_s: 116713.81764006615
  timers:
    learn_throughput: 703.697
    learn_time_ms: 2685.817
    load_throughput: 174805.829
    load_time_ms: 10.812
    sample_throughput: 1.279
    sample_time_ms: 1477423.929
    update_time_ms: 30.253
  timestamp: 1632126012
  timesteps_since_restore: 0
  timesteps_total: 147420
  training_iteration: 78
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     78 |           116714 | 147420 |   4.6839 |              7.68413 |              1.63659 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 149310
  custom_metrics: {}
  date: 2021-09-20_02-11-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.412428453667944
  episode_reward_mean: 4.752882499310505
  episode_reward_min: 1.5329764976567795
  episodes_this_iter: 270
  episodes_total: 21330
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.517999958712608e-05
          entropy: 12.652510643005371
          entropy_coeff: 0.00042201561154797673
          kl: 0.014757473953068256
          model: {}
          policy_loss: -0.17438463866710663
          total_loss: -0.12022191286087036
          vf_explained_var: 0.9922392964363098
          vf_loss: 0.025882916525006294
    num_agent_steps_sampled: 149310
    num_agent_steps_trained: 149310
    num_steps_sampled: 149310
    num_steps_trained: 149310
  iterations_since_restore: 79
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.74292332638406
    ram_util_percent: 6.600000000000001
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.101151533853358
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3602.7581214745255
    mean_inference_ms: 2.5506101976676905
    mean_raw_obs_processing_ms: 241.2734208044297
  time_since_restore: 119814.18523573875
  time_this_iter_s: 3100.3675956726074
  time_total_s: 119814.18523573875
  timers:
    learn_throughput: 703.721
    learn_time_ms: 2685.725
    load_throughput: 175693.423
    load_time_ms: 10.757
    sample_throughput: 1.143
    sample_time_ms: 1653198.45
    update_time_ms: 30.176
  timestamp: 1632129112
  timesteps_since_restore: 0
  timesteps_total: 149310
  training_iteration: 79
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     79 |           119814 | 149310 |  4.75288 |              7.41243 |              1.53298 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 151200
  custom_metrics: {}
  date: 2021-09-20_02-25-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.707504381973354
  episode_reward_mean: 4.814264531411655
  episode_reward_min: 1.8207781886930179
  episodes_this_iter: 270
  episodes_total: 21600
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.49900025059469e-05
          entropy: 12.54987907409668
          entropy_coeff: 0.00042101580766029656
          kl: 0.015261645428836346
          model: {}
          policy_loss: -0.17628665268421173
          total_loss: -0.12371654063463211
          vf_explained_var: 0.9930974245071411
          vf_loss: 0.023085853084921837
    num_agent_steps_sampled: 151200
    num_agent_steps_trained: 151200
    num_steps_sampled: 151200
    num_steps_trained: 151200
  iterations_since_restore: 80
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.992825112107624
    ram_util_percent: 6.6000000000000005
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10110057490894517
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3588.9034454116822
    mean_inference_ms: 2.546200413697144
    mean_raw_obs_processing_ms: 240.7762480423307
  time_since_restore: 120614.78116965294
  time_this_iter_s: 800.5959339141846
  time_total_s: 120614.78116965294
  timers:
    learn_throughput: 702.919
    learn_time_ms: 2688.787
    load_throughput: 178700.701
    load_time_ms: 10.576
    sample_throughput: 1.139
    sample_time_ms: 1659704.34
    update_time_ms: 29.761
  timestamp: 1632129913
  timesteps_since_restore: 0
  timesteps_total: 151200
  training_iteration: 80
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     80 |           120615 | 151200 |  4.81426 |               7.7075 |              1.82078 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 153090
  custom_metrics: {}
  date: 2021-09-20_02-50-47
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.961621419692187
  episode_reward_mean: 4.643800943829004
  episode_reward_min: 1.800562245109695
  episodes_this_iter: 270
  episodes_total: 21870
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.479999814881012e-05
          entropy: 12.514769554138184
          entropy_coeff: 0.0004200160037726164
          kl: 0.015148923732340336
          model: {}
          policy_loss: -0.20354725420475006
          total_loss: -0.14684487879276276
          vf_explained_var: 0.991269052028656
          vf_loss: 0.02744765393435955
    num_agent_steps_sampled: 153090
    num_agent_steps_trained: 153090
    num_steps_sampled: 153090
    num_steps_trained: 153090
  iterations_since_restore: 81
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.46177985948477
    ram_util_percent: 6.6
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10103453351265262
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3593.3875297640366
    mean_inference_ms: 2.5422217714045483
    mean_raw_obs_processing_ms: 240.28650065973036
  time_since_restore: 122148.63842487335
  time_this_iter_s: 1533.8572552204132
  time_total_s: 122148.63842487335
  timers:
    learn_throughput: 702.264
    learn_time_ms: 2691.295
    load_throughput: 179526.287
    load_time_ms: 10.528
    sample_throughput: 1.15
    sample_time_ms: 1642855.35
    update_time_ms: 29.85
  timestamp: 1632131447
  timesteps_since_restore: 0
  timesteps_total: 153090
  training_iteration: 81
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     81 |           122149 | 153090 |   4.6438 |              7.96162 |              1.80056 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 154980
  custom_metrics: {}
  date: 2021-09-20_02-57-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.531440419255171
  episode_reward_mean: 4.820220842162566
  episode_reward_min: 1.5602070819149754
  episodes_this_iter: 270
  episodes_total: 22140
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.461000106763095e-05
          entropy: 12.48527717590332
          entropy_coeff: 0.0004190161998849362
          kl: 0.015676597133278847
          model: {}
          policy_loss: -0.1950378566980362
          total_loss: -0.13845522701740265
          vf_explained_var: 0.992434561252594
          vf_loss: 0.0261008869856596
    num_agent_steps_sampled: 154980
    num_agent_steps_trained: 154980
    num_steps_sampled: 154980
    num_steps_trained: 154980
  iterations_since_restore: 82
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 43.158596491228074
    ram_util_percent: 6.6000000000000005
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10098530591942526
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3573.254334316014
    mean_inference_ms: 2.5382497572473874
    mean_raw_obs_processing_ms: 239.81282564779315
  time_since_restore: 122558.11037778854
  time_this_iter_s: 409.47195291519165
  time_total_s: 122558.11037778854
  timers:
    learn_throughput: 701.86
    learn_time_ms: 2692.845
    load_throughput: 179155.856
    load_time_ms: 10.549
    sample_throughput: 1.871
    sample_time_ms: 1010076.017
    update_time_ms: 29.619
  timestamp: 1632131856
  timesteps_since_restore: 0
  timesteps_total: 154980
  training_iteration: 82
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     82 |           122558 | 154980 |  4.82022 |              7.53144 |              1.56021 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 156870
  custom_metrics: {}
  date: 2021-09-20_03-06-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.339212929245507
  episode_reward_mean: 4.763585628959533
  episode_reward_min: 1.7665371661780054
  episodes_this_iter: 270
  episodes_total: 22410
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.441999671049416e-05
          entropy: 12.509676933288574
          entropy_coeff: 0.00041801639599725604
          kl: 0.014411726035177708
          model: {}
          policy_loss: -0.17323698103427887
          total_loss: -0.12042666226625443
          vf_explained_var: 0.9924545288085938
          vf_loss: 0.02520786039531231
    num_agent_steps_sampled: 156870
    num_agent_steps_trained: 156870
    num_steps_sampled: 156870
    num_steps_trained: 156870
  iterations_since_restore: 83
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.16447368421053
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10093616408939739
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3554.183555095045
    mean_inference_ms: 2.5345460711260186
    mean_raw_obs_processing_ms: 239.35385892005573
  time_since_restore: 123103.91871929169
  time_this_iter_s: 545.8083415031433
  time_total_s: 123103.91871929169
  timers:
    learn_throughput: 701.777
    learn_time_ms: 2693.163
    load_throughput: 179147.353
    load_time_ms: 10.55
    sample_throughput: 1.896
    sample_time_ms: 996712.057
    update_time_ms: 29.353
  timestamp: 1632132402
  timesteps_since_restore: 0
  timesteps_total: 156870
  training_iteration: 83
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     83 |           123104 | 156870 |  4.76359 |              7.33921 |              1.76654 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 158760
  custom_metrics: {}
  date: 2021-09-20_03-20-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.404725721462477
  episode_reward_mean: 4.700100211602841
  episode_reward_min: 2.0112662483597687
  episodes_this_iter: 270
  episodes_total: 22680
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.422999962931499e-05
          entropy: 12.489410400390625
          entropy_coeff: 0.00041701659210957587
          kl: 0.017345910891890526
          model: {}
          policy_loss: -0.18299749493598938
          total_loss: -0.12203235924243927
          vf_explained_var: 0.9915176630020142
          vf_loss: 0.02665727213025093
    num_agent_steps_sampled: 158760
    num_agent_steps_trained: 158760
    num_steps_sampled: 158760
    num_steps_trained: 158760
  iterations_since_restore: 84
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.037866666666666
    ram_util_percent: 6.600000000000001
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10087875629815053
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3549.446461731058
    mean_inference_ms: 2.530604844171498
    mean_raw_obs_processing_ms: 238.90253897261528
  time_since_restore: 123911.72223424911
  time_this_iter_s: 807.803514957428
  time_total_s: 123911.72223424911
  timers:
    learn_throughput: 702.599
    learn_time_ms: 2690.014
    load_throughput: 180850.008
    load_time_ms: 10.451
    sample_throughput: 1.906
    sample_time_ms: 991656.472
    update_time_ms: 29.489
  timestamp: 1632133210
  timesteps_since_restore: 0
  timesteps_total: 158760
  training_iteration: 84
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     84 |           123912 | 158760 |   4.7001 |              7.40473 |              2.01127 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 160650
  custom_metrics: {}
  date: 2021-09-20_03-26-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.688910359423443
  episode_reward_mean: 4.634267276949566
  episode_reward_min: 1.451581822044171
  episodes_this_iter: 270
  episodes_total: 22950
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.404000254813582e-05
          entropy: 12.51148509979248
          entropy_coeff: 0.0004160167882218957
          kl: 0.014886935241520405
          model: {}
          policy_loss: -0.17927682399749756
          total_loss: -0.1256798952817917
          vf_explained_var: 0.9921703934669495
          vf_loss: 0.024887626990675926
    num_agent_steps_sampled: 160650
    num_agent_steps_trained: 160650
    num_steps_sampled: 160650
    num_steps_trained: 160650
  iterations_since_restore: 85
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.216636197440586
    ram_util_percent: 6.600000000000001
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10082331945466347
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3528.7514230343972
    mean_inference_ms: 2.527029865671903
    mean_raw_obs_processing_ms: 238.45615065186408
  time_since_restore: 124304.08498263359
  time_this_iter_s: 392.3627483844757
  time_total_s: 124304.08498263359
  timers:
    learn_throughput: 702.417
    learn_time_ms: 2690.711
    load_throughput: 181256.907
    load_time_ms: 10.427
    sample_throughput: 1.885
    sample_time_ms: 1002450.922
    update_time_ms: 29.32
  timestamp: 1632133602
  timesteps_since_restore: 0
  timesteps_total: 160650
  training_iteration: 85
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     85 |           124304 | 160650 |  4.63427 |              7.68891 |              1.45158 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 162540
  custom_metrics: {}
  date: 2021-09-20_03-38-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.286320908914064
  episode_reward_mean: 4.709189920508437
  episode_reward_min: 2.191798966778263
  episodes_this_iter: 270
  episodes_total: 23220
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.384999819099903e-05
          entropy: 12.501176834106445
          entropy_coeff: 0.000415017013438046
          kl: 0.014878357760608196
          model: {}
          policy_loss: -0.17592351138591766
          total_loss: -0.12356967478990555
          vf_explained_var: 0.9926894903182983
          vf_loss: 0.02364729531109333
    num_agent_steps_sampled: 162540
    num_agent_steps_trained: 162540
    num_steps_sampled: 162540
    num_steps_trained: 162540
  iterations_since_restore: 86
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 43.078403275332654
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10077347380968034
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3519.27211111915
    mean_inference_ms: 2.523709098296505
    mean_raw_obs_processing_ms: 238.00652399615646
  time_since_restore: 125005.76737499237
  time_this_iter_s: 701.6823923587799
  time_total_s: 125005.76737499237
  timers:
    learn_throughput: 702.518
    learn_time_ms: 2690.321
    load_throughput: 178989.599
    load_time_ms: 10.559
    sample_throughput: 1.888
    sample_time_ms: 1001143.142
    update_time_ms: 29.283
  timestamp: 1632134304
  timesteps_since_restore: 0
  timesteps_total: 162540
  training_iteration: 86
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     86 |           125006 | 162540 |  4.70919 |              7.28632 |               2.1918 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 164430
  custom_metrics: {}
  date: 2021-09-20_04-02-14
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.023427028588234
  episode_reward_mean: 4.730082265133463
  episode_reward_min: 1.9826931284686786
  episodes_this_iter: 270
  episodes_total: 23490
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.366000110981986e-05
          entropy: 12.475786209106445
          entropy_coeff: 0.0004140172095503658
          kl: 0.016079548746347427
          model: {}
          policy_loss: -0.18375994265079498
          total_loss: -0.1292116492986679
          vf_explained_var: 0.9929117560386658
          vf_loss: 0.023082228377461433
    num_agent_steps_sampled: 164430
    num_agent_steps_trained: 164430
    num_steps_sampled: 164430
    num_steps_trained: 164430
  iterations_since_restore: 87
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.7245983935743
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10081340949856202
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3518.7011202346644
    mean_inference_ms: 2.5202718953275274
    mean_raw_obs_processing_ms: 237.5749046762752
  time_since_restore: 126435.99886655807
  time_this_iter_s: 1430.2314915657043
  time_total_s: 126435.99886655807
  timers:
    learn_throughput: 702.642
    learn_time_ms: 2689.848
    load_throughput: 178866.824
    load_time_ms: 10.567
    sample_throughput: 1.854
    sample_time_ms: 1019417.49
    update_time_ms: 28.937
  timestamp: 1632135734
  timesteps_since_restore: 0
  timesteps_total: 164430
  training_iteration: 87
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     87 |           126436 | 164430 |  4.73008 |              8.02343 |              1.98269 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 166320
  custom_metrics: {}
  date: 2021-09-20_04-18-32
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.694581484416164
  episode_reward_mean: 4.682261717738698
  episode_reward_min: 1.3259383752822176
  episodes_this_iter: 270
  episodes_total: 23760
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.346999675268307e-05
          entropy: 12.451231002807617
          entropy_coeff: 0.00041301740566268563
          kl: 0.01453785877674818
          model: {}
          policy_loss: -0.17801430821418762
          total_loss: -0.12217211723327637
          vf_explained_var: 0.9913374185562134
          vf_loss: 0.027865737676620483
    num_agent_steps_sampled: 166320
    num_agent_steps_trained: 166320
    num_steps_sampled: 166320
    num_steps_trained: 166320
  iterations_since_restore: 88
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.42975753122704
    ram_util_percent: 6.6000000000000005
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10078619730404172
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3500.216599358109
    mean_inference_ms: 2.5168985643146495
    mean_raw_obs_processing_ms: 237.15688425223192
  time_since_restore: 127413.46435236931
  time_this_iter_s: 977.4654858112335
  time_total_s: 127413.46435236931
  timers:
    learn_throughput: 702.779
    learn_time_ms: 2689.325
    load_throughput: 179595.024
    load_time_ms: 10.524
    sample_throughput: 1.771
    sample_time_ms: 1067104.66
    update_time_ms: 29.098
  timestamp: 1632136712
  timesteps_since_restore: 0
  timesteps_total: 166320
  training_iteration: 88
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     88 |           127413 | 166320 |  4.68226 |              7.69458 |              1.32594 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 168210
  custom_metrics: {}
  date: 2021-09-20_04-31-19
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.550484814222968
  episode_reward_mean: 4.75515524147526
  episode_reward_min: 2.2283498505650043
  episodes_this_iter: 270
  episodes_total: 24030
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.32799996715039e-05
          entropy: 12.411152839660645
          entropy_coeff: 0.00041201760177500546
          kl: 0.015728900209069252
          model: {}
          policy_loss: -0.18952324986457825
          total_loss: -0.13295143842697144
          vf_explained_var: 0.9923952221870422
          vf_loss: 0.02585303783416748
    num_agent_steps_sampled: 168210
    num_agent_steps_trained: 168210
    num_steps_sampled: 168210
    num_steps_trained: 168210
  iterations_since_restore: 89
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.80842696629214
    ram_util_percent: 6.6
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10078856127082747
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3500.1041430424866
    mean_inference_ms: 2.513197656657699
    mean_raw_obs_processing_ms: 236.74626511005565
  time_since_restore: 128180.31710243225
  time_this_iter_s: 766.8527500629425
  time_total_s: 128180.31710243225
  timers:
    learn_throughput: 702.914
    learn_time_ms: 2688.806
    load_throughput: 179643.456
    load_time_ms: 10.521
    sample_throughput: 2.267
    sample_time_ms: 833754.284
    update_time_ms: 29.06
  timestamp: 1632137479
  timesteps_since_restore: 0
  timesteps_total: 168210
  training_iteration: 89
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     89 |           128180 | 168210 |  4.75516 |              7.55048 |              2.22835 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 170100
  custom_metrics: {}
  date: 2021-09-20_04-37-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.810056651627648
  episode_reward_mean: 4.718995537942216
  episode_reward_min: 1.6351764495298118
  episodes_this_iter: 270
  episodes_total: 24300
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.309000259032473e-05
          entropy: 12.385110855102539
          entropy_coeff: 0.0004110177978873253
          kl: 0.014749428257346153
          model: {}
          policy_loss: -0.18612027168273926
          total_loss: -0.13666307926177979
          vf_explained_var: 0.9935889840126038
          vf_loss: 0.02094666101038456
    num_agent_steps_sampled: 170100
    num_agent_steps_trained: 170100
    num_steps_sampled: 170100
    num_steps_trained: 170100
  iterations_since_restore: 90
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.312382739212005
    ram_util_percent: 6.6
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10078763052850483
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3481.2434069346405
    mean_inference_ms: 2.5097238498241476
    mean_raw_obs_processing_ms: 236.3381524931496
  time_since_restore: 128563.25856685638
  time_this_iter_s: 382.9414644241333
  time_total_s: 128563.25856685638
  timers:
    learn_throughput: 703.692
    learn_time_ms: 2685.835
    load_throughput: 179609.673
    load_time_ms: 10.523
    sample_throughput: 2.386
    sample_time_ms: 791991.726
    update_time_ms: 29.035
  timestamp: 1632137862
  timesteps_since_restore: 0
  timesteps_total: 170100
  training_iteration: 90
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     90 |           128563 | 170100 |    4.719 |              7.81006 |              1.63518 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 171990
  custom_metrics: {}
  date: 2021-09-20_04-52-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.825948170926079
  episode_reward_mean: 4.742001793428528
  episode_reward_min: 1.7797507667764296
  episodes_this_iter: 270
  episodes_total: 24570
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.289999823318794e-05
          entropy: 12.394807815551758
          entropy_coeff: 0.0004100179939996451
          kl: 0.015593861229717731
          model: {}
          policy_loss: -0.19822123646736145
          total_loss: -0.14381039142608643
          vf_explained_var: 0.9925912618637085
          vf_loss: 0.023968160152435303
    num_agent_steps_sampled: 171990
    num_agent_steps_trained: 171990
    num_steps_sampled: 171990
    num_steps_trained: 171990
  iterations_since_restore: 91
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.145500000000006
    ram_util_percent: 6.6
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10077874454451195
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3459.153291990381
    mean_inference_ms: 2.5067175655323433
    mean_raw_obs_processing_ms: 235.94559195081482
  time_since_restore: 129425.1116592884
  time_this_iter_s: 861.8530924320221
  time_total_s: 129425.1116592884
  timers:
    learn_throughput: 703.501
    learn_time_ms: 2686.565
    load_throughput: 178138.501
    load_time_ms: 10.61
    sample_throughput: 2.608
    sample_time_ms: 724790.49
    update_time_ms: 29.079
  timestamp: 1632138724
  timesteps_since_restore: 0
  timesteps_total: 171990
  training_iteration: 91
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     91 |           129425 | 171990 |    4.742 |              7.82595 |              1.77975 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 173880
  custom_metrics: {}
  date: 2021-09-20_05-06-09
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.750010515227893
  episode_reward_mean: 4.730994150460137
  episode_reward_min: 1.7860800094316438
  episodes_this_iter: 270
  episodes_total: 24840
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.271000115200877e-05
          entropy: 12.361159324645996
          entropy_coeff: 0.00040901819011196494
          kl: 0.016473354771733284
          model: {}
          policy_loss: -0.19746346771717072
          total_loss: -0.14043529331684113
          vf_explained_var: 0.9925956130027771
          vf_loss: 0.024555753916502
    num_agent_steps_sampled: 173880
    num_agent_steps_trained: 173880
    num_steps_sampled: 173880
    num_steps_trained: 173880
  iterations_since_restore: 92
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.410714285714285
    ram_util_percent: 6.6000000000000005
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10076894865081772
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3447.4107468617676
    mean_inference_ms: 2.503797860884466
    mean_raw_obs_processing_ms: 235.5527862311745
  time_since_restore: 130270.33700299263
  time_this_iter_s: 845.2253437042236
  time_total_s: 130270.33700299263
  timers:
    learn_throughput: 703.615
    learn_time_ms: 2686.127
    load_throughput: 178231.421
    load_time_ms: 10.604
    sample_throughput: 2.46
    sample_time_ms: 768366.184
    update_time_ms: 29.147
  timestamp: 1632139569
  timesteps_since_restore: 0
  timesteps_total: 173880
  training_iteration: 92
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     92 |           130270 | 173880 |  4.73099 |              7.75001 |              1.78608 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 175770
  custom_metrics: {}
  date: 2021-09-20_05-19-15
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.777009232379203
  episode_reward_mean: 4.7073096372605265
  episode_reward_min: 1.9101172590995295
  episodes_this_iter: 270
  episodes_total: 25110
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.251999679487199e-05
          entropy: 12.314671516418457
          entropy_coeff: 0.00040801838622428477
          kl: 0.01408771239221096
          model: {}
          policy_loss: -0.19050469994544983
          total_loss: -0.13906177878379822
          vf_explained_var: 0.9926002621650696
          vf_loss: 0.024373942986130714
    num_agent_steps_sampled: 175770
    num_agent_steps_trained: 175770
    num_steps_sampled: 175770
    num_steps_trained: 175770
  iterations_since_restore: 93
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.59698630136986
    ram_util_percent: 6.6000000000000005
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10074205637920142
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3447.76449006283
    mean_inference_ms: 2.5008829480118044
    mean_raw_obs_processing_ms: 235.17331500853214
  time_since_restore: 131056.24857378006
  time_this_iter_s: 785.9115707874298
  time_total_s: 131056.24857378006
  timers:
    learn_throughput: 703.419
    learn_time_ms: 2686.875
    load_throughput: 179106.068
    load_time_ms: 10.552
    sample_throughput: 2.385
    sample_time_ms: 792376.174
    update_time_ms: 29.207
  timestamp: 1632140355
  timesteps_since_restore: 0
  timesteps_total: 175770
  training_iteration: 93
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     93 |           131056 | 175770 |  4.70731 |              7.77701 |              1.91012 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 177660
  custom_metrics: {}
  date: 2021-09-20_05-42-35
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.572167226848547
  episode_reward_mean: 4.719624173176899
  episode_reward_min: 2.0253270867968323
  episodes_this_iter: 270
  episodes_total: 25380
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.232999971369281e-05
          entropy: 12.391615867614746
          entropy_coeff: 0.00040701861144043505
          kl: 0.014855971559882164
          model: {}
          policy_loss: -0.17166195809841156
          total_loss: -0.1191563755273819
          vf_explained_var: 0.9926932454109192
          vf_loss: 0.02370542846620083
    num_agent_steps_sampled: 177660
    num_agent_steps_trained: 177660
    num_steps_sampled: 177660
    num_steps_trained: 177660
  iterations_since_restore: 94
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.753490759753596
    ram_util_percent: 6.599999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10072568990358262
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3453.512969163598
    mean_inference_ms: 2.4982364840346465
    mean_raw_obs_processing_ms: 234.80636804852585
  time_since_restore: 132455.73283481598
  time_this_iter_s: 1399.4842610359192
  time_total_s: 132455.73283481598
  timers:
    learn_throughput: 702.504
    learn_time_ms: 2690.376
    load_throughput: 179249.84
    load_time_ms: 10.544
    sample_throughput: 2.22
    sample_time_ms: 851540.921
    update_time_ms: 28.775
  timestamp: 1632141755
  timesteps_since_restore: 0
  timesteps_total: 177660
  training_iteration: 94
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     94 |           132456 | 177660 |  4.71962 |              7.57217 |              2.02533 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 179550
  custom_metrics: {}
  date: 2021-09-20_05-51-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.018522819485391
  episode_reward_mean: 4.767751115769773
  episode_reward_min: 1.864224797646831
  episodes_this_iter: 270
  episodes_total: 25650
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.214000263251364e-05
          entropy: 12.357938766479492
          entropy_coeff: 0.0004060188075527549
          kl: 0.015367050655186176
          model: {}
          policy_loss: -0.1819225549697876
          total_loss: -0.12800583243370056
          vf_explained_var: 0.9927264451980591
          vf_loss: 0.02392621524631977
    num_agent_steps_sampled: 179550
    num_agent_steps_trained: 179550
    num_steps_sampled: 179550
    num_steps_trained: 179550
  iterations_since_restore: 95
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.14242424242424
    ram_util_percent: 6.599586776859503
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10070957484559637
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3434.263357698319
    mean_inference_ms: 2.4956578619509258
    mean_raw_obs_processing_ms: 234.44624757330183
  time_since_restore: 132977.5073544979
  time_this_iter_s: 521.7745196819305
  time_total_s: 132977.5073544979
  timers:
    learn_throughput: 702.344
    learn_time_ms: 2690.987
    load_throughput: 176461.144
    load_time_ms: 10.711
    sample_throughput: 2.186
    sample_time_ms: 864481.332
    update_time_ms: 28.974
  timestamp: 1632142277
  timesteps_since_restore: 0
  timesteps_total: 179550
  training_iteration: 95
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     95 |           132978 | 179550 |  4.76775 |              7.01852 |              1.86422 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 181440
  custom_metrics: {}
  date: 2021-09-20_06-11-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.036025439086021
  episode_reward_mean: 4.811377397408299
  episode_reward_min: 2.2644884156909875
  episodes_this_iter: 270
  episodes_total: 25920
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.194999827537686e-05
          entropy: 12.265589714050293
          entropy_coeff: 0.0004050190036650747
          kl: 0.01420143898576498
          model: {}
          policy_loss: -0.17594002187252045
          total_loss: -0.12464342266321182
          vf_explained_var: 0.9927329421043396
          vf_loss: 0.023911764845252037
    num_agent_steps_sampled: 181440
    num_agent_steps_trained: 181440
    num_steps_sampled: 181440
    num_steps_trained: 181440
  iterations_since_restore: 96
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.717065868263475
    ram_util_percent: 6.599101796407185
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10067150478896138
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3422.7070872448503
    mean_inference_ms: 2.4927588728986527
    mean_raw_obs_processing_ms: 234.08458170312403
  time_since_restore: 134176.7314825058
  time_this_iter_s: 1199.2241280078888
  time_total_s: 134176.7314825058
  timers:
    learn_throughput: 702.725
    learn_time_ms: 2689.53
    load_throughput: 178102.481
    load_time_ms: 10.612
    sample_throughput: 2.067
    sample_time_ms: 914236.983
    update_time_ms: 29.29
  timestamp: 1632143476
  timesteps_since_restore: 0
  timesteps_total: 181440
  training_iteration: 96
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     96 |           134177 | 181440 |  4.81138 |              7.03603 |              2.26449 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 183330
  custom_metrics: {}
  date: 2021-09-20_06-39-38
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.538868571173204
  episode_reward_mean: 4.7133875238678415
  episode_reward_min: 1.6697648287520324
  episodes_this_iter: 270
  episodes_total: 26190
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.176000119419768e-05
          entropy: 12.295951843261719
          entropy_coeff: 0.00040401919977739453
          kl: 0.015622863546013832
          model: {}
          policy_loss: -0.18296442925930023
          total_loss: -0.12833760678768158
          vf_explained_var: 0.9928106665611267
          vf_loss: 0.02400376833975315
    num_agent_steps_sampled: 183330
    num_agent_steps_trained: 183330
    num_steps_sampled: 183330
    num_steps_trained: 183330
  iterations_since_restore: 97
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.34027015618405
    ram_util_percent: 6.6000000000000005
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1006309078976415
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3437.0216035935164
    mean_inference_ms: 2.490109895459734
    mean_raw_obs_processing_ms: 233.73563814718105
  time_since_restore: 135878.50558757782
  time_this_iter_s: 1701.7741050720215
  time_total_s: 135878.50558757782
  timers:
    learn_throughput: 702.741
    learn_time_ms: 2689.469
    load_throughput: 178070.475
    load_time_ms: 10.614
    sample_throughput: 2.008
    sample_time_ms: 941390.941
    update_time_ms: 29.305
  timestamp: 1632145178
  timesteps_since_restore: 0
  timesteps_total: 183330
  training_iteration: 97
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     97 |           135879 | 183330 |  4.71339 |              7.53887 |              1.66976 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 185220
  custom_metrics: {}
  date: 2021-09-20_06-58-38
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.784017712937457
  episode_reward_mean: 4.698772165700551
  episode_reward_min: 1.984894763266127
  episodes_this_iter: 270
  episodes_total: 26460
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.15699968370609e-05
          entropy: 12.294882774353027
          entropy_coeff: 0.00040301939588971436
          kl: 0.0159876998513937
          model: {}
          policy_loss: -0.18684209883213043
          total_loss: -0.1306985467672348
          vf_explained_var: 0.9925380945205688
          vf_loss: 0.024676654487848282
    num_agent_steps_sampled: 185220
    num_agent_steps_trained: 185220
    num_steps_sampled: 185220
    num_steps_trained: 185220
  iterations_since_restore: 98
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.42172544080604
    ram_util_percent: 6.5437027707808575
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10065029625150325
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3433.821639165446
    mean_inference_ms: 2.4873104001817743
    mean_raw_obs_processing_ms: 233.3990844027752
  time_since_restore: 137018.87039637566
  time_this_iter_s: 1140.3648087978363
  time_total_s: 137018.87039637566
  timers:
    learn_throughput: 702.939
    learn_time_ms: 2688.713
    load_throughput: 178792.595
    load_time_ms: 10.571
    sample_throughput: 1.974
    sample_time_ms: 957681.9
    update_time_ms: 28.986
  timestamp: 1632146318
  timesteps_since_restore: 0
  timesteps_total: 185220
  training_iteration: 98
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     98 |           137019 | 185220 |  4.69877 |              7.78402 |              1.98489 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 187110
  custom_metrics: {}
  date: 2021-09-20_07-16-50
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.526048974428369
  episode_reward_mean: 4.755444239793812
  episode_reward_min: 1.8954428367543545
  episodes_this_iter: 270
  episodes_total: 26730
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.137999975588173e-05
          entropy: 12.32016658782959
          entropy_coeff: 0.0004020195920020342
          kl: 0.01734261028468609
          model: {}
          policy_loss: -0.1734490543603897
          total_loss: -0.11227840185165405
          vf_explained_var: 0.9918560981750488
          vf_loss: 0.02661495842039585
    num_agent_steps_sampled: 187110
    num_agent_steps_trained: 187110
    num_steps_sampled: 187110
    num_steps_trained: 187110
  iterations_since_restore: 99
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.93730263157895
    ram_util_percent: 6.555592105263158
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10061784434749552
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3421.9401493534456
    mean_inference_ms: 2.4846406736635944
    mean_raw_obs_processing_ms: 233.06815299227893
  time_since_restore: 138110.57343673706
  time_this_iter_s: 1091.7030403614044
  time_total_s: 138110.57343673706
  timers:
    learn_throughput: 703.194
    learn_time_ms: 2687.736
    load_throughput: 175656.05
    load_time_ms: 10.76
    sample_throughput: 1.909
    sample_time_ms: 990166.915
    update_time_ms: 28.921
  timestamp: 1632147410
  timesteps_since_restore: 0
  timesteps_total: 187110
  training_iteration: 99
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |     99 |           138111 | 187110 |  4.75544 |              7.52605 |              1.89544 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 189000
  custom_metrics: {}
  date: 2021-09-20_07-40-09
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.373186777063166
  episode_reward_mean: 4.8833986959769184
  episode_reward_min: 2.043700370845145
  episodes_this_iter: 270
  episodes_total: 27000
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.119000267470255e-05
          entropy: 12.331743240356445
          entropy_coeff: 0.000401019788114354
          kl: 0.014275035820901394
          model: {}
          policy_loss: -0.169293612241745
          total_loss: -0.11867296695709229
          vf_explained_var: 0.9932589530944824
          vf_loss: 0.02304561622440815
    num_agent_steps_sampled: 189000
    num_agent_steps_trained: 189000
    num_steps_sampled: 189000
    num_steps_trained: 189000
  iterations_since_restore: 100
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.05143737166324
    ram_util_percent: 6.520174537987679
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10057320585837524
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3411.4992701241813
    mean_inference_ms: 2.4817807456265704
    mean_raw_obs_processing_ms: 232.74396247721577
  time_since_restore: 139509.5649344921
  time_this_iter_s: 1398.9914977550507
  time_total_s: 139509.5649344921
  timers:
    learn_throughput: 702.536
    learn_time_ms: 2690.253
    load_throughput: 175330.093
    load_time_ms: 10.78
    sample_throughput: 1.731
    sample_time_ms: 1091769.292
    update_time_ms: 29.08
  timestamp: 1632148809
  timesteps_since_restore: 0
  timesteps_total: 189000
  training_iteration: 100
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    100 |           139510 | 189000 |   4.8834 |              7.37319 |               2.0437 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 190890
  custom_metrics: {}
  date: 2021-09-20_07-58-00
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.921971286525836
  episode_reward_mean: 4.656114922739638
  episode_reward_min: 1.9472539914112386
  episodes_this_iter: 270
  episodes_total: 27270
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.099999831756577e-05
          entropy: 12.290690422058105
          entropy_coeff: 0.0004000200133305043
          kl: 0.01493595726788044
          model: {}
          policy_loss: -0.19370292127132416
          total_loss: -0.1395547091960907
          vf_explained_var: 0.9920200109481812
          vf_loss: 0.02503874897956848
    num_agent_steps_sampled: 190890
    num_agent_steps_trained: 190890
    num_steps_sampled: 190890
    num_steps_trained: 190890
  iterations_since_restore: 101
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.138255033557044
    ram_util_percent: 6.501879194630872
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10055500305599137
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3421.2469578600576
    mean_inference_ms: 2.4792340594020725
    mean_raw_obs_processing_ms: 232.43350717070027
  time_since_restore: 140580.702573061
  time_this_iter_s: 1071.1376385688782
  time_total_s: 140580.702573061
  timers:
    learn_throughput: 703.045
    learn_time_ms: 2688.306
    load_throughput: 176202.273
    load_time_ms: 10.726
    sample_throughput: 1.699
    sample_time_ms: 1112699.8
    update_time_ms: 28.926
  timestamp: 1632149880
  timesteps_since_restore: 0
  timesteps_total: 190890
  training_iteration: 101
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    101 |           140581 | 190890 |  4.65611 |              7.92197 |              1.94725 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 192780
  custom_metrics: {}
  date: 2021-09-20_08-07-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.976721749772309
  episode_reward_mean: 4.7892613840898655
  episode_reward_min: 1.7609228273439446
  episodes_this_iter: 270
  episodes_total: 27540
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.08100012363866e-05
          entropy: 12.290249824523926
          entropy_coeff: 0.0003990202094428241
          kl: 0.014464662410318851
          model: {}
          policy_loss: -0.1812492460012436
          total_loss: -0.1295928806066513
          vf_explained_var: 0.9930748343467712
          vf_loss: 0.023608101531863213
    num_agent_steps_sampled: 192780
    num_agent_steps_trained: 192780
    num_steps_sampled: 192780
    num_steps_trained: 192780
  iterations_since_restore: 102
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.70885922330097
    ram_util_percent: 6.5021844660194175
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10052850456960531
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3413.687720855034
    mean_inference_ms: 2.4765697246211698
    mean_raw_obs_processing_ms: 232.11943002880798
  time_since_restore: 141172.42175459862
  time_this_iter_s: 591.7191815376282
  time_total_s: 141172.42175459862
  timers:
    learn_throughput: 703.827
    learn_time_ms: 2685.317
    load_throughput: 175711.337
    load_time_ms: 10.756
    sample_throughput: 1.738
    sample_time_ms: 1087352.13
    update_time_ms: 28.92
  timestamp: 1632150472
  timesteps_since_restore: 0
  timesteps_total: 192780
  training_iteration: 102
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    102 |           141172 | 192780 |  4.78926 |              7.97672 |              1.76092 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 194670
  custom_metrics: {}
  date: 2021-09-20_08-41-34
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.158372514316454
  episode_reward_mean: 4.794705569694887
  episode_reward_min: 2.268532721124686
  episodes_this_iter: 270
  episodes_total: 27810
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.061999687924981e-05
          entropy: 12.282023429870605
          entropy_coeff: 0.00039802040555514395
          kl: 0.015711771324276924
          model: {}
          policy_loss: -0.1832166463136673
          total_loss: -0.12865863740444183
          vf_explained_var: 0.9929147958755493
          vf_loss: 0.023653116077184677
    num_agent_steps_sampled: 194670
    num_agent_steps_trained: 194670
    num_steps_sampled: 194670
    num_steps_trained: 194670
  iterations_since_restore: 103
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.20347887823927
    ram_util_percent: 6.502733404330847
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10049431167531651
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3421.0777268738098
    mean_inference_ms: 2.474188452760027
    mean_raw_obs_processing_ms: 231.82922818908511
  time_since_restore: 143194.81114912033
  time_this_iter_s: 2022.3893945217133
  time_total_s: 143194.81114912033
  timers:
    learn_throughput: 704.807
    learn_time_ms: 2681.587
    load_throughput: 175309.155
    load_time_ms: 10.781
    sample_throughput: 1.561
    sample_time_ms: 1211002.95
    update_time_ms: 28.941
  timestamp: 1632152494
  timesteps_since_restore: 0
  timesteps_total: 194670
  training_iteration: 103
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    103 |           143195 | 194670 |  4.79471 |              8.15837 |              2.26853 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 196560
  custom_metrics: {}
  date: 2021-09-20_10-05-03
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.584950121324861
  episode_reward_mean: 4.6782520521912145
  episode_reward_min: 1.566505348347506
  episodes_this_iter: 270
  episodes_total: 28080
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.042999979807064e-05
          entropy: 12.234312057495117
          entropy_coeff: 0.0003970206016674638
          kl: 0.015054181218147278
          model: {}
          policy_loss: -0.193117156624794
          total_loss: -0.14095726609230042
          vf_explained_var: 0.9928038716316223
          vf_loss: 0.022721869871020317
    num_agent_steps_sampled: 196560
    num_agent_steps_trained: 196560
    num_steps_sampled: 196560
    num_steps_trained: 196560
  iterations_since_restore: 104
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.09789096126255
    ram_util_percent: 6.5017216642754665
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10047303893611646
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3461.170358413473
    mean_inference_ms: 2.4721027678171117
    mean_raw_obs_processing_ms: 231.53766291449207
  time_since_restore: 148202.93158197403
  time_this_iter_s: 5008.120432853699
  time_total_s: 148202.93158197403
  timers:
    learn_throughput: 705.083
    learn_time_ms: 2680.534
    load_throughput: 159464.482
    load_time_ms: 11.852
    sample_throughput: 1.202
    sample_time_ms: 1571867.003
    update_time_ms: 29.207
  timestamp: 1632157503
  timesteps_since_restore: 0
  timesteps_total: 196560
  training_iteration: 104
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    104 |           148203 | 196560 |  4.67825 |              7.58495 |              1.56651 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 198450
  custom_metrics: {}
  date: 2021-09-20_10-14-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.96913840194797
  episode_reward_mean: 4.796625775426344
  episode_reward_min: 2.181448552330785
  episodes_this_iter: 270
  episodes_total: 28350
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.024000271689147e-05
          entropy: 12.248867988586426
          entropy_coeff: 0.0003960207977797836
          kl: 0.015503866598010063
          model: {}
          policy_loss: -0.1909267157316208
          total_loss: -0.13642863929271698
          vf_explained_var: 0.9927534461021423
          vf_loss: 0.024029141291975975
    num_agent_steps_sampled: 198450
    num_agent_steps_trained: 198450
    num_steps_sampled: 198450
    num_steps_trained: 198450
  iterations_since_restore: 105
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.346419753086415
    ram_util_percent: 6.521481481481481
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10045089489246034
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3451.551774919377
    mean_inference_ms: 2.4698113231772747
    mean_raw_obs_processing_ms: 231.25877825308655
  time_since_restore: 148783.98805618286
  time_this_iter_s: 581.0564742088318
  time_total_s: 148783.98805618286
  timers:
    learn_throughput: 705.519
    learn_time_ms: 2678.878
    load_throughput: 159635.32
    load_time_ms: 11.839
    sample_throughput: 1.198
    sample_time_ms: 1577796.96
    update_time_ms: 28.98
  timestamp: 1632158084
  timesteps_since_restore: 0
  timesteps_total: 198450
  training_iteration: 105
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    105 |           148784 | 198450 |  4.79663 |              7.96914 |              2.18145 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 200340
  custom_metrics: {}
  date: 2021-09-20_10-25-57
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.4796057448269835
  episode_reward_mean: 4.744876836062354
  episode_reward_min: 1.7341753051824522
  episodes_this_iter: 270
  episodes_total: 28620
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.004999835975468e-05
          entropy: 12.224515914916992
          entropy_coeff: 0.00039502099389210343
          kl: 0.01543251983821392
          model: {}
          policy_loss: -0.1771784871816635
          total_loss: -0.12249436229467392
          vf_explained_var: 0.9924386739730835
          vf_loss: 0.024355856701731682
    num_agent_steps_sampled: 200340
    num_agent_steps_trained: 200340
    num_steps_sampled: 200340
    num_steps_trained: 200340
  iterations_since_restore: 106
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.67748132337246
    ram_util_percent: 6.507257203842049
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1004295073210829
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3439.7968893756383
    mean_inference_ms: 2.467429581716005
    mean_raw_obs_processing_ms: 230.98580203667026
  time_since_restore: 149457.45998477936
  time_this_iter_s: 673.4719285964966
  time_total_s: 149457.45998477936
  timers:
    learn_throughput: 705.961
    learn_time_ms: 2677.203
    load_throughput: 159514.861
    load_time_ms: 11.848
    sample_throughput: 1.239
    sample_time_ms: 1525223.776
    update_time_ms: 28.8
  timestamp: 1632158757
  timesteps_since_restore: 0
  timesteps_total: 200340
  training_iteration: 106
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    106 |           149457 | 200340 |  4.74488 |              7.47961 |              1.73418 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 202230
  custom_metrics: {}
  date: 2021-09-20_10-36-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.510351554121212
  episode_reward_mean: 4.780906371846222
  episode_reward_min: 1.8175241190583893
  episodes_this_iter: 270
  episodes_total: 28890
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.986000127857551e-05
          entropy: 12.20064640045166
          entropy_coeff: 0.00039402119000442326
          kl: 0.01571420021355152
          model: {}
          policy_loss: -0.18736182153224945
          total_loss: -0.13345657289028168
          vf_explained_var: 0.9931949973106384
          vf_loss: 0.022913649678230286
    num_agent_steps_sampled: 202230
    num_agent_steps_trained: 202230
    num_steps_sampled: 202230
    num_steps_trained: 202230
  iterations_since_restore: 107
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.44491428571428
    ram_util_percent: 6.2942857142857145
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10040260370362107
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3427.6667016138013
    mean_inference_ms: 2.4650201561146226
    mean_raw_obs_processing_ms: 230.74505118889226
  time_since_restore: 150085.89026141167
  time_this_iter_s: 628.430276632309
  time_total_s: 150085.89026141167
  timers:
    learn_throughput: 707.612
    learn_time_ms: 2670.955
    load_throughput: 159631.784
    load_time_ms: 11.84
    sample_throughput: 1.333
    sample_time_ms: 1417896.101
    update_time_ms: 28.713
  timestamp: 1632159386
  timesteps_since_restore: 0
  timesteps_total: 202230
  training_iteration: 107
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    107 |           150086 | 202230 |  4.78091 |              7.51035 |              1.81752 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 204120
  custom_metrics: {}
  date: 2021-09-20_11-28-29
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.230037280545124
  episode_reward_mean: 4.798526366401799
  episode_reward_min: 1.7390639386912161
  episodes_this_iter: 270
  episodes_total: 29160
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.966999692143872e-05
          entropy: 12.19330883026123
          entropy_coeff: 0.0003930213861167431
          kl: 0.0169844888150692
          model: {}
          policy_loss: -0.17773260176181793
          total_loss: -0.11812051385641098
          vf_explained_var: 0.9922291040420532
          vf_loss: 0.025711530819535255
    num_agent_steps_sampled: 204120
    num_agent_steps_trained: 204120
    num_steps_sampled: 204120
    num_steps_trained: 204120
  iterations_since_restore: 108
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.352922227335476
    ram_util_percent: 6.175701794753797
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10039203534823486
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3432.30986032418
    mean_inference_ms: 2.4640772579693917
    mean_raw_obs_processing_ms: 230.54175421131222
  time_since_restore: 153209.42992568016
  time_this_iter_s: 3123.5396642684937
  time_total_s: 153209.42992568016
  timers:
    learn_throughput: 707.354
    learn_time_ms: 2671.929
    load_throughput: 158365.738
    load_time_ms: 11.934
    sample_throughput: 1.169
    sample_time_ms: 1616212.368
    update_time_ms: 28.905
  timestamp: 1632162509
  timesteps_since_restore: 0
  timesteps_total: 204120
  training_iteration: 108
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    108 |           153209 | 204120 |  4.79853 |              7.23004 |              1.73906 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 206010
  custom_metrics: {}
  date: 2021-09-20_11-37-53
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.693434783570339
  episode_reward_mean: 4.8168482797387835
  episode_reward_min: 1.8757341322737267
  episodes_this_iter: 270
  episodes_total: 29430
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.947999984025955e-05
          entropy: 12.194313049316406
          entropy_coeff: 0.00039202161133289337
          kl: 0.014805154874920845
          model: {}
          policy_loss: -0.18042655289173126
          total_loss: -0.1274893581867218
          vf_explained_var: 0.9929219484329224
          vf_loss: 0.023989643901586533
    num_agent_steps_sampled: 206010
    num_agent_steps_trained: 206010
    num_steps_sampled: 206010
    num_steps_trained: 206010
  iterations_since_restore: 109
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.97436224489796
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10038095181078928
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3416.192927584098
    mean_inference_ms: 2.462256949092881
    mean_raw_obs_processing_ms: 230.30628683021786
  time_since_restore: 153772.87483024597
  time_this_iter_s: 563.4449045658112
  time_total_s: 153772.87483024597
  timers:
    learn_throughput: 707.498
    learn_time_ms: 2671.385
    load_throughput: 160744.316
    load_time_ms: 11.758
    sample_throughput: 1.209
    sample_time_ms: 1563386.921
    update_time_ms: 28.811
  timestamp: 1632163073
  timesteps_since_restore: 0
  timesteps_total: 206010
  training_iteration: 109
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    109 |           153773 | 206010 |  4.81685 |              7.69343 |              1.87573 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 207900
  custom_metrics: {}
  date: 2021-09-20_11-55-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.4615383040892125
  episode_reward_mean: 4.799675451670794
  episode_reward_min: 1.6545266737664348
  episodes_this_iter: 270
  episodes_total: 29700
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.929000275908038e-05
          entropy: 12.159802436828613
          entropy_coeff: 0.0003910218074452132
          kl: 0.014898575842380524
          model: {}
          policy_loss: -0.18176719546318054
          total_loss: -0.12602919340133667
          vf_explained_var: 0.9919717907905579
          vf_loss: 0.02655193582177162
    num_agent_steps_sampled: 207900
    num_agent_steps_trained: 207900
    num_steps_sampled: 207900
    num_steps_trained: 207900
  iterations_since_restore: 110
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.47822853279243
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10038045402341836
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3410.71123787924
    mean_inference_ms: 2.4607673972514874
    mean_raw_obs_processing_ms: 230.08370912791034
  time_since_restore: 154835.95665550232
  time_this_iter_s: 1063.0818252563477
  time_total_s: 154835.95665550232
  timers:
    learn_throughput: 707.064
    learn_time_ms: 2673.024
    load_throughput: 161122.981
    load_time_ms: 11.73
    sample_throughput: 1.235
    sample_time_ms: 1529794.32
    update_time_ms: 29.031
  timestamp: 1632164136
  timesteps_since_restore: 0
  timesteps_total: 207900
  training_iteration: 110
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    110 |           154836 | 207900 |  4.79968 |              7.46154 |              1.65453 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 209790
  custom_metrics: {}
  date: 2021-09-20_12-07-37
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.046638310163878
  episode_reward_mean: 4.853012056088046
  episode_reward_min: 1.9516674164196297
  episodes_this_iter: 270
  episodes_total: 29970
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.90999984019436e-05
          entropy: 12.158838272094727
          entropy_coeff: 0.000390022003557533
          kl: 0.01716499961912632
          model: {}
          policy_loss: -0.1909162551164627
          total_loss: -0.13330256938934326
          vf_explained_var: 0.9928559064865112
          vf_loss: 0.02325189672410488
    num_agent_steps_sampled: 209790
    num_agent_steps_trained: 209790
    num_steps_sampled: 209790
    num_steps_trained: 209790
  iterations_since_restore: 111
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.09681274900398
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10041084575755292
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3398.1405135553705
    mean_inference_ms: 2.4592448388222676
    mean_raw_obs_processing_ms: 229.845694204278
  time_since_restore: 155557.12757897377
  time_this_iter_s: 721.1709234714508
  time_total_s: 155557.12757897377
  timers:
    learn_throughput: 706.273
    learn_time_ms: 2676.02
    load_throughput: 160242.622
    load_time_ms: 11.795
    sample_throughput: 1.264
    sample_time_ms: 1494794.825
    update_time_ms: 29.265
  timestamp: 1632164857
  timesteps_since_restore: 0
  timesteps_total: 209790
  training_iteration: 111
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    111 |           155557 | 209790 |  4.85301 |              8.04664 |              1.95167 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 211680
  custom_metrics: {}
  date: 2021-09-20_12-16-25
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.456374397346734
  episode_reward_mean: 4.8957363087806565
  episode_reward_min: 2.2619703730595355
  episodes_this_iter: 270
  episodes_total: 30240
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.891000132076442e-05
          entropy: 12.153158187866211
          entropy_coeff: 0.00038902219966985285
          kl: 0.01437980867922306
          model: {}
          policy_loss: -0.20155645906925201
          total_loss: -0.1515331119298935
          vf_explained_var: 0.9936271905899048
          vf_loss: 0.02199220284819603
    num_agent_steps_sampled: 211680
    num_agent_steps_trained: 211680
    num_steps_sampled: 211680
    num_steps_trained: 211680
  iterations_since_restore: 112
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.46557823129252
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10040097614496611
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3384.1950494039866
    mean_inference_ms: 2.4576339112055714
    mean_raw_obs_processing_ms: 229.5942473219646
  time_since_restore: 156085.30660557747
  time_this_iter_s: 528.1790266036987
  time_total_s: 156085.30660557747
  timers:
    learn_throughput: 705.467
    learn_time_ms: 2679.078
    load_throughput: 160214.122
    load_time_ms: 11.797
    sample_throughput: 1.27
    sample_time_ms: 1488437.811
    update_time_ms: 29.484
  timestamp: 1632165385
  timesteps_since_restore: 0
  timesteps_total: 211680
  training_iteration: 112
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    112 |           156085 | 211680 |  4.89574 |              7.45637 |              2.26197 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 213570
  custom_metrics: {}
  date: 2021-09-20_12-28-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.837406160651298
  episode_reward_mean: 4.783818458601667
  episode_reward_min: 1.6884343289062018
  episodes_this_iter: 270
  episodes_total: 30510
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.871999696362764e-05
          entropy: 12.117238998413086
          entropy_coeff: 0.0003880223957821727
          kl: 0.01632962003350258
          model: {}
          policy_loss: -0.17381034791469574
          total_loss: -0.11726295202970505
          vf_explained_var: 0.992737352848053
          vf_loss: 0.024048209190368652
    num_agent_steps_sampled: 213570
    num_agent_steps_trained: 213570
    num_steps_sampled: 213570
    num_steps_trained: 213570
  iterations_since_restore: 113
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.25321100917431
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10039989498835751
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3371.176950003501
    mean_inference_ms: 2.4557867443100516
    mean_raw_obs_processing_ms: 229.34641829706405
  time_since_restore: 156790.1958477497
  time_this_iter_s: 704.8892421722412
  time_total_s: 156790.1958477497
  timers:
    learn_throughput: 704.861
    learn_time_ms: 2681.381
    load_throughput: 159828.755
    load_time_ms: 11.825
    sample_throughput: 1.393
    sample_time_ms: 1356685.586
    update_time_ms: 29.417
  timestamp: 1632166090
  timesteps_since_restore: 0
  timesteps_total: 213570
  training_iteration: 113
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    113 |           156790 | 213570 |  4.78382 |              7.83741 |              1.68843 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 215460
  custom_metrics: {}
  date: 2021-09-20_12-51-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.109732184444565
  episode_reward_mean: 4.799255604230995
  episode_reward_min: 2.3113048886872094
  episodes_this_iter: 270
  episodes_total: 30780
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.852999988244846e-05
          entropy: 12.115452766418457
          entropy_coeff: 0.0003870225918944925
          kl: 0.01658315397799015
          model: {}
          policy_loss: -0.1811966598033905
          total_loss: -0.125778466463089
          vf_explained_var: 0.9931869506835938
          vf_loss: 0.02232864499092102
    num_agent_steps_sampled: 215460
    num_agent_steps_trained: 215460
    num_steps_sampled: 215460
    num_steps_trained: 215460
  iterations_since_restore: 114
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.6426735218509
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10042970074735658
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3368.8974440751645
    mean_inference_ms: 2.4541282430424225
    mean_raw_obs_processing_ms: 229.09959702166745
  time_since_restore: 158186.97287869453
  time_this_iter_s: 1396.7770309448242
  time_total_s: 158186.97287869453
  timers:
    learn_throughput: 704.919
    learn_time_ms: 2681.159
    load_throughput: 175510.599
    load_time_ms: 10.769
    sample_throughput: 1.898
    sample_time_ms: 995552.401
    update_time_ms: 29.126
  timestamp: 1632167487
  timesteps_since_restore: 0
  timesteps_total: 215460
  training_iteration: 114
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    114 |           158187 | 215460 |  4.79926 |              7.10973 |               2.3113 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 217350
  custom_metrics: {}
  date: 2021-09-20_13-09-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.91248864506523
  episode_reward_mean: 4.813393677124651
  episode_reward_min: 1.5369321877354325
  episodes_this_iter: 270
  episodes_total: 31050
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.834000280126929e-05
          entropy: 12.05737590789795
          entropy_coeff: 0.00038602278800681233
          kl: 0.01502471324056387
          model: {}
          policy_loss: -0.17798399925231934
          total_loss: -0.1252399981021881
          vf_explained_var: 0.992963969707489
          vf_loss: 0.023170219734311104
    num_agent_steps_sampled: 217350
    num_agent_steps_trained: 217350
    num_steps_sampled: 217350
    num_steps_trained: 217350
  iterations_since_restore: 115
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.021975147155004
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10040954636149474
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3357.6907170592563
    mean_inference_ms: 2.452207902471183
    mean_raw_obs_processing_ms: 228.87445663883457
  time_since_restore: 159284.5716264248
  time_this_iter_s: 1097.5987477302551
  time_total_s: 159284.5716264248
  timers:
    learn_throughput: 704.614
    learn_time_ms: 2682.318
    load_throughput: 178628.622
    load_time_ms: 10.581
    sample_throughput: 1.805
    sample_time_ms: 1047205.391
    update_time_ms: 29.435
  timestamp: 1632168585
  timesteps_since_restore: 0
  timesteps_total: 217350
  training_iteration: 115
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    115 |           159285 | 217350 |  4.81339 |              7.91249 |              1.53693 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 219240
  custom_metrics: {}
  date: 2021-09-20_13-24-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.000450757514901
  episode_reward_mean: 4.871886275183987
  episode_reward_min: 2.0079906934034035
  episodes_this_iter: 270
  episodes_total: 31320
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.814999844413251e-05
          entropy: 12.034564018249512
          entropy_coeff: 0.0003850230132229626
          kl: 0.016086595132946968
          model: {}
          policy_loss: -0.18664279580116272
          total_loss: -0.13183574378490448
          vf_explained_var: 0.9934156537055969
          vf_loss: 0.022793354466557503
    num_agent_steps_sampled: 219240
    num_agent_steps_trained: 219240
    num_steps_sampled: 219240
    num_steps_trained: 219240
  iterations_since_restore: 116
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.800585284280935
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10037248290841737
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3346.2164452305547
    mean_inference_ms: 2.4503741326834745
    mean_raw_obs_processing_ms: 228.65163968947542
  time_since_restore: 160143.91240739822
  time_this_iter_s: 859.3407809734344
  time_total_s: 160143.91240739822
  timers:
    learn_throughput: 703.641
    learn_time_ms: 2686.028
    load_throughput: 179085.432
    load_time_ms: 10.554
    sample_throughput: 1.773
    sample_time_ms: 1065788.56
    update_time_ms: 29.452
  timestamp: 1632169444
  timesteps_since_restore: 0
  timesteps_total: 219240
  training_iteration: 116
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    116 |           160144 | 219240 |  4.87189 |              8.00045 |              2.00799 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 221130
  custom_metrics: {}
  date: 2021-09-20_13-49-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.695377630953121
  episode_reward_mean: 4.871037755261466
  episode_reward_min: 2.150913026275848
  episodes_this_iter: 270
  episodes_total: 31590
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.796000136295334e-05
          entropy: 11.988842964172363
          entropy_coeff: 0.00038402320933528244
          kl: 0.01602189429104328
          model: {}
          policy_loss: -0.1857520490884781
          total_loss: -0.13285353779792786
          vf_explained_var: 0.9939097762107849
          vf_loss: 0.021002624183893204
    num_agent_steps_sampled: 221130
    num_agent_steps_trained: 221130
    num_steps_sampled: 221130
    num_steps_trained: 221130
  iterations_since_restore: 117
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.14554317548747
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10034829870422307
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3343.0670155936455
    mean_inference_ms: 2.449276631930059
    mean_raw_obs_processing_ms: 228.44741912576987
  time_since_restore: 161691.3340497017
  time_this_iter_s: 1547.4216423034668
  time_total_s: 161691.3340497017
  timers:
    learn_throughput: 701.759
    learn_time_ms: 2693.231
    load_throughput: 178924.96
    load_time_ms: 10.563
    sample_throughput: 1.633
    sample_time_ms: 1157680.764
    update_time_ms: 29.405
  timestamp: 1632170992
  timesteps_since_restore: 0
  timesteps_total: 221130
  training_iteration: 117
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    117 |           161691 | 221130 |  4.87104 |              7.69538 |              2.15091 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 223020
  custom_metrics: {}
  date: 2021-09-20_14-00-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.807110670590973
  episode_reward_mean: 4.8916144437920694
  episode_reward_min: 1.4755912709310437
  episodes_this_iter: 270
  episodes_total: 31860
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.776999700581655e-05
          entropy: 12.024928092956543
          entropy_coeff: 0.00038302340544760227
          kl: 0.017402013763785362
          model: {}
          policy_loss: -0.18245308101177216
          total_loss: -0.12329091876745224
          vf_explained_var: 0.9930339455604553
          vf_loss: 0.024124035611748695
    num_agent_steps_sampled: 223020
    num_agent_steps_trained: 223020
    num_steps_sampled: 223020
    num_steps_trained: 223020
  iterations_since_restore: 118
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.118171806167396
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10033961208451457
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3331.2804294790117
    mean_inference_ms: 2.447767099140017
    mean_raw_obs_processing_ms: 228.2309525765817
  time_since_restore: 162343.89064764977
  time_this_iter_s: 652.5565979480743
  time_total_s: 162343.89064764977
  timers:
    learn_throughput: 702.444
    learn_time_ms: 2690.607
    load_throughput: 179632.872
    load_time_ms: 10.521
    sample_throughput: 2.076
    sample_time_ms: 910585.395
    update_time_ms: 29.601
  timestamp: 1632171644
  timesteps_since_restore: 0
  timesteps_total: 223020
  training_iteration: 118
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    118 |           162344 | 223020 |  4.89161 |              7.80711 |              1.47559 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 224910
  custom_metrics: {}
  date: 2021-09-20_14-06-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.507297773310461
  episode_reward_mean: 4.801822277140649
  episode_reward_min: 1.8840547565161252
  episodes_this_iter: 270
  episodes_total: 32130
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.757999992463738e-05
          entropy: 12.084236145019531
          entropy_coeff: 0.0003820236015599221
          kl: 0.015307171270251274
          model: {}
          policy_loss: -0.19355307519435883
          total_loss: -0.1382356435060501
          vf_explained_var: 0.992591917514801
          vf_loss: 0.02506227418780327
    num_agent_steps_sampled: 224910
    num_agent_steps_trained: 224910
    num_steps_sampled: 224910
    num_steps_trained: 224910
  iterations_since_restore: 119
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.862227074235804
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10033030328680899
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3314.371465595771
    mean_inference_ms: 2.4459959165037515
    mean_raw_obs_processing_ms: 228.01186551138525
  time_since_restore: 162672.92868614197
  time_this_iter_s: 329.03803849220276
  time_total_s: 162672.92868614197
  timers:
    learn_throughput: 702.926
    learn_time_ms: 2688.759
    load_throughput: 178798.644
    load_time_ms: 10.571
    sample_throughput: 2.13
    sample_time_ms: 887147.61
    update_time_ms: 29.641
  timestamp: 1632171973
  timesteps_since_restore: 0
  timesteps_total: 224910
  training_iteration: 119
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    119 |           162673 | 224910 |  4.80182 |               7.5073 |              1.88405 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 226800
  custom_metrics: {}
  date: 2021-09-20_14-12-34
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.747311492381684
  episode_reward_mean: 4.767507581058991
  episode_reward_min: 1.826943143537284
  episodes_this_iter: 270
  episodes_total: 32400
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.73900028434582e-05
          entropy: 12.058714866638184
          entropy_coeff: 0.0003810237976722419
          kl: 0.015358700416982174
          model: {}
          policy_loss: -0.18231573700904846
          total_loss: -0.12721017003059387
          vf_explained_var: 0.9924829602241516
          vf_loss: 0.024711182340979576
    num_agent_steps_sampled: 226800
    num_agent_steps_trained: 226800
    num_steps_sampled: 226800
    num_steps_trained: 226800
  iterations_since_restore: 120
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 43.26521739130435
    ram_util_percent: 6.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10032210349091077
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3298.7849100280046
    mean_inference_ms: 2.444066750668911
    mean_raw_obs_processing_ms: 227.7919654992015
  time_since_restore: 163053.13612937927
  time_this_iter_s: 380.2074432373047
  time_total_s: 163053.13612937927
  timers:
    learn_throughput: 704.366
    learn_time_ms: 2683.264
    load_throughput: 178045.678
    load_time_ms: 10.615
    sample_throughput: 2.308
    sample_time_ms: 818866.083
    update_time_ms: 29.338
  timestamp: 1632172354
  timesteps_since_restore: 0
  timesteps_total: 226800
  training_iteration: 120
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    120 |           163053 | 226800 |  4.76751 |              7.74731 |              1.82694 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 228690
  custom_metrics: {}
  date: 2021-09-20_14-53-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.946743740832987
  episode_reward_mean: 4.807558336479159
  episode_reward_min: 1.8404610929461618
  episodes_this_iter: 270
  episodes_total: 32670
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.719999848632142e-05
          entropy: 12.067285537719727
          entropy_coeff: 0.00038002399378456175
          kl: 0.014952396973967552
          model: {}
          policy_loss: -0.20798955857753754
          total_loss: -0.15535414218902588
          vf_explained_var: 0.9929612278938293
          vf_loss: 0.023157881572842598
    num_agent_steps_sampled: 228690
    num_agent_steps_trained: 228690
    num_steps_sampled: 228690
    num_steps_trained: 228690
  iterations_since_restore: 121
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.10936591041303
    ram_util_percent: 6.59962187318208
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10029144187084538
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3294.1759279883127
    mean_inference_ms: 2.4423225619135747
    mean_raw_obs_processing_ms: 227.57023895632096
  time_since_restore: 165522.6107392311
  time_this_iter_s: 2469.474609851837
  time_total_s: 165522.6107392311
  timers:
    learn_throughput: 706.534
    learn_time_ms: 2675.031
    load_throughput: 179606.417
    load_time_ms: 10.523
    sample_throughput: 1.902
    sample_time_ms: 993704.141
    update_time_ms: 29.334
  timestamp: 1632174823
  timesteps_since_restore: 0
  timesteps_total: 228690
  training_iteration: 121
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    121 |           165523 | 228690 |  4.80756 |              7.94674 |              1.84046 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 230580
  custom_metrics: {}
  date: 2021-09-20_15-03-15
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.296360852929655
  episode_reward_mean: 4.833085550230145
  episode_reward_min: 2.141275611761316
  episodes_this_iter: 270
  episodes_total: 32940
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.701000140514225e-05
          entropy: 12.02833080291748
          entropy_coeff: 0.0003790241898968816
          kl: 0.013873758725821972
          model: {}
          policy_loss: -0.1771816909313202
          total_loss: -0.1277512162923813
          vf_explained_var: 0.9933368563652039
          vf_loss: 0.02238333784043789
    num_agent_steps_sampled: 230580
    num_agent_steps_trained: 230580
    num_steps_sampled: 230580
    num_steps_trained: 230580
  iterations_since_restore: 122
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.553894472361804
    ram_util_percent: 6.6
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10028069173757767
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3287.233060430835
    mean_inference_ms: 2.44064670833327
    mean_raw_obs_processing_ms: 227.36585100804444
  time_since_restore: 166094.79258179665
  time_this_iter_s: 572.1818425655365
  time_total_s: 166094.79258179665
  timers:
    learn_throughput: 706.201
    learn_time_ms: 2676.293
    load_throughput: 179874.578
    load_time_ms: 10.507
    sample_throughput: 1.894
    sample_time_ms: 998103.194
    update_time_ms: 29.2
  timestamp: 1632175395
  timesteps_since_restore: 0
  timesteps_total: 230580
  training_iteration: 122
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    122 |           166095 | 230580 |  4.83309 |              7.29636 |              2.14128 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 232470
  custom_metrics: {}
  date: 2021-09-20_15-12-02
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.100914600308606
  episode_reward_mean: 4.867287677674583
  episode_reward_min: 1.9324027564957769
  episodes_this_iter: 270
  episodes_total: 33210
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.681999704800546e-05
          entropy: 12.06016731262207
          entropy_coeff: 0.0003780243860092014
          kl: 0.01591889187693596
          model: {}
          policy_loss: -0.17800113558769226
          total_loss: -0.12425161898136139
          vf_explained_var: 0.9935242533683777
          vf_loss: 0.022043317556381226
    num_agent_steps_sampled: 232470
    num_agent_steps_trained: 232470
    num_steps_sampled: 232470
    num_steps_trained: 232470
  iterations_since_restore: 123
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.61418826739427
    ram_util_percent: 6.6000000000000005
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10029302900320931
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3286.221576383258
    mean_inference_ms: 2.4386754534813972
    mean_raw_obs_processing_ms: 227.17892519649374
  time_since_restore: 166621.66012358665
  time_this_iter_s: 526.8675417900085
  time_total_s: 166621.66012358665
  timers:
    learn_throughput: 705.836
    learn_time_ms: 2677.675
    load_throughput: 177606.485
    load_time_ms: 10.642
    sample_throughput: 1.928
    sample_time_ms: 980300.201
    update_time_ms: 29.205
  timestamp: 1632175922
  timesteps_since_restore: 0
  timesteps_total: 232470
  training_iteration: 123
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    123 |           166622 | 232470 |  4.86729 |              8.10091 |               1.9324 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 234360
  custom_metrics: {}
  date: 2021-09-20_15-30-47
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.589914804268313
  episode_reward_mean: 4.769683463718586
  episode_reward_min: 1.9265677047492757
  episodes_this_iter: 270
  episodes_total: 33480
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.662999996682629e-05
          entropy: 11.97288703918457
          entropy_coeff: 0.0003770246112253517
          kl: 0.015760263428092003
          model: {}
          policy_loss: -0.19012679159641266
          total_loss: -0.13717105984687805
          vf_explained_var: 0.9934822916984558
          vf_loss: 0.02156597003340721
    num_agent_steps_sampled: 234360
    num_agent_steps_trained: 234360
    num_steps_sampled: 234360
    num_steps_trained: 234360
  iterations_since_restore: 124
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.60383141762453
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10029837765869863
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3272.6795972858954
    mean_inference_ms: 2.4373253082328747
    mean_raw_obs_processing_ms: 226.98517750594638
  time_since_restore: 167746.18387246132
  time_this_iter_s: 1124.5237488746643
  time_total_s: 167746.18387246132
  timers:
    learn_throughput: 704.894
    learn_time_ms: 2681.254
    load_throughput: 177641.907
    load_time_ms: 10.639
    sample_throughput: 1.983
    sample_time_ms: 953071.571
    update_time_ms: 29.388
  timestamp: 1632177047
  timesteps_since_restore: 0
  timesteps_total: 234360
  training_iteration: 124
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    124 |           167746 | 234360 |  4.76968 |              7.58991 |              1.92657 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 236250
  custom_metrics: {}
  date: 2021-09-20_15-37-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.552635625915359
  episode_reward_mean: 4.779360816878101
  episode_reward_min: 1.869096670904707
  episodes_this_iter: 270
  episodes_total: 33750
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.644000288564712e-05
          entropy: 11.950299263000488
          entropy_coeff: 0.0003760248073376715
          kl: 0.015146425925195217
          model: {}
          policy_loss: -0.17978262901306152
          total_loss: -0.1266513168811798
          vf_explained_var: 0.992947518825531
          vf_loss: 0.023119481280446053
    num_agent_steps_sampled: 236250
    num_agent_steps_trained: 236250
    num_steps_sampled: 236250
    num_steps_trained: 236250
  iterations_since_restore: 125
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.682575757575755
    ram_util_percent: 6.6
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10029615896880847
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3257.5758110305155
    mean_inference_ms: 2.4358655317262827
    mean_raw_obs_processing_ms: 226.7823883353331
  time_since_restore: 168125.24161624908
  time_this_iter_s: 379.0577437877655
  time_total_s: 168125.24161624908
  timers:
    learn_throughput: 703.727
    learn_time_ms: 2685.699
    load_throughput: 176909.291
    load_time_ms: 10.683
    sample_throughput: 2.145
    sample_time_ms: 881212.476
    update_time_ms: 29.661
  timestamp: 1632177426
  timesteps_since_restore: 0
  timesteps_total: 236250
  training_iteration: 125
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    125 |           168125 | 236250 |  4.77936 |              7.55264 |               1.8691 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 238140
  custom_metrics: {}
  date: 2021-09-20_17-29-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.804301620938146
  episode_reward_mean: 4.787430535013654
  episode_reward_min: 1.5928472482264144
  episodes_this_iter: 270
  episodes_total: 34020
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.624999852851033e-05
          entropy: 11.964339256286621
          entropy_coeff: 0.00037502500344999135
          kl: 0.016060641035437584
          model: {}
          policy_loss: -0.19565273821353912
          total_loss: -0.14024190604686737
          vf_explained_var: 0.9929632544517517
          vf_loss: 0.023309608921408653
    num_agent_steps_sampled: 238140
    num_agent_steps_trained: 238140
    num_steps_sampled: 238140
    num_steps_trained: 238140
  iterations_since_restore: 126
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.94458998935037
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10028201427305625
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3276.8459487179
    mean_inference_ms: 2.4341419473323276
    mean_raw_obs_processing_ms: 226.58128758415373
  time_since_restore: 174870.54802680016
  time_this_iter_s: 6745.306410551071
  time_total_s: 174870.54802680016
  timers:
    learn_throughput: 704.283
    learn_time_ms: 2683.58
    load_throughput: 175864.924
    load_time_ms: 10.747
    sample_throughput: 1.286
    sample_time_ms: 1469810.473
    update_time_ms: 29.82
  timestamp: 1632184171
  timesteps_since_restore: 0
  timesteps_total: 238140
  training_iteration: 126
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    126 |           174871 | 238140 |  4.78743 |               7.8043 |              1.59285 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 240030
  custom_metrics: {}
  date: 2021-09-20_18-02-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.578158508419211
  episode_reward_mean: 4.810536093938833
  episode_reward_min: 1.880997283867744
  episodes_this_iter: 270
  episodes_total: 34290
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.606000144733116e-05
          entropy: 11.883776664733887
          entropy_coeff: 0.00037402519956231117
          kl: 0.015668747946619987
          model: {}
          policy_loss: -0.1868719756603241
          total_loss: -0.13058961927890778
          vf_explained_var: 0.9926193356513977
          vf_loss: 0.025031816214323044
    num_agent_steps_sampled: 240030
    num_agent_steps_trained: 240030
    num_steps_sampled: 240030
    num_steps_trained: 240030
  iterations_since_restore: 127
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.94798108403056
    ram_util_percent: 6.6
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10030613454230593
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3274.2107375007186
    mean_inference_ms: 2.4326463107317404
    mean_raw_obs_processing_ms: 226.42929164190852
  time_since_restore: 176845.04432749748
  time_this_iter_s: 1974.4963006973267
  time_total_s: 176845.04432749748
  timers:
    learn_throughput: 704.747
    learn_time_ms: 2681.815
    load_throughput: 174931.968
    load_time_ms: 10.804
    sample_throughput: 1.25
    sample_time_ms: 1512519.039
    update_time_ms: 29.843
  timestamp: 1632186146
  timesteps_since_restore: 0
  timesteps_total: 240030
  training_iteration: 127
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    127 |           176845 | 240030 |  4.81054 |              7.57816 |                1.881 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 241920
  custom_metrics: {}
  date: 2021-09-20_18-10-59
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.11941096983469
  episode_reward_mean: 4.799696159436951
  episode_reward_min: 1.682012873695708
  episodes_this_iter: 270
  episodes_total: 34560
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.586999709019437e-05
          entropy: 11.933984756469727
          entropy_coeff: 0.000373025395674631
          kl: 0.015070794150233269
          model: {}
          policy_loss: -0.190080925822258
          total_loss: -0.13648411631584167
          vf_explained_var: 0.9928665161132812
          vf_loss: 0.023715320974588394
    num_agent_steps_sampled: 241920
    num_agent_steps_trained: 241920
    num_steps_sampled: 241920
    num_steps_trained: 241920
  iterations_since_restore: 128
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.0095104895105
    ram_util_percent: 6.6
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10029188561426217
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3268.0412567683006
    mean_inference_ms: 2.431070643305983
    mean_raw_obs_processing_ms: 226.25485727224645
  time_since_restore: 177358.3647725582
  time_this_iter_s: 513.32044506073
  time_total_s: 177358.3647725582
  timers:
    learn_throughput: 704.6
    learn_time_ms: 2682.375
    load_throughput: 174899.162
    load_time_ms: 10.806
    sample_throughput: 1.261
    sample_time_ms: 1498594.629
    update_time_ms: 29.711
  timestamp: 1632186659
  timesteps_since_restore: 0
  timesteps_total: 241920
  training_iteration: 128
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    128 |           177358 | 241920 |   4.7997 |              8.11941 |              1.68201 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 243810
  custom_metrics: {}
  date: 2021-09-20_18-23-14
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.738896224203171
  episode_reward_mean: 4.7487356753358
  episode_reward_min: 1.912874948770535
  episodes_this_iter: 270
  episodes_total: 34830
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.56800000090152e-05
          entropy: 11.894876480102539
          entropy_coeff: 0.0003720255917869508
          kl: 0.01573028415441513
          model: {}
          policy_loss: -0.18494926393032074
          total_loss: -0.1272520273923874
          vf_explained_var: 0.9920142292976379
          vf_loss: 0.026286860927939415
    num_agent_steps_sampled: 243810
    num_agent_steps_trained: 243810
    num_steps_sampled: 243810
    num_steps_trained: 243810
  iterations_since_restore: 129
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.90029325513196
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10028590591881964
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3262.8694910491868
    mean_inference_ms: 2.4299060829977472
    mean_raw_obs_processing_ms: 226.09385972033863
  time_since_restore: 178093.4344239235
  time_this_iter_s: 735.0696513652802
  time_total_s: 178093.4344239235
  timers:
    learn_throughput: 704.71
    learn_time_ms: 2681.954
    load_throughput: 174846.698
    load_time_ms: 10.809
    sample_throughput: 1.228
    sample_time_ms: 1539198.169
    update_time_ms: 30.007
  timestamp: 1632187394
  timesteps_since_restore: 0
  timesteps_total: 243810
  training_iteration: 129
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    129 |           178093 | 243810 |  4.74874 |               7.7389 |              1.91287 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 245700
  custom_metrics: {}
  date: 2021-09-20_18-35-30
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.206867985647712
  episode_reward_mean: 4.711774063209487
  episode_reward_min: 1.9448097503188246
  episodes_this_iter: 270
  episodes_total: 35100
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.549000292783603e-05
          entropy: 11.873178482055664
          entropy_coeff: 0.00037102578789927065
          kl: 0.014962993562221527
          model: {}
          policy_loss: -0.19224913418293
          total_loss: -0.1374669373035431
          vf_explained_var: 0.9921534657478333
          vf_loss: 0.025099843740463257
    num_agent_steps_sampled: 245700
    num_agent_steps_trained: 245700
    num_steps_sampled: 245700
    num_steps_trained: 245700
  iterations_since_restore: 130
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.40244379276637
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1002691647814752
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3255.2014516043755
    mean_inference_ms: 2.4283207122103216
    mean_raw_obs_processing_ms: 225.9220175623135
  time_since_restore: 178828.62820720673
  time_this_iter_s: 735.1937832832336
  time_total_s: 178828.62820720673
  timers:
    learn_throughput: 704.928
    learn_time_ms: 2681.124
    load_throughput: 175547.133
    load_time_ms: 10.766
    sample_throughput: 1.2
    sample_time_ms: 1574697.257
    update_time_ms: 29.872
  timestamp: 1632188130
  timesteps_since_restore: 0
  timesteps_total: 245700
  training_iteration: 130
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    130 |           178829 | 245700 |  4.71177 |              7.20687 |              1.94481 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 247590
  custom_metrics: {}
  date: 2021-09-20_18-54-14
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.542808874561045
  episode_reward_mean: 4.881191520278782
  episode_reward_min: 1.773293516933875
  episodes_this_iter: 270
  episodes_total: 35370
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.529999857069924e-05
          entropy: 11.88436222076416
          entropy_coeff: 0.00037002601311542094
          kl: 0.01564328745007515
          model: {}
          policy_loss: -0.17308099567890167
          total_loss: -0.12038785964250565
          vf_explained_var: 0.9936889410018921
          vf_loss: 0.021453293040394783
    num_agent_steps_sampled: 247590
    num_agent_steps_trained: 247590
    num_steps_sampled: 247590
    num_steps_trained: 247590
  iterations_since_restore: 131
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.62442455242967
    ram_util_percent: 6.599999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10027911807660896
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3252.638421360378
    mean_inference_ms: 2.42669120351245
    mean_raw_obs_processing_ms: 225.7578853266862
  time_since_restore: 179953.37563347816
  time_this_iter_s: 1124.7474262714386
  time_total_s: 179953.37563347816
  timers:
    learn_throughput: 702.999
    learn_time_ms: 2688.483
    load_throughput: 176086.42
    load_time_ms: 10.733
    sample_throughput: 1.312
    sample_time_ms: 1440217.31
    update_time_ms: 29.964
  timestamp: 1632189254
  timesteps_since_restore: 0
  timesteps_total: 247590
  training_iteration: 131
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    131 |           179953 | 247590 |  4.88119 |              7.54281 |              1.77329 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 249480
  custom_metrics: {}
  date: 2021-09-20_19-05-20
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.400391437409469
  episode_reward_mean: 4.704935017980417
  episode_reward_min: 1.717486271848101
  episodes_this_iter: 270
  episodes_total: 35640
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.511000148952007e-05
          entropy: 11.93828296661377
          entropy_coeff: 0.00036902620922774076
          kl: 0.016808222979307175
          model: {}
          policy_loss: -0.19034446775913239
          total_loss: -0.1341690719127655
          vf_explained_var: 0.9929804801940918
          vf_loss: 0.02228970266878605
    num_agent_steps_sampled: 249480
    num_agent_steps_trained: 249480
    num_steps_sampled: 249480
    num_steps_trained: 249480
  iterations_since_restore: 132
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.365189189189195
    ram_util_percent: 6.420864864864865
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10027564603725576
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3245.798945255492
    mean_inference_ms: 2.4252066170871034
    mean_raw_obs_processing_ms: 225.58088710936894
  time_since_restore: 180618.98888540268
  time_this_iter_s: 665.6132519245148
  time_total_s: 180618.98888540268
  timers:
    learn_throughput: 704.212
    learn_time_ms: 2683.85
    load_throughput: 176695.567
    load_time_ms: 10.696
    sample_throughput: 1.304
    sample_time_ms: 1449565.0
    update_time_ms: 30.273
  timestamp: 1632189920
  timesteps_since_restore: 0
  timesteps_total: 249480
  training_iteration: 132
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    132 |           180619 | 249480 |  4.70494 |              7.40039 |              1.71749 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 251370
  custom_metrics: {}
  date: 2021-09-20_19-25-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.207560330225789
  episode_reward_mean: 4.782293222751736
  episode_reward_min: 1.8531011492658052
  episodes_this_iter: 270
  episodes_total: 35910
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.491999713238329e-05
          entropy: 11.901314735412598
          entropy_coeff: 0.0003680264053400606
          kl: 0.0156338382512331
          model: {}
          policy_loss: -0.18101534247398376
          total_loss: -0.12620596587657928
          vf_explained_var: 0.9928246736526489
          vf_loss: 0.02357351779937744
    num_agent_steps_sampled: 251370
    num_agent_steps_trained: 251370
    num_steps_sampled: 251370
    num_steps_trained: 251370
  iterations_since_restore: 133
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 16.105138806851745
    ram_util_percent: 5.857531010041345
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10026736974322781
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3241.741832225347
    mean_inference_ms: 2.4246446242785304
    mean_raw_obs_processing_ms: 225.40601180449664
  time_since_restore: 181838.58396577835
  time_this_iter_s: 1219.5950803756714
  time_total_s: 181838.58396577835
  timers:
    learn_throughput: 705.914
    learn_time_ms: 2677.381
    load_throughput: 178098.079
    load_time_ms: 10.612
    sample_throughput: 1.244
    sample_time_ms: 1518843.904
    update_time_ms: 30.552
  timestamp: 1632191140
  timesteps_since_restore: 0
  timesteps_total: 251370
  training_iteration: 133
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    133 |           181839 | 251370 |  4.78229 |              8.20756 |               1.8531 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 253260
  custom_metrics: {}
  date: 2021-09-20_19-34-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.56782982482792
  episode_reward_mean: 4.751635343039891
  episode_reward_min: 2.0963841154204568
  episodes_this_iter: 270
  episodes_total: 36180
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.473000005120412e-05
          entropy: 11.859319686889648
          entropy_coeff: 0.0003670266014523804
          kl: 0.014165742322802544
          model: {}
          policy_loss: -0.20380917191505432
          total_loss: -0.155266672372818
          vf_explained_var: 0.9937869310379028
          vf_loss: 0.020623845979571342
    num_agent_steps_sampled: 253260
    num_agent_steps_trained: 253260
    num_steps_sampled: 253260
    num_steps_trained: 253260
  iterations_since_restore: 134
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 17.57771883289125
    ram_util_percent: 6.388726790450928
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10025677340082277
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3230.0538837595673
    mean_inference_ms: 2.423457863764641
    mean_raw_obs_processing_ms: 225.2296572065209
  time_since_restore: 182382.43895173073
  time_this_iter_s: 543.8549859523773
  time_total_s: 182382.43895173073
  timers:
    learn_throughput: 707.516
    learn_time_ms: 2671.317
    load_throughput: 179180.558
    load_time_ms: 10.548
    sample_throughput: 1.294
    sample_time_ms: 1460782.401
    update_time_ms: 30.845
  timestamp: 1632191684
  timesteps_since_restore: 0
  timesteps_total: 253260
  training_iteration: 134
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    134 |           182382 | 253260 |  4.75164 |              7.56783 |              2.09638 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 255150
  custom_metrics: {}
  date: 2021-09-20_19-38-47
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.910862840995185
  episode_reward_mean: 4.793566048141014
  episode_reward_min: 2.167264469590116
  episodes_this_iter: 270
  episodes_total: 36450
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.454000297002494e-05
          entropy: 11.82880973815918
          entropy_coeff: 0.00036602679756470025
          kl: 0.014885634183883667
          model: {}
          policy_loss: -0.19416549801826477
          total_loss: -0.14320430159568787
          vf_explained_var: 0.9936359524726868
          vf_loss: 0.021379519253969193
    num_agent_steps_sampled: 255150
    num_agent_steps_trained: 255150
    num_steps_sampled: 255150
    num_steps_trained: 255150
  iterations_since_restore: 135
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.110089020771515
    ram_util_percent: 6.594362017804153
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10025990771162087
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3217.121600239916
    mean_inference_ms: 2.422273638367193
    mean_raw_obs_processing_ms: 227.21342307517247
  time_since_restore: 182625.47848773003
  time_this_iter_s: 243.0395359992981
  time_total_s: 182625.47848773003
  timers:
    learn_throughput: 710.845
    learn_time_ms: 2658.808
    load_throughput: 177299.821
    load_time_ms: 10.66
    sample_throughput: 1.306
    sample_time_ms: 1447193.98
    update_time_ms: 30.206
  timestamp: 1632191927
  timesteps_since_restore: 0
  timesteps_total: 255150
  training_iteration: 135
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    135 |           182625 | 255150 |  4.79357 |              7.91086 |              2.16726 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 257040
  custom_metrics: {}
  date: 2021-09-20_20-10-22
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.74659565080763
  episode_reward_mean: 4.798428442155957
  episode_reward_min: 1.7852872527371584
  episodes_this_iter: 270
  episodes_total: 36720
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.434999861288816e-05
          entropy: 11.793051719665527
          entropy_coeff: 0.0003650269936770201
          kl: 0.013661960139870644
          model: {}
          policy_loss: -0.18508318066596985
          total_loss: -0.13545288145542145
          vf_explained_var: 0.9931120276451111
          vf_loss: 0.022811414673924446
    num_agent_steps_sampled: 257040
    num_agent_steps_trained: 257040
    num_steps_sampled: 257040
    num_steps_trained: 257040
  iterations_since_restore: 136
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 17.155872291904217
    ram_util_percent: 6.545001900418091
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10026935470965231
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3216.0932225050933
    mean_inference_ms: 2.421691331245049
    mean_raw_obs_processing_ms: 227.51100348628992
  time_since_restore: 184520.51416802406
  time_this_iter_s: 1895.0356802940369
  time_total_s: 184520.51416802406
  timers:
    learn_throughput: 712.843
    learn_time_ms: 2651.355
    load_throughput: 177479.639
    load_time_ms: 10.649
    sample_throughput: 1.965
    sample_time_ms: 962051.272
    update_time_ms: 30.06
  timestamp: 1632193822
  timesteps_since_restore: 0
  timesteps_total: 257040
  training_iteration: 136
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    136 |           184521 | 257040 |  4.79843 |               7.7466 |              1.78529 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 258930
  custom_metrics: {}
  date: 2021-09-20_20-17-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.710332688885133
  episode_reward_mean: 4.793154517070239
  episode_reward_min: 1.9802962505926376
  episodes_this_iter: 270
  episodes_total: 36990
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.416000153170899e-05
          entropy: 11.796449661254883
          entropy_coeff: 0.0003640271897893399
          kl: 0.014099270105361938
          model: {}
          policy_loss: -0.1788860708475113
          total_loss: -0.12880167365074158
          vf_explained_var: 0.9932681918144226
          vf_loss: 0.022258708253502846
    num_agent_steps_sampled: 258930
    num_agent_steps_trained: 258930
    num_steps_sampled: 258930
    num_steps_trained: 258930
  iterations_since_restore: 137
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.488907849829353
    ram_util_percent: 6.882764505119455
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10025270669030929
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3204.205666197988
    mean_inference_ms: 2.420623800390245
    mean_raw_obs_processing_ms: 227.32980402748296
  time_since_restore: 184942.75228953362
  time_this_iter_s: 422.238121509552
  time_total_s: 184942.75228953362
  timers:
    learn_throughput: 714.288
    learn_time_ms: 2645.991
    load_throughput: 178126.893
    load_time_ms: 10.61
    sample_throughput: 2.342
    sample_time_ms: 806831.321
    update_time_ms: 30.079
  timestamp: 1632194244
  timesteps_since_restore: 0
  timesteps_total: 258930
  training_iteration: 137
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    137 |           184943 | 258930 |  4.79315 |              7.71033 |               1.9803 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 260820
  custom_metrics: {}
  date: 2021-09-20_20-23-34
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.744640504606092
  episode_reward_mean: 4.777622340970336
  episode_reward_min: 2.017725443622793
  episodes_this_iter: 270
  episodes_total: 37260
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.39699971745722e-05
          entropy: 11.794116020202637
          entropy_coeff: 0.0003630273859016597
          kl: 0.01608257368206978
          model: {}
          policy_loss: -0.1780690848827362
          total_loss: -0.12237168103456497
          vf_explained_var: 0.9928088784217834
          vf_loss: 0.023340877145528793
    num_agent_steps_sampled: 260820
    num_agent_steps_trained: 260820
    num_steps_sampled: 260820
    num_steps_trained: 260820
  iterations_since_restore: 138
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.06485436893204
    ram_util_percent: 6.52621359223301
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10023506366866929
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3193.721488908526
    mean_inference_ms: 2.4197762254276163
    mean_raw_obs_processing_ms: 227.14701073353027
  time_since_restore: 185313.00866818428
  time_this_iter_s: 370.2563786506653
  time_total_s: 185313.00866818428
  timers:
    learn_throughput: 716.354
    learn_time_ms: 2638.359
    load_throughput: 177435.544
    load_time_ms: 10.652
    sample_throughput: 2.385
    sample_time_ms: 792532.862
    update_time_ms: 29.835
  timestamp: 1632194614
  timesteps_since_restore: 0
  timesteps_total: 260820
  training_iteration: 138
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    138 |           185313 | 260820 |  4.77762 |              7.74464 |              2.01773 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 262710
  custom_metrics: {}
  date: 2021-09-20_20-53-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.586241435185541
  episode_reward_mean: 4.699232226328837
  episode_reward_min: 2.1344605832483845
  episodes_this_iter: 270
  episodes_total: 37530
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.378000009339303e-05
          entropy: 11.884035110473633
          entropy_coeff: 0.00036202761111781
          kl: 0.014135071076452732
          model: {}
          policy_loss: -0.1714690923690796
          total_loss: -0.1214536502957344
          vf_explained_var: 0.9933551549911499
          vf_loss: 0.022116316482424736
    num_agent_steps_sampled: 262710
    num_agent_steps_trained: 262710
    num_steps_sampled: 262710
    num_steps_trained: 262710
  iterations_since_restore: 139
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.103393939393936
    ram_util_percent: 6.806141414141415
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10030399832572864
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3202.635681335063
    mean_inference_ms: 2.419006858735267
    mean_raw_obs_processing_ms: 226.97189398776467
  time_since_restore: 187094.46065306664
  time_this_iter_s: 1781.4519848823547
  time_total_s: 187094.46065306664
  timers:
    learn_throughput: 717.892
    learn_time_ms: 2632.707
    load_throughput: 177492.355
    load_time_ms: 10.648
    sample_throughput: 2.107
    sample_time_ms: 897176.778
    update_time_ms: 29.636
  timestamp: 1632196396
  timesteps_since_restore: 0
  timesteps_total: 262710
  training_iteration: 139
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    139 |           187094 | 262710 |  4.69923 |              7.58624 |              2.13446 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 264600
  custom_metrics: {}
  date: 2021-09-20_21-23-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.847977045019866
  episode_reward_mean: 4.803824279132724
  episode_reward_min: 2.0857969141401362
  episodes_this_iter: 270
  episodes_total: 37800
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.359000301221386e-05
          entropy: 11.848196983337402
          entropy_coeff: 0.00036102780723012984
          kl: 0.014254562556743622
          model: {}
          policy_loss: -0.1963205635547638
          total_loss: -0.1456502228975296
          vf_explained_var: 0.9933172464370728
          vf_loss: 0.022474201396107674
    num_agent_steps_sampled: 264600
    num_agent_steps_trained: 264600
    num_steps_sampled: 264600
    num_steps_trained: 264600
  iterations_since_restore: 140
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.082311320754716
    ram_util_percent: 6.989661949685534
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10028753518936648
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3198.877471384949
    mean_inference_ms: 2.4179192204640847
    mean_raw_obs_processing_ms: 226.80231801920522
  time_since_restore: 188924.92206263542
  time_this_iter_s: 1830.4614095687866
  time_total_s: 188924.92206263542
  timers:
    learn_throughput: 718.13
    learn_time_ms: 2631.836
    load_throughput: 177058.257
    load_time_ms: 10.674
    sample_throughput: 1.877
    sample_time_ms: 1006704.674
    update_time_ms: 29.512
  timestamp: 1632198226
  timesteps_since_restore: 0
  timesteps_total: 264600
  training_iteration: 140
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    140 |           188925 | 264600 |  4.80382 |              7.84798 |               2.0858 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 266490
  custom_metrics: {}
  date: 2021-09-20_21-42-49
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.5910687479486505
  episode_reward_mean: 4.754507486532144
  episode_reward_min: 2.0899310234624693
  episodes_this_iter: 270
  episodes_total: 38070
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.339999865507707e-05
          entropy: 11.805731773376465
          entropy_coeff: 0.00036002800334244967
          kl: 0.015416798181831837
          model: {}
          policy_loss: -0.1830003559589386
          total_loss: -0.13129988312721252
          vf_explained_var: 0.99363774061203
          vf_loss: 0.02082947827875614
    num_agent_steps_sampled: 266490
    num_agent_steps_trained: 266490
    num_steps_sampled: 266490
    num_steps_trained: 266490
  iterations_since_restore: 141
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 43.25047199496539
    ram_util_percent: 7.3466960352422905
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10025811497169206
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3194.994877811223
    mean_inference_ms: 2.4165249738232717
    mean_raw_obs_processing_ms: 226.64049240305457
  time_since_restore: 190067.4835216999
  time_this_iter_s: 1142.5614590644836
  time_total_s: 190067.4835216999
  timers:
    learn_throughput: 718.405
    learn_time_ms: 2630.829
    load_throughput: 175524.589
    load_time_ms: 10.768
    sample_throughput: 1.874
    sample_time_ms: 1008487.446
    update_time_ms: 29.293
  timestamp: 1632199369
  timesteps_since_restore: 0
  timesteps_total: 266490
  training_iteration: 141
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    141 |           190067 | 266490 |  4.75451 |              7.59107 |              2.08993 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 268380
  custom_metrics: {}
  date: 2021-09-20_21-50-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.395277920869608
  episode_reward_mean: 4.7715780838601844
  episode_reward_min: 1.9116129959597399
  episodes_this_iter: 270
  episodes_total: 38340
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.32100015738979e-05
          entropy: 11.715118408203125
          entropy_coeff: 0.0003590281994547695
          kl: 0.01580062322318554
          model: {}
          policy_loss: -0.18825693428516388
          total_loss: -0.13120771944522858
          vf_explained_var: 0.9924377799034119
          vf_loss: 0.025259489193558693
    num_agent_steps_sampled: 268380
    num_agent_steps_trained: 268380
    num_steps_sampled: 268380
    num_steps_trained: 268380
  iterations_since_restore: 142
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.77021276595745
    ram_util_percent: 7.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.100233084111787
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3186.616926858463
    mean_inference_ms: 2.415587545437908
    mean_raw_obs_processing_ms: 226.49079598636956
  time_since_restore: 190506.6289563179
  time_this_iter_s: 439.1454346179962
  time_total_s: 190506.6289563179
  timers:
    learn_throughput: 718.451
    learn_time_ms: 2630.659
    load_throughput: 174879.099
    load_time_ms: 10.807
    sample_throughput: 1.917
    sample_time_ms: 985840.815
    update_time_ms: 28.857
  timestamp: 1632199808
  timesteps_since_restore: 0
  timesteps_total: 268380
  training_iteration: 142
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    142 |           190507 | 268380 |  4.77158 |              7.39528 |              1.91161 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 270270
  custom_metrics: {}
  date: 2021-09-20_22-23-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.757389988133786
  episode_reward_mean: 4.893702717271223
  episode_reward_min: 1.883435083874261
  episodes_this_iter: 270
  episodes_total: 38610
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.301999721676111e-05
          entropy: 11.764204978942871
          entropy_coeff: 0.0003580283955670893
          kl: 0.014433964155614376
          model: {}
          policy_loss: -0.19250062108039856
          total_loss: -0.14298294484615326
          vf_explained_var: 0.9938640594482422
          vf_loss: 0.020847246050834656
    num_agent_steps_sampled: 270270
    num_agent_steps_trained: 270270
    num_steps_sampled: 270270
    num_steps_trained: 270270
  iterations_since_restore: 143
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.12951699463327
    ram_util_percent: 7.220322003577818
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10023058431767179
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3182.486769431852
    mean_inference_ms: 2.414627037810842
    mean_raw_obs_processing_ms: 226.3339514713051
  time_since_restore: 192514.91027021408
  time_this_iter_s: 2008.2813138961792
  time_total_s: 192514.91027021408
  timers:
    learn_throughput: 717.585
    learn_time_ms: 2633.835
    load_throughput: 175169.31
    load_time_ms: 10.79
    sample_throughput: 1.775
    sample_time_ms: 1064706.24
    update_time_ms: 28.623
  timestamp: 1632201816
  timesteps_since_restore: 0
  timesteps_total: 270270
  training_iteration: 143
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    143 |           192515 | 270270 |   4.8937 |              7.75739 |              1.88344 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 272160
  custom_metrics: {}
  date: 2021-09-20_22-40-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.774179952648149
  episode_reward_mean: 4.780370127534187
  episode_reward_min: 1.7623462330403632
  episodes_this_iter: 270
  episodes_total: 38880
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.283000013558194e-05
          entropy: 11.773711204528809
          entropy_coeff: 0.00035702859167940915
          kl: 0.015274611301720142
          model: {}
          policy_loss: -0.19708526134490967
          total_loss: -0.14729133248329163
          vf_explained_var: 0.9941636323928833
          vf_loss: 0.019199976697564125
    num_agent_steps_sampled: 272160
    num_agent_steps_trained: 272160
    num_steps_sampled: 272160
    num_steps_trained: 272160
  iterations_since_restore: 144
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.331929824561406
    ram_util_percent: 7.217192982456142
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10022182986801048
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3178.5741030713493
    mean_inference_ms: 2.4134174113246565
    mean_raw_obs_processing_ms: 226.17472384262695
  time_since_restore: 193538.69698905945
  time_this_iter_s: 1023.7867188453674
  time_total_s: 193538.69698905945
  timers:
    learn_throughput: 717.241
    learn_time_ms: 2635.099
    load_throughput: 175170.084
    load_time_ms: 10.79
    sample_throughput: 1.699
    sample_time_ms: 1112698.773
    update_time_ms: 28.293
  timestamp: 1632202840
  timesteps_since_restore: 0
  timesteps_total: 272160
  training_iteration: 144
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    144 |           193539 | 272160 |  4.78037 |              7.77418 |              1.76235 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 274050
  custom_metrics: {}
  date: 2021-09-20_22-47-00
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.774285612632111
  episode_reward_mean: 4.782773074342856
  episode_reward_min: 1.9107787813932784
  episodes_this_iter: 270
  episodes_total: 39150
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.264000305440277e-05
          entropy: 11.798301696777344
          entropy_coeff: 0.000356028787791729
          kl: 0.015781903639435768
          model: {}
          policy_loss: -0.20273254811763763
          total_loss: -0.14954732358455658
          vf_explained_var: 0.9936352968215942
          vf_loss: 0.02143258787691593
    num_agent_steps_sampled: 274050
    num_agent_steps_trained: 274050
    num_steps_sampled: 274050
    num_steps_trained: 274050
  iterations_since_restore: 145
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.194886363636364
    ram_util_percent: 7.299999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10019480852754928
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3168.2486241818183
    mean_inference_ms: 2.41199101030795
    mean_raw_obs_processing_ms: 226.02375572145183
  time_since_restore: 193918.0727379322
  time_this_iter_s: 379.37574887275696
  time_total_s: 193918.0727379322
  timers:
    learn_throughput: 716.531
    learn_time_ms: 2637.709
    load_throughput: 177655.442
    load_time_ms: 10.639
    sample_throughput: 1.678
    sample_time_ms: 1126329.958
    update_time_ms: 28.296
  timestamp: 1632203220
  timesteps_since_restore: 0
  timesteps_total: 274050
  training_iteration: 145
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    145 |           193918 | 274050 |  4.78277 |              7.77429 |              1.91078 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 275940
  custom_metrics: {}
  date: 2021-09-20_22-59-47
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.427070092904136
  episode_reward_mean: 4.815634133703633
  episode_reward_min: 1.4902601472247563
  episodes_this_iter: 270
  episodes_total: 39420
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.244999869726598e-05
          entropy: 11.714775085449219
          entropy_coeff: 0.00035502901300787926
          kl: 0.014747574925422668
          model: {}
          policy_loss: -0.2122284322977066
          total_loss: -0.15839901566505432
          vf_explained_var: 0.9929138422012329
          vf_loss: 0.024391669780015945
    num_agent_steps_sampled: 275940
    num_agent_steps_trained: 275940
    num_steps_sampled: 275940
    num_steps_trained: 275940
  iterations_since_restore: 146
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.50205799812909
    ram_util_percent: 7.419176800748363
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1002377556100721
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3157.2662227214014
    mean_inference_ms: 2.410768066574825
    mean_raw_obs_processing_ms: 225.8612108606257
  time_since_restore: 194685.68871808052
  time_this_iter_s: 767.6159801483154
  time_total_s: 194685.68871808052
  timers:
    learn_throughput: 715.316
    learn_time_ms: 2642.187
    load_throughput: 178037.681
    load_time_ms: 10.616
    sample_throughput: 1.864
    sample_time_ms: 1013706.964
    update_time_ms: 28.153
  timestamp: 1632203987
  timesteps_since_restore: 0
  timesteps_total: 275940
  training_iteration: 146
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    146 |           194686 | 275940 |  4.81563 |              7.42707 |              1.49026 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 277830
  custom_metrics: {}
  date: 2021-09-20_23-13-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.864942045897967
  episode_reward_mean: 4.878479507815291
  episode_reward_min: 2.223722537144867
  episodes_this_iter: 270
  episodes_total: 39690
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.226000161608681e-05
          entropy: 11.728899002075195
          entropy_coeff: 0.0003540292091201991
          kl: 0.014896363951265812
          model: {}
          policy_loss: -0.1710515171289444
          total_loss: -0.1182122603058815
          vf_explained_var: 0.9929984211921692
          vf_loss: 0.02305583842098713
    num_agent_steps_sampled: 277830
    num_agent_steps_trained: 277830
    num_steps_sampled: 277830
    num_steps_trained: 277830
  iterations_since_restore: 147
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 43.12346119536129
    ram_util_percent: 7.538537020517396
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10022770543004358
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3145.4433400359953
    mean_inference_ms: 2.40950814969116
    mean_raw_obs_processing_ms: 225.70092687547972
  time_since_restore: 195491.5005955696
  time_this_iter_s: 805.81187748909
  time_total_s: 195491.5005955696
  timers:
    learn_throughput: 714.28
    learn_time_ms: 2646.02
    load_throughput: 177995.706
    load_time_ms: 10.618
    sample_throughput: 1.796
    sample_time_ms: 1052060.408
    update_time_ms: 28.121
  timestamp: 1632204793
  timesteps_since_restore: 0
  timesteps_total: 277830
  training_iteration: 147
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    147 |           195492 | 277830 |  4.87848 |              7.86494 |              2.22372 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 279720
  custom_metrics: {}
  date: 2021-09-21_00-06-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.77595357147
  episode_reward_mean: 4.775907747939658
  episode_reward_min: 1.7389518084950784
  episodes_this_iter: 270
  episodes_total: 39960
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.206999725895002e-05
          entropy: 11.712411880493164
          entropy_coeff: 0.0003530294052325189
          kl: 0.017140265554189682
          model: {}
          policy_loss: -0.1733684092760086
          total_loss: -0.11523132771253586
          vf_explained_var: 0.9929104447364807
          vf_loss: 0.023224230855703354
    num_agent_steps_sampled: 279720
    num_agent_steps_trained: 279720
    num_steps_sampled: 279720
    num_steps_trained: 279720
  iterations_since_restore: 148
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.02733634311512
    ram_util_percent: 7.834943566591421
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10020216904535734
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3149.1491915279976
    mean_inference_ms: 2.4083388671490606
    mean_raw_obs_processing_ms: 225.54442457038354
  time_since_restore: 198674.95842671394
  time_this_iter_s: 3183.457831144333
  time_total_s: 198674.95842671394
  timers:
    learn_throughput: 713.198
    learn_time_ms: 2650.037
    load_throughput: 179511.652
    load_time_ms: 10.529
    sample_throughput: 1.417
    sample_time_ms: 1333376.255
    update_time_ms: 28.233
  timestamp: 1632207977
  timesteps_since_restore: 0
  timesteps_total: 279720
  training_iteration: 148
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    148 |           198675 | 279720 |  4.77591 |              7.77595 |              1.73895 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 281610
  custom_metrics: {}
  date: 2021-09-21_00-40-49
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.802536712287775
  episode_reward_mean: 4.792562371609146
  episode_reward_min: 2.5605557757425825
  episodes_this_iter: 270
  episodes_total: 40230
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.188000017777085e-05
          entropy: 11.791391372680664
          entropy_coeff: 0.00035202960134483874
          kl: 0.01349283754825592
          model: {}
          policy_loss: -0.21663250029087067
          total_loss: -0.1689978688955307
          vf_explained_var: 0.9936385750770569
          vf_loss: 0.02104719541966915
    num_agent_steps_sampled: 281610
    num_agent_steps_trained: 281610
    num_steps_sampled: 281610
    num_steps_trained: 281610
  iterations_since_restore: 149
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.09160887656033
    ram_util_percent: 7.8999999999999995
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10019547535356868
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3143.2898519917067
    mean_inference_ms: 2.407219515339472
    mean_raw_obs_processing_ms: 225.38705871941644
  time_since_restore: 200746.87026143074
  time_this_iter_s: 2071.911834716797
  time_total_s: 200746.87026143074
  timers:
    learn_throughput: 711.849
    learn_time_ms: 2655.057
    load_throughput: 179076.532
    load_time_ms: 10.554
    sample_throughput: 1.387
    sample_time_ms: 1362416.999
    update_time_ms: 28.281
  timestamp: 1632210049
  timesteps_since_restore: 0
  timesteps_total: 281610
  training_iteration: 149
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    149 |           200747 | 281610 |  4.79256 |              7.80254 |              2.56056 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 283500
  custom_metrics: {}
  date: 2021-09-21_00-58-01
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.227910687012628
  episode_reward_mean: 4.766353072473637
  episode_reward_min: 1.9301786628468123
  episodes_this_iter: 270
  episodes_total: 40500
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.169000309659168e-05
          entropy: 11.719731330871582
          entropy_coeff: 0.00035102979745715857
          kl: 0.014953339472413063
          model: {}
          policy_loss: -0.17158088088035583
          total_loss: -0.12161680310964584
          vf_explained_var: 0.9939575791358948
          vf_loss: 0.02001247927546501
    num_agent_steps_sampled: 283500
    num_agent_steps_trained: 283500
    num_steps_sampled: 283500
    num_steps_trained: 283500
  iterations_since_restore: 150
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.04243902439025
    ram_util_percent: 7.984250871080139
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10018486633413756
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3144.5274591315633
    mean_inference_ms: 2.405747140563667
    mean_raw_obs_processing_ms: 225.23866646011635
  time_since_restore: 201779.04364156723
  time_this_iter_s: 1032.1733801364899
  time_total_s: 201779.04364156723
  timers:
    learn_throughput: 711.703
    learn_time_ms: 2655.6
    load_throughput: 178164.125
    load_time_ms: 10.608
    sample_throughput: 1.474
    sample_time_ms: 1282587.071
    update_time_ms: 28.536
  timestamp: 1632211081
  timesteps_since_restore: 0
  timesteps_total: 283500
  training_iteration: 150
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 21.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    150 |           201779 | 283500 |  4.76635 |              8.22791 |              1.93018 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 285390
  custom_metrics: {}
  date: 2021-09-21_01-09-59
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.824823628483495
  episode_reward_mean: 4.796831911221675
  episode_reward_min: 1.6123237333868958
  episodes_this_iter: 270
  episodes_total: 40770
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.14999987394549e-05
          entropy: 11.727235794067383
          entropy_coeff: 0.0003500299935694784
          kl: 0.01616668701171875
          model: {}
          policy_loss: -0.18843774497509003
          total_loss: -0.13266243040561676
          vf_explained_var: 0.9929307699203491
          vf_loss: 0.023050466552376747
    num_agent_steps_sampled: 285390
    num_agent_steps_trained: 285390
    num_steps_sampled: 285390
    num_steps_trained: 285390
  iterations_since_restore: 151
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.131131131131134
    ram_util_percent: 8.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10018029633383338
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3140.470792594865
    mean_inference_ms: 2.4046533018619374
    mean_raw_obs_processing_ms: 225.10217657190273
  time_since_restore: 202497.0818989277
  time_this_iter_s: 718.0382573604584
  time_total_s: 202497.0818989277
  timers:
    learn_throughput: 712.675
    learn_time_ms: 2651.979
    load_throughput: 163916.565
    load_time_ms: 11.53
    sample_throughput: 1.524
    sample_time_ms: 1240137.188
    update_time_ms: 28.63
  timestamp: 1632211799
  timesteps_since_restore: 0
  timesteps_total: 285390
  training_iteration: 151
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    151 |           202497 | 285390 |  4.79683 |              7.82482 |              1.61232 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 287280
  custom_metrics: {}
  date: 2021-09-21_01-19-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.505406246413046
  episode_reward_mean: 4.886047116957286
  episode_reward_min: 2.05189231897757
  episodes_this_iter: 270
  episodes_total: 41040
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.131000165827572e-05
          entropy: 11.669323921203613
          entropy_coeff: 0.0003490301896817982
          kl: 0.015465527772903442
          model: {}
          policy_loss: -0.1698187291622162
          total_loss: -0.11512800306081772
          vf_explained_var: 0.9933915734291077
          vf_loss: 0.023531269282102585
    num_agent_steps_sampled: 287280
    num_agent_steps_trained: 287280
    num_steps_sampled: 287280
    num_steps_trained: 287280
  iterations_since_restore: 152
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.63423312883436
    ram_util_percent: 8.044294478527606
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10016033400296066
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3132.535030095958
    mean_inference_ms: 2.4035613944920655
    mean_raw_obs_processing_ms: 224.9534879026832
  time_since_restore: 203082.38033938408
  time_this_iter_s: 585.2984404563904
  time_total_s: 203082.38033938408
  timers:
    learn_throughput: 713.235
    learn_time_ms: 2649.897
    load_throughput: 163918.599
    load_time_ms: 11.53
    sample_throughput: 1.506
    sample_time_ms: 1254754.752
    update_time_ms: 28.732
  timestamp: 1632212384
  timesteps_since_restore: 0
  timesteps_total: 287280
  training_iteration: 152
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    152 |           203082 | 287280 |  4.88605 |              7.50541 |              2.05189 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 289170
  custom_metrics: {}
  date: 2021-09-21_01-32-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.774885951276886
  episode_reward_mean: 4.741927190878498
  episode_reward_min: 1.9849124311683424
  episodes_this_iter: 270
  episodes_total: 41310
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.111999730113894e-05
          entropy: 11.778304100036621
          entropy_coeff: 0.00034803038579411805
          kl: 0.016740716993808746
          model: {}
          policy_loss: -0.18485772609710693
          total_loss: -0.12918119132518768
          vf_explained_var: 0.9931530952453613
          vf_loss: 0.02163826860487461
    num_agent_steps_sampled: 289170
    num_agent_steps_trained: 289170
    num_steps_sampled: 289170
    num_steps_trained: 289170
  iterations_since_restore: 153
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.51865889212828
    ram_util_percent: 8.099999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10013394321078975
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3124.686237177702
    mean_inference_ms: 2.4021781135121714
    mean_raw_obs_processing_ms: 224.8101145869143
  time_since_restore: 203821.83265805244
  time_this_iter_s: 739.4523186683655
  time_total_s: 203821.83265805244
  timers:
    learn_throughput: 712.996
    learn_time_ms: 2650.788
    load_throughput: 164610.591
    load_time_ms: 11.482
    sample_throughput: 1.676
    sample_time_ms: 1127871.235
    update_time_ms: 28.62
  timestamp: 1632213124
  timesteps_since_restore: 0
  timesteps_total: 289170
  training_iteration: 153
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    153 |           203822 | 289170 |  4.74193 |              7.77489 |              1.98491 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 291060
  custom_metrics: {}
  date: 2021-09-21_01-50-29
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.074435202356256
  episode_reward_mean: 4.753560464691328
  episode_reward_min: 1.670969225935187
  episodes_this_iter: 270
  episodes_total: 41580
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.093000021995977e-05
          entropy: 11.725753784179688
          entropy_coeff: 0.00034703061101026833
          kl: 0.014226669445633888
          model: {}
          policy_loss: -0.1876261979341507
          total_loss: -0.13782036304473877
          vf_explained_var: 0.9936633706092834
          vf_loss: 0.02146492525935173
    num_agent_steps_sampled: 291060
    num_agent_steps_trained: 291060
    num_steps_sampled: 291060
    num_steps_trained: 291060
  iterations_since_restore: 154
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.306887589343724
    ram_util_percent: 8.143209876543208
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10011922744307929
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3119.6337840168144
    mean_inference_ms: 2.401185269461312
    mean_raw_obs_processing_ms: 224.65687508123665
  time_since_restore: 204927.23123383522
  time_this_iter_s: 1105.3985757827759
  time_total_s: 204927.23123383522
  timers:
    learn_throughput: 712.992
    learn_time_ms: 2650.802
    load_throughput: 161646.691
    load_time_ms: 11.692
    sample_throughput: 1.664
    sample_time_ms: 1136031.383
    update_time_ms: 28.673
  timestamp: 1632214229
  timesteps_since_restore: 0
  timesteps_total: 291060
  training_iteration: 154
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    154 |           204927 | 291060 |  4.75356 |              8.07444 |              1.67097 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 292950
  custom_metrics: {}
  date: 2021-09-21_01-59-20
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.788249585749168
  episode_reward_mean: 4.882945009840215
  episode_reward_min: 1.7397528284390402
  episodes_this_iter: 270
  episodes_total: 41850
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.07400031387806e-05
          entropy: 11.682610511779785
          entropy_coeff: 0.00034603080712258816
          kl: 0.015231585130095482
          model: {}
          policy_loss: -0.17391236126422882
          total_loss: -0.1206757053732872
          vf_explained_var: 0.9934967756271362
          vf_loss: 0.022579718381166458
    num_agent_steps_sampled: 292950
    num_agent_steps_trained: 292950
    num_steps_sampled: 292950
    num_steps_trained: 292950
  iterations_since_restore: 155
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.2769647696477
    ram_util_percent: 8.200000000000001
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10009988241743802
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3110.661195937094
    mean_inference_ms: 2.400178491336096
    mean_raw_obs_processing_ms: 224.51166729987509
  time_since_restore: 205457.84732794762
  time_this_iter_s: 530.6160941123962
  time_total_s: 205457.84732794762
  timers:
    learn_throughput: 712.986
    learn_time_ms: 2650.825
    load_throughput: 161236.043
    load_time_ms: 11.722
    sample_throughput: 1.642
    sample_time_ms: 1151154.882
    update_time_ms: 28.796
  timestamp: 1632214760
  timesteps_since_restore: 0
  timesteps_total: 292950
  training_iteration: 155
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    155 |           205458 | 292950 |  4.88295 |              7.78825 |              1.73975 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 294840
  custom_metrics: {}
  date: 2021-09-21_02-08-48
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.560463877841351
  episode_reward_mean: 4.759438873954137
  episode_reward_min: 1.5436362913742145
  episodes_this_iter: 270
  episodes_total: 42120
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.054999878164381e-05
          entropy: 11.679346084594727
          entropy_coeff: 0.000345031003234908
          kl: 0.014452056027948856
          model: {}
          policy_loss: -0.19731128215789795
          total_loss: -0.1465977281332016
          vf_explained_var: 0.9935370087623596
          vf_loss: 0.0218197088688612
    num_agent_steps_sampled: 294840
    num_agent_steps_trained: 294840
    num_steps_sampled: 294840
    num_steps_trained: 294840
  iterations_since_restore: 156
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.77016434892541
    ram_util_percent: 8.374462705436155
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10011071703759115
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3099.469518323643
    mean_inference_ms: 2.3989577963853534
    mean_raw_obs_processing_ms: 224.3762603025536
  time_since_restore: 206026.34470653534
  time_this_iter_s: 568.4973785877228
  time_total_s: 206026.34470653534
  timers:
    learn_throughput: 712.713
    learn_time_ms: 2651.838
    load_throughput: 160894.066
    load_time_ms: 11.747
    sample_throughput: 1.671
    sample_time_ms: 1131242.0
    update_time_ms: 28.844
  timestamp: 1632215328
  timesteps_since_restore: 0
  timesteps_total: 294840
  training_iteration: 156
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    156 |           206026 | 294840 |  4.75944 |              7.56046 |              1.54364 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 296730
  custom_metrics: {}
  date: 2021-09-21_02-16-21
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.263512321138402
  episode_reward_mean: 4.788748129231917
  episode_reward_min: 1.9793893992237832
  episodes_this_iter: 270
  episodes_total: 42390
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.036000170046464e-05
          entropy: 11.679951667785645
          entropy_coeff: 0.0003440311993472278
          kl: 0.013661597855389118
          model: {}
          policy_loss: -0.17990411818027496
          total_loss: -0.1318136304616928
          vf_explained_var: 0.9937111139297485
          vf_loss: 0.02098594792187214
    num_agent_steps_sampled: 296730
    num_agent_steps_trained: 296730
    num_steps_sampled: 296730
    num_steps_trained: 296730
  iterations_since_restore: 157
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.82142857142857
    ram_util_percent: 8.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10008320601125059
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3087.625794577514
    mean_inference_ms: 2.3977756251276565
    mean_raw_obs_processing_ms: 224.2251149404144
  time_since_restore: 206479.02210712433
  time_this_iter_s: 452.67740058898926
  time_total_s: 206479.02210712433
  timers:
    learn_throughput: 713.161
    learn_time_ms: 2650.174
    load_throughput: 161179.656
    load_time_ms: 11.726
    sample_throughput: 1.725
    sample_time_ms: 1095930.093
    update_time_ms: 29.032
  timestamp: 1632215781
  timesteps_since_restore: 0
  timesteps_total: 296730
  training_iteration: 157
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    157 |           206479 | 296730 |  4.78875 |              7.26351 |              1.97939 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 298620
  custom_metrics: {}
  date: 2021-09-21_02-31-18
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.885723156626776
  episode_reward_mean: 4.846373615030795
  episode_reward_min: 1.9318707022883843
  episodes_this_iter: 270
  episodes_total: 42660
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.016999734332785e-05
          entropy: 11.69422721862793
          entropy_coeff: 0.00034303139545954764
          kl: 0.014372446574270725
          model: {}
          policy_loss: -0.18612436950206757
          total_loss: -0.1352330893278122
          vf_explained_var: 0.9936116337776184
          vf_loss: 0.022160544991493225
    num_agent_steps_sampled: 298620
    num_agent_steps_trained: 298620
    num_steps_sampled: 298620
    num_steps_trained: 298620
  iterations_since_restore: 158
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.31282051282051
    ram_util_percent: 9.256490384615384
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10007160405941411
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3086.2938696182023
    mean_inference_ms: 2.39676440985711
    mean_raw_obs_processing_ms: 224.07800329174248
  time_since_restore: 207375.9231889248
  time_this_iter_s: 896.9010818004608
  time_total_s: 207375.9231889248
  timers:
    learn_throughput: 712.501
    learn_time_ms: 2652.629
    load_throughput: 159440.107
    load_time_ms: 11.854
    sample_throughput: 2.179
    sample_time_ms: 867272.012
    update_time_ms: 29.061
  timestamp: 1632216678
  timesteps_since_restore: 0
  timesteps_total: 298620
  training_iteration: 158
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    158 |           207376 | 298620 |  4.84637 |              7.88572 |              1.93187 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 300510
  custom_metrics: {}
  date: 2021-09-21_02-45-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.645891603214358
  episode_reward_mean: 4.796103187396008
  episode_reward_min: 2.0814852404465203
  episodes_this_iter: 270
  episodes_total: 42930
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.998000026214868e-05
          entropy: 11.612730026245117
          entropy_coeff: 0.00034203159157186747
          kl: 0.01605142094194889
          model: {}
          policy_loss: -0.18477654457092285
          total_loss: -0.1300630271434784
          vf_explained_var: 0.993273913860321
          vf_loss: 0.02211831323802471
    num_agent_steps_sampled: 300510
    num_agent_steps_trained: 300510
    num_steps_sampled: 300510
    num_steps_trained: 300510
  iterations_since_restore: 159
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.11583747927031
    ram_util_percent: 9.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10008592124542706
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3083.755617999458
    mean_inference_ms: 2.3955797787886635
    mean_raw_obs_processing_ms: 223.93633685322706
  time_since_restore: 208242.9559173584
  time_this_iter_s: 867.032728433609
  time_total_s: 208242.9559173584
  timers:
    learn_throughput: 711.962
    learn_time_ms: 2654.635
    load_throughput: 159437.221
    load_time_ms: 11.854
    sample_throughput: 2.531
    sample_time_ms: 746782.061
    update_time_ms: 28.955
  timestamp: 1632217545
  timesteps_since_restore: 0
  timesteps_total: 300510
  training_iteration: 159
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    159 |           208243 | 300510 |   4.7961 |              7.64589 |              2.08149 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 302400
  custom_metrics: {}
  date: 2021-09-21_03-04-57
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.554177728655431
  episode_reward_mean: 4.852671072708913
  episode_reward_min: 2.304308938039335
  episodes_this_iter: 270
  episodes_total: 43200
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.97900031809695e-05
          entropy: 11.608283042907715
          entropy_coeff: 0.0003410317876841873
          kl: 0.015066919848322868
          model: {}
          policy_loss: -0.1846485137939453
          total_loss: -0.13247863948345184
          vf_explained_var: 0.9937341809272766
          vf_loss: 0.021804319694638252
    num_agent_steps_sampled: 302400
    num_agent_steps_trained: 302400
    num_steps_sampled: 302400
    num_steps_trained: 302400
  iterations_since_restore: 160
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.86487835308796
    ram_util_percent: 9.20361821584529
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10005761806837246
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3080.2717838151493
    mean_inference_ms: 2.3945331991163115
    mean_raw_obs_processing_ms: 223.79810745428017
  time_since_restore: 209394.87349033356
  time_this_iter_s: 1151.9175729751587
  time_total_s: 209394.87349033356
  timers:
    learn_throughput: 711.237
    learn_time_ms: 2657.342
    load_throughput: 160531.428
    load_time_ms: 11.773
    sample_throughput: 2.491
    sample_time_ms: 758754.619
    update_time_ms: 29.062
  timestamp: 1632218697
  timesteps_since_restore: 0
  timesteps_total: 302400
  training_iteration: 160
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    160 |           209395 | 302400 |  4.85267 |              7.55418 |              2.30431 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 304290
  custom_metrics: {}
  date: 2021-09-21_03-14-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.78810163817922
  episode_reward_mean: 4.82619165106152
  episode_reward_min: 1.8226130417924482
  episodes_this_iter: 270
  episodes_total: 43470
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.959999882383272e-05
          entropy: 11.678601264953613
          entropy_coeff: 0.0003400320129003376
          kl: 0.015501489862799644
          model: {}
          policy_loss: -0.19329200685024261
          total_loss: -0.14032135903835297
          vf_explained_var: 0.9937002062797546
          vf_loss: 0.021627426147460938
    num_agent_steps_sampled: 304290
    num_agent_steps_trained: 304290
    num_steps_sampled: 304290
    num_steps_trained: 304290
  iterations_since_restore: 161
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.57953144266338
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10004298268177442
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3069.477711149142
    mean_inference_ms: 2.3935626486481985
    mean_raw_obs_processing_ms: 223.66466938083647
  time_since_restore: 209977.45305776596
  time_this_iter_s: 582.5795674324036
  time_total_s: 209977.45305776596
  timers:
    learn_throughput: 710.561
    learn_time_ms: 2659.869
    load_throughput: 174431.186
    load_time_ms: 10.835
    sample_throughput: 2.536
    sample_time_ms: 745207.62
    update_time_ms: 29.052
  timestamp: 1632219280
  timesteps_since_restore: 0
  timesteps_total: 304290
  training_iteration: 161
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    161 |           209977 | 304290 |  4.82619 |               7.7881 |              1.82261 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 306180
  custom_metrics: {}
  date: 2021-09-21_03-30-19
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.065569605264721
  episode_reward_mean: 4.824966100753767
  episode_reward_min: 1.557705801517954
  episodes_this_iter: 270
  episodes_total: 43740
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.941000174265355e-05
          entropy: 11.64168643951416
          entropy_coeff: 0.0003390322090126574
          kl: 0.014636474661529064
          model: {}
          policy_loss: -0.18622252345085144
          total_loss: -0.1368575245141983
          vf_explained_var: 0.9939984083175659
          vf_loss: 0.0199681855738163
    num_agent_steps_sampled: 306180
    num_agent_steps_trained: 306180
    num_steps_sampled: 306180
    num_steps_trained: 306180
  iterations_since_restore: 162
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.1362662586075
    ram_util_percent: 9.091736801836264
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10001956370634076
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3061.2454131859777
    mean_inference_ms: 2.392471880204721
    mean_raw_obs_processing_ms: 223.52951984923274
  time_since_restore: 210916.97738695145
  time_this_iter_s: 939.5243291854858
  time_total_s: 210916.97738695145
  timers:
    learn_throughput: 709.604
    learn_time_ms: 2663.456
    load_throughput: 174172.11
    load_time_ms: 10.851
    sample_throughput: 2.421
    sample_time_ms: 780626.37
    update_time_ms: 29.007
  timestamp: 1632220219
  timesteps_since_restore: 0
  timesteps_total: 306180
  training_iteration: 162
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    162 |           210917 | 306180 |  4.82497 |              7.06557 |              1.55771 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 308070
  custom_metrics: {}
  date: 2021-09-21_05-12-30
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.324026892692608
  episode_reward_mean: 4.793973331894136
  episode_reward_min: 1.826250097841052
  episodes_this_iter: 270
  episodes_total: 44010
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.921999738551676e-05
          entropy: 11.64697551727295
          entropy_coeff: 0.00033803240512497723
          kl: 0.016847552731633186
          model: {}
          policy_loss: -0.19171813130378723
          total_loss: -0.13402123749256134
          vf_explained_var: 0.9929900765419006
          vf_loss: 0.023253098130226135
    num_agent_steps_sampled: 308070
    num_agent_steps_trained: 308070
    num_steps_sampled: 308070
    num_steps_trained: 308070
  iterations_since_restore: 163
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.94601593625498
    ram_util_percent: 9.017951722521676
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10005502457440096
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3073.250114253837
    mean_inference_ms: 2.3916545596303522
    mean_raw_obs_processing_ms: 223.3990677318838
  time_since_restore: 217047.5398030281
  time_this_iter_s: 6130.56241607666
  time_total_s: 217047.5398030281
  timers:
    learn_throughput: 709.23
    learn_time_ms: 2664.863
    load_throughput: 173458.288
    load_time_ms: 10.896
    sample_throughput: 1.432
    sample_time_ms: 1319735.113
    update_time_ms: 29.372
  timestamp: 1632226350
  timesteps_since_restore: 0
  timesteps_total: 308070
  training_iteration: 163
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    163 |           217048 | 308070 |  4.79397 |              7.32403 |              1.82625 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 309960
  custom_metrics: {}
  date: 2021-09-21_05-22-53
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.103485898366378
  episode_reward_mean: 4.837824594617409
  episode_reward_min: 1.7297110624838477
  episodes_this_iter: 270
  episodes_total: 44280
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.903000030433759e-05
          entropy: 11.766846656799316
          entropy_coeff: 0.00033703260123729706
          kl: 0.015468998812139034
          model: {}
          policy_loss: -0.17571641504764557
          total_loss: -0.12268096953630447
          vf_explained_var: 0.9937825202941895
          vf_loss: 0.02176094986498356
    num_agent_steps_sampled: 309960
    num_agent_steps_trained: 309960
    num_steps_sampled: 309960
    num_steps_trained: 309960
  iterations_since_restore: 164
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.00427251732101
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10004616761676967
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3066.982579151686
    mean_inference_ms: 2.390883539740151
    mean_raw_obs_processing_ms: 223.2779088501091
  time_since_restore: 217670.3495604992
  time_this_iter_s: 622.8097574710846
  time_total_s: 217670.3495604992
  timers:
    learn_throughput: 708.885
    learn_time_ms: 2666.157
    load_throughput: 175660.331
    load_time_ms: 10.759
    sample_throughput: 1.486
    sample_time_ms: 1271475.564
    update_time_ms: 29.289
  timestamp: 1632226973
  timesteps_since_restore: 0
  timesteps_total: 309960
  training_iteration: 164
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    164 |           217670 | 309960 |  4.83782 |              8.10349 |              1.72971 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 311850
  custom_metrics: {}
  date: 2021-09-21_05-35-58
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.117980154129668
  episode_reward_mean: 4.797740720940157
  episode_reward_min: 1.71817011934521
  episodes_this_iter: 270
  episodes_total: 44550
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.884000322315842e-05
          entropy: 11.767109870910645
          entropy_coeff: 0.0003360327973496169
          kl: 0.016103286296129227
          model: {}
          policy_loss: -0.1974194198846817
          total_loss: -0.14152558147907257
          vf_explained_var: 0.9932289719581604
          vf_loss: 0.02316267415881157
    num_agent_steps_sampled: 311850
    num_agent_steps_trained: 311850
    num_steps_sampled: 311850
    num_steps_trained: 311850
  iterations_since_restore: 165
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.03608058608059
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10003607221556593
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3062.5753774943582
    mean_inference_ms: 2.389923584759823
    mean_raw_obs_processing_ms: 223.13468658360696
  time_since_restore: 218454.87047100067
  time_this_iter_s: 784.5209105014801
  time_total_s: 218454.87047100067
  timers:
    learn_throughput: 708.089
    learn_time_ms: 2669.155
    load_throughput: 175427.869
    load_time_ms: 10.774
    sample_throughput: 1.457
    sample_time_ms: 1296863.241
    update_time_ms: 29.478
  timestamp: 1632227758
  timesteps_since_restore: 0
  timesteps_total: 311850
  training_iteration: 165
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    165 |           218455 | 311850 |  4.79774 |              8.11798 |              1.71817 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 313740
  custom_metrics: {}
  date: 2021-09-21_06-00-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.821076808145812
  episode_reward_mean: 4.769518645080127
  episode_reward_min: 1.638631481673854
  episodes_this_iter: 270
  episodes_total: 44820
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.864999886602163e-05
          entropy: 11.712288856506348
          entropy_coeff: 0.0003350329934619367
          kl: 0.015997353941202164
          model: {}
          policy_loss: -0.18283142149448395
          total_loss: -0.12714077532291412
          vf_explained_var: 0.9930258989334106
          vf_loss: 0.023170677945017815
    num_agent_steps_sampled: 313740
    num_agent_steps_trained: 313740
    num_steps_sampled: 313740
    num_steps_trained: 313740
  iterations_since_restore: 166
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.59089588377723
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10001652898754271
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3064.509144501589
    mean_inference_ms: 2.3888072838397765
    mean_raw_obs_processing_ms: 223.0094303205544
  time_since_restore: 219939.35577249527
  time_this_iter_s: 1484.4853014945984
  time_total_s: 219939.35577249527
  timers:
    learn_throughput: 708.211
    learn_time_ms: 2668.694
    load_throughput: 175345.606
    load_time_ms: 10.779
    sample_throughput: 1.361
    sample_time_ms: 1388462.719
    update_time_ms: 29.553
  timestamp: 1632229242
  timesteps_since_restore: 0
  timesteps_total: 313740
  training_iteration: 166
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    166 |           219939 | 313740 |  4.76952 |              7.82108 |              1.63863 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 315630
  custom_metrics: {}
  date: 2021-09-21_06-18-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.416232618799535
  episode_reward_mean: 4.789255235636931
  episode_reward_min: 1.5191988174283304
  episodes_this_iter: 270
  episodes_total: 45090
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.846000178484246e-05
          entropy: 11.670794486999512
          entropy_coeff: 0.00033403318957425654
          kl: 0.016409268602728844
          model: {}
          policy_loss: -0.18451453745365143
          total_loss: -0.12875983119010925
          vf_explained_var: 0.9934086203575134
          vf_loss: 0.022270794957876205
    num_agent_steps_sampled: 315630
    num_agent_steps_trained: 315630
    num_steps_sampled: 315630
    num_steps_trained: 315630
  iterations_since_restore: 167
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.09661803713528
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10001519369161277
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3068.3005091398513
    mean_inference_ms: 2.3880014154834894
    mean_raw_obs_processing_ms: 222.87187510604537
  time_since_restore: 221023.15748405457
  time_this_iter_s: 1083.8017115592957
  time_total_s: 221023.15748405457
  timers:
    learn_throughput: 708.078
    learn_time_ms: 2669.198
    load_throughput: 174483.401
    load_time_ms: 10.832
    sample_throughput: 1.302
    sample_time_ms: 1451575.084
    update_time_ms: 29.231
  timestamp: 1632230326
  timesteps_since_restore: 0
  timesteps_total: 315630
  training_iteration: 167
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    167 |           221023 | 315630 |  4.78926 |              7.41623 |               1.5192 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 317520
  custom_metrics: {}
  date: 2021-09-21_06-27-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.775132844420545
  episode_reward_mean: 4.847854844627119
  episode_reward_min: 1.8626002223760312
  episodes_this_iter: 270
  episodes_total: 45360
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.826999742770568e-05
          entropy: 11.717726707458496
          entropy_coeff: 0.00033303338568657637
          kl: 0.014500828459858894
          model: {}
          policy_loss: -0.1600298136472702
          total_loss: -0.1088799312710762
          vf_explained_var: 0.9936546683311462
          vf_loss: 0.02201761305332184
    num_agent_steps_sampled: 317520
    num_agent_steps_trained: 317520
    num_steps_sampled: 317520
    num_steps_trained: 317520
  iterations_since_restore: 168
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.22302543507362
    ram_util_percent: 9.000133868808568
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09999599823960204
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3061.715317165003
    mean_inference_ms: 2.3870512430090436
    mean_raw_obs_processing_ms: 222.74085851695938
  time_since_restore: 221559.65620183945
  time_this_iter_s: 536.4987177848816
  time_total_s: 221559.65620183945
  timers:
    learn_throughput: 709.019
    learn_time_ms: 2665.654
    load_throughput: 175978.142
    load_time_ms: 10.74
    sample_throughput: 1.335
    sample_time_ms: 1415538.631
    update_time_ms: 29.685
  timestamp: 1632230862
  timesteps_since_restore: 0
  timesteps_total: 317520
  training_iteration: 168
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    168 |           221560 | 317520 |  4.84785 |              7.77513 |               1.8626 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 319410
  custom_metrics: {}
  date: 2021-09-21_07-01-34
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.417444638010896
  episode_reward_mean: 4.841825482182247
  episode_reward_min: 2.0805042683324886
  episodes_this_iter: 270
  episodes_total: 45630
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.80800003465265e-05
          entropy: 11.798386573791504
          entropy_coeff: 0.00033203361090272665
          kl: 0.014439644291996956
          model: {}
          policy_loss: -0.19479285180568695
          total_loss: -0.14198558032512665
          vf_explained_var: 0.9929597973823547
          vf_loss: 0.023829404264688492
    num_agent_steps_sampled: 319410
    num_agent_steps_trained: 319410
    num_steps_sampled: 319410
    num_steps_trained: 319410
  iterations_since_restore: 169
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.106123893805304
    ram_util_percent: 8.95837168141593
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09998167668553758
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3063.8413900911887
    mean_inference_ms: 2.3860680812435837
    mean_raw_obs_processing_ms: 222.60378123460083
  time_since_restore: 223591.01162719727
  time_this_iter_s: 2031.3554253578186
  time_total_s: 223591.01162719727
  timers:
    learn_throughput: 708.861
    learn_time_ms: 2666.248
    load_throughput: 176742.841
    load_time_ms: 10.694
    sample_throughput: 1.234
    sample_time_ms: 1531969.378
    update_time_ms: 29.946
  timestamp: 1632232894
  timesteps_since_restore: 0
  timesteps_total: 319410
  training_iteration: 169
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    169 |           223591 | 319410 |  4.84183 |              7.41744 |               2.0805 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 321300
  custom_metrics: {}
  date: 2021-09-21_07-06-56
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6743611522293005
  episode_reward_mean: 4.7700961763080745
  episode_reward_min: 2.1013483433906694
  episodes_this_iter: 270
  episodes_total: 45900
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.789000326534733e-05
          entropy: 11.723234176635742
          entropy_coeff: 0.0003310338070150465
          kl: 0.014633511193096638
          model: {}
          policy_loss: -0.18238653242588043
          total_loss: -0.13068898022174835
          vf_explained_var: 0.9932575225830078
          vf_loss: 0.022241374477744102
    num_agent_steps_sampled: 321300
    num_agent_steps_trained: 321300
    num_steps_sampled: 321300
    num_steps_trained: 321300
  iterations_since_restore: 170
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.255803571428565
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09998495424819422
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3053.4315339263976
    mean_inference_ms: 2.38579782281867
    mean_raw_obs_processing_ms: 222.49979232590962
  time_since_restore: 223913.01308321953
  time_this_iter_s: 322.0014560222626
  time_total_s: 223913.01308321953
  timers:
    learn_throughput: 708.44
    learn_time_ms: 2667.835
    load_throughput: 176548.783
    load_time_ms: 10.705
    sample_throughput: 1.304
    sample_time_ms: 1448975.632
    update_time_ms: 30.417
  timestamp: 1632233216
  timesteps_since_restore: 0
  timesteps_total: 321300
  training_iteration: 170
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    170 |           223913 | 321300 |   4.7701 |              7.67436 |              2.10135 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 323190
  custom_metrics: {}
  date: 2021-09-21_07-21-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.761101185151862
  episode_reward_mean: 4.822396495412883
  episode_reward_min: 1.742374734891866
  episodes_this_iter: 270
  episodes_total: 46170
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.769999890821055e-05
          entropy: 11.752336502075195
          entropy_coeff: 0.0003300340031273663
          kl: 0.013798721134662628
          model: {}
          policy_loss: -0.18269774317741394
          total_loss: -0.13315698504447937
          vf_explained_var: 0.9933112263679504
          vf_loss: 0.021984219551086426
    num_agent_steps_sampled: 323190
    num_agent_steps_trained: 323190
    num_steps_sampled: 323190
    num_steps_trained: 323190
  iterations_since_restore: 171
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.842834008097164
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09996247901760176
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3045.865563960655
    mean_inference_ms: 2.385222902432917
    mean_raw_obs_processing_ms: 222.3897259633392
  time_since_restore: 224800.46187758446
  time_this_iter_s: 887.4487943649292
  time_total_s: 224800.46187758446
  timers:
    learn_throughput: 708.829
    learn_time_ms: 2666.369
    load_throughput: 176745.6
    load_time_ms: 10.693
    sample_throughput: 1.277
    sample_time_ms: 1479463.402
    update_time_ms: 30.881
  timestamp: 1632234103
  timesteps_since_restore: 0
  timesteps_total: 323190
  training_iteration: 171
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    171 |           224800 | 323190 |   4.8224 |               7.7611 |              1.74237 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 325080
  custom_metrics: {}
  date: 2021-09-21_07-38-56
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.5482980725059825
  episode_reward_mean: 4.827127436169745
  episode_reward_min: 1.2642039182445728
  episodes_this_iter: 270
  episodes_total: 46440
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.751000182703137e-05
          entropy: 11.753851890563965
          entropy_coeff: 0.00032903419923968613
          kl: 0.015352271497249603
          model: {}
          policy_loss: -0.18290673196315765
          total_loss: -0.12772305309772491
          vf_explained_var: 0.9926890134811401
          vf_loss: 0.024076715111732483
    num_agent_steps_sampled: 325080
    num_agent_steps_trained: 325080
    num_steps_sampled: 325080
    num_steps_trained: 325080
  iterations_since_restore: 172
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.412325905292484
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10000280375527792
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3044.6448785829866
    mean_inference_ms: 2.3847567852397904
    mean_raw_obs_processing_ms: 222.28211255655125
  time_since_restore: 225832.68869066238
  time_this_iter_s: 1032.2268130779266
  time_total_s: 225832.68869066238
  timers:
    learn_throughput: 708.48
    learn_time_ms: 2667.681
    load_throughput: 175624.139
    load_time_ms: 10.762
    sample_throughput: 1.27
    sample_time_ms: 1488732.096
    update_time_ms: 31.343
  timestamp: 1632235136
  timesteps_since_restore: 0
  timesteps_total: 325080
  training_iteration: 172
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    172 |           225833 | 325080 |  4.82713 |               7.5483 |               1.2642 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 326970
  custom_metrics: {}
  date: 2021-09-21_08-18-02
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.617955045545944
  episode_reward_mean: 4.892149444441804
  episode_reward_min: 1.9484697909123976
  episodes_this_iter: 270
  episodes_total: 46710
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.731999746989459e-05
          entropy: 11.746542930603027
          entropy_coeff: 0.00032803439535200596
          kl: 0.01577804610133171
          model: {}
          policy_loss: -0.18446435034275055
          total_loss: -0.1312243640422821
          vf_explained_var: 0.9938738942146301
          vf_loss: 0.021148893982172012
    num_agent_steps_sampled: 326970
    num_agent_steps_trained: 326970
    num_steps_sampled: 326970
    num_steps_trained: 326970
  iterations_since_restore: 173
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.49920343137255
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10002078022530238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3050.574298488141
    mean_inference_ms: 2.3842054728208297
    mean_raw_obs_processing_ms: 222.1693034115279
  time_since_restore: 228179.301081419
  time_this_iter_s: 2346.612390756607
  time_total_s: 228179.301081419
  timers:
    learn_throughput: 708.341
    learn_time_ms: 2668.207
    load_throughput: 175086.515
    load_time_ms: 10.795
    sample_throughput: 1.702
    sample_time_ms: 1110336.886
    update_time_ms: 31.141
  timestamp: 1632237482
  timesteps_since_restore: 0
  timesteps_total: 326970
  training_iteration: 173
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    173 |           228179 | 326970 |  4.89215 |              7.61796 |              1.94847 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 328860
  custom_metrics: {}
  date: 2021-09-21_08-30-05
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.78596328709073
  episode_reward_mean: 4.887604685182261
  episode_reward_min: 2.185597475468791
  episodes_this_iter: 270
  episodes_total: 46980
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.713000038871542e-05
          entropy: 11.730807304382324
          entropy_coeff: 0.0003270345914643258
          kl: 0.014653444290161133
          model: {}
          policy_loss: -0.1991853415966034
          total_loss: -0.14765770733356476
          vf_explained_var: 0.9937335848808289
          vf_loss: 0.02198163792490959
    num_agent_steps_sampled: 328860
    num_agent_steps_trained: 328860
    num_steps_sampled: 328860
    num_steps_trained: 328860
  iterations_since_restore: 174
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.21432835820895
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1000171501307968
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3051.0183011813897
    mean_inference_ms: 2.3833111503982662
    mean_raw_obs_processing_ms: 222.06765050523833
  time_since_restore: 228902.0523877144
  time_this_iter_s: 722.7513062953949
  time_total_s: 228902.0523877144
  timers:
    learn_throughput: 708.427
    learn_time_ms: 2667.884
    load_throughput: 176383.403
    load_time_ms: 10.715
    sample_throughput: 1.687
    sample_time_ms: 1120331.52
    update_time_ms: 31.803
  timestamp: 1632238205
  timesteps_since_restore: 0
  timesteps_total: 328860
  training_iteration: 174
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    174 |           228902 | 328860 |   4.8876 |              7.78596 |               2.1856 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 330750
  custom_metrics: {}
  date: 2021-09-21_08-47-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.614625497843206
  episode_reward_mean: 4.920888335914141
  episode_reward_min: 2.192487502163009
  episodes_this_iter: 270
  episodes_total: 47250
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.694000330753624e-05
          entropy: 11.67151927947998
          entropy_coeff: 0.0003260347875766456
          kl: 0.016473589465022087
          model: {}
          policy_loss: -0.20384402573108673
          total_loss: -0.14943833649158478
          vf_explained_var: 0.9940264821052551
          vf_loss: 0.02068210579454899
    num_agent_steps_sampled: 330750
    num_agent_steps_trained: 330750
    num_steps_sampled: 330750
    num_steps_trained: 330750
  iterations_since_restore: 175
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.66734837799718
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09999598589535338
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3049.6710226855976
    mean_inference_ms: 2.3828248866552606
    mean_raw_obs_processing_ms: 221.96553864858
  time_since_restore: 229921.27727222443
  time_this_iter_s: 1019.2248845100403
  time_total_s: 229921.27727222443
  timers:
    learn_throughput: 708.664
    learn_time_ms: 2666.989
    load_throughput: 177501.893
    load_time_ms: 10.648
    sample_throughput: 1.652
    sample_time_ms: 1143802.285
    update_time_ms: 31.562
  timestamp: 1632239224
  timesteps_since_restore: 0
  timesteps_total: 330750
  training_iteration: 175
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    175 |           229921 | 330750 |  4.92089 |              7.61463 |              2.19249 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 332640
  custom_metrics: {}
  date: 2021-09-21_08-53-09
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.847066563676539
  episode_reward_mean: 4.79244033114715
  episode_reward_min: 1.6478651293553512
  episodes_this_iter: 270
  episodes_total: 47520
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.674999895039946e-05
          entropy: 11.681913375854492
          entropy_coeff: 0.0003250350127927959
          kl: 0.01501467078924179
          model: {}
          policy_loss: -0.18498684465885162
          total_loss: -0.13190658390522003
          vf_explained_var: 0.9935302734375
          vf_loss: 0.022671997547149658
    num_agent_steps_sampled: 332640
    num_agent_steps_trained: 332640
    num_steps_sampled: 332640
    num_steps_trained: 332640
  iterations_since_restore: 176
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.43893280632411
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09999055255588694
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3041.220409706975
    mean_inference_ms: 2.381960642650353
    mean_raw_obs_processing_ms: 221.85457468623915
  time_since_restore: 230285.47145080566
  time_this_iter_s: 364.1941785812378
  time_total_s: 230285.47145080566
  timers:
    learn_throughput: 708.459
    learn_time_ms: 2667.761
    load_throughput: 177918.204
    load_time_ms: 10.623
    sample_throughput: 1.832
    sample_time_ms: 1031772.068
    update_time_ms: 32.165
  timestamp: 1632239589
  timesteps_since_restore: 0
  timesteps_total: 332640
  training_iteration: 176
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    176 |           230285 | 332640 |  4.79244 |              7.84707 |              1.64787 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 334530
  custom_metrics: {}
  date: 2021-09-21_09-18-59
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.858384684369142
  episode_reward_mean: 4.862559371596858
  episode_reward_min: 2.3526045306695416
  episodes_this_iter: 270
  episodes_total: 47790
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.656000186922029e-05
          entropy: 11.680289268493652
          entropy_coeff: 0.0003240352089051157
          kl: 0.015510394237935543
          model: {}
          policy_loss: -0.18297037482261658
          total_loss: -0.1288670003414154
          vf_explained_var: 0.9932678937911987
          vf_loss: 0.022553568705916405
    num_agent_steps_sampled: 334530
    num_agent_steps_trained: 334530
    num_steps_sampled: 334530
    num_steps_trained: 334530
  iterations_since_restore: 177
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.915306122448975
    ram_util_percent: 9.006725417439704
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10000204332199031
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3040.032842005284
    mean_inference_ms: 2.3816141396092614
    mean_raw_obs_processing_ms: 221.74168037349125
  time_since_restore: 231835.34925174713
  time_this_iter_s: 1549.8778009414673
  time_total_s: 231835.34925174713
  timers:
    learn_throughput: 707.979
    learn_time_ms: 2669.57
    load_throughput: 178101.681
    load_time_ms: 10.612
    sample_throughput: 1.753
    sample_time_ms: 1078377.426
    update_time_ms: 32.597
  timestamp: 1632241139
  timesteps_since_restore: 0
  timesteps_total: 334530
  training_iteration: 177
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    177 |           231835 | 334530 |  4.86256 |              7.85838 |               2.3526 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 336420
  custom_metrics: {}
  date: 2021-09-21_09-44-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.668409927299965
  episode_reward_mean: 4.772091940875109
  episode_reward_min: 1.8521142899899872
  episodes_this_iter: 270
  episodes_total: 48060
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.63699975120835e-05
          entropy: 11.589494705200195
          entropy_coeff: 0.00032303540501743555
          kl: 0.015175544656813145
          model: {}
          policy_loss: -0.1735287606716156
          total_loss: -0.1206793263554573
          vf_explained_var: 0.9930720329284668
          vf_loss: 0.022021468728780746
    num_agent_steps_sampled: 336420
    num_agent_steps_trained: 336420
    num_steps_sampled: 336420
    num_steps_trained: 336420
  iterations_since_restore: 178
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.395156031672094
    ram_util_percent: 9.040894271075919
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09999696073285438
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3037.981728894956
    mean_inference_ms: 2.3811866481537485
    mean_raw_obs_processing_ms: 221.6293778311604
  time_since_restore: 233378.76331305504
  time_this_iter_s: 1543.414061307907
  time_total_s: 233378.76331305504
  timers:
    learn_throughput: 706.185
    learn_time_ms: 2676.351
    load_throughput: 176959.445
    load_time_ms: 10.68
    sample_throughput: 1.603
    sample_time_ms: 1179061.585
    update_time_ms: 32.105
  timestamp: 1632242682
  timesteps_since_restore: 0
  timesteps_total: 336420
  training_iteration: 178
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    178 |           233379 | 336420 |  4.77209 |              7.66841 |              1.85211 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 338310
  custom_metrics: {}
  date: 2021-09-21_09-52-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.750852938097691
  episode_reward_mean: 4.835474645996048
  episode_reward_min: 1.5734830327078806
  episodes_this_iter: 270
  episodes_total: 48330
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.618000043090433e-05
          entropy: 11.612306594848633
          entropy_coeff: 0.0003220356011297554
          kl: 0.01579461060464382
          model: {}
          policy_loss: -0.18401160836219788
          total_loss: -0.129160538315773
          vf_explained_var: 0.9935285449028015
          vf_loss: 0.022608542814850807
    num_agent_steps_sampled: 338310
    num_agent_steps_trained: 338310
    num_steps_sampled: 338310
    num_steps_trained: 338310
  iterations_since_restore: 179
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.977156549520764
    ram_util_percent: 9.1
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09998189715966879
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3030.325009039529
    mean_inference_ms: 2.380964469264389
    mean_raw_obs_processing_ms: 221.53307294279904
  time_since_restore: 233828.2977824211
  time_this_iter_s: 449.5344693660736
  time_total_s: 233828.2977824211
  timers:
    learn_throughput: 706.502
    learn_time_ms: 2675.153
    load_throughput: 176178.386
    load_time_ms: 10.728
    sample_throughput: 1.851
    sample_time_ms: 1020881.933
    update_time_ms: 31.894
  timestamp: 1632243132
  timesteps_since_restore: 0
  timesteps_total: 338310
  training_iteration: 179
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    179 |           233828 | 338310 |  4.83547 |              7.75085 |              1.57348 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 340200
  custom_metrics: {}
  date: 2021-09-21_10-06-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 6.916083169250236
  episode_reward_mean: 4.841424567189256
  episode_reward_min: 1.7850951630602128
  episodes_this_iter: 270
  episodes_total: 48600
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.599000334972516e-05
          entropy: 11.607711791992188
          entropy_coeff: 0.0003210357972420752
          kl: 0.016391770914196968
          model: {}
          policy_loss: -0.1871095895767212
          total_loss: -0.13419845700263977
          vf_explained_var: 0.9941033124923706
          vf_loss: 0.01929512247443199
    num_agent_steps_sampled: 340200
    num_agent_steps_trained: 340200
    num_steps_sampled: 340200
    num_steps_trained: 340200
  iterations_since_restore: 180
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.81372384937238
    ram_util_percent: 8.95389121338912
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09999551500841103
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3027.148662518633
    mean_inference_ms: 2.3804383912092715
    mean_raw_obs_processing_ms: 221.43598167690348
  time_since_restore: 234687.6349415779
  time_this_iter_s: 859.3371591567993
  time_total_s: 234687.6349415779
  timers:
    learn_throughput: 706.064
    learn_time_ms: 2676.812
    load_throughput: 175831.767
    load_time_ms: 10.749
    sample_throughput: 1.759
    sample_time_ms: 1074613.997
    update_time_ms: 31.201
  timestamp: 1632243991
  timesteps_since_restore: 0
  timesteps_total: 340200
  training_iteration: 180
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    180 |           234688 | 340200 |  4.84142 |              6.91608 |               1.7851 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 342090
  custom_metrics: {}
  date: 2021-09-21_10-15-39
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.976212804059085
  episode_reward_mean: 4.753248934640208
  episode_reward_min: 1.9134985362211108
  episodes_this_iter: 270
  episodes_total: 48870
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.579999899258837e-05
          entropy: 11.683807373046875
          entropy_coeff: 0.00032003599335439503
          kl: 0.01804105006158352
          model: {}
          policy_loss: -0.18535743653774261
          total_loss: -0.1262214034795761
          vf_explained_var: 0.9932093620300293
          vf_loss: 0.02177547477185726
    num_agent_steps_sampled: 342090
    num_agent_steps_trained: 342090
    num_steps_sampled: 342090
    num_steps_trained: 342090
  iterations_since_restore: 181
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.77421465968586
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09999880824499043
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3019.4804387131257
    mean_inference_ms: 2.3801957811150354
    mean_raw_obs_processing_ms: 221.33651218488157
  time_since_restore: 235235.95791840553
  time_this_iter_s: 548.3229768276215
  time_total_s: 235235.95791840553
  timers:
    learn_throughput: 706.27
    learn_time_ms: 2676.029
    load_throughput: 175795.504
    load_time_ms: 10.751
    sample_throughput: 1.816
    sample_time_ms: 1040702.188
    update_time_ms: 30.769
  timestamp: 1632244539
  timesteps_since_restore: 0
  timesteps_total: 342090
  training_iteration: 181
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    181 |           235236 | 342090 |  4.75325 |              7.97621 |               1.9135 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 343980
  custom_metrics: {}
  date: 2021-09-21_10-28-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.84402781856462
  episode_reward_mean: 4.838819979522793
  episode_reward_min: 1.9683079658884288
  episodes_this_iter: 270
  episodes_total: 49140
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.56100019114092e-05
          entropy: 11.721650123596191
          entropy_coeff: 0.00031903618946671486
          kl: 0.014387408271431923
          model: {}
          policy_loss: -0.19083134829998016
          total_loss: -0.14019690454006195
          vf_explained_var: 0.9936349987983704
          vf_loss: 0.021597793325781822
    num_agent_steps_sampled: 343980
    num_agent_steps_trained: 343980
    num_steps_sampled: 343980
    num_steps_trained: 343980
  iterations_since_restore: 182
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.42802303262956
    ram_util_percent: 9.19088291746641
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09998686011583091
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3014.011892647384
    mean_inference_ms: 2.3796880629564865
    mean_raw_obs_processing_ms: 221.23381438272702
  time_since_restore: 235984.55872511864
  time_this_iter_s: 748.6008067131042
  time_total_s: 235984.55872511864
  timers:
    learn_throughput: 705.507
    learn_time_ms: 2678.924
    load_throughput: 177453.02
    load_time_ms: 10.651
    sample_throughput: 1.867
    sample_time_ms: 1012336.865
    update_time_ms: 30.379
  timestamp: 1632245288
  timesteps_since_restore: 0
  timesteps_total: 343980
  training_iteration: 182
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    182 |           235985 | 343980 |  4.83882 |              7.84403 |              1.96831 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 345870
  custom_metrics: {}
  date: 2021-09-21_10-39-59
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.109065321057066
  episode_reward_mean: 4.805163124021754
  episode_reward_min: 1.8105925436108188
  episodes_this_iter: 270
  episodes_total: 49410
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.541999755427241e-05
          entropy: 11.670560836791992
          entropy_coeff: 0.0003180363855790347
          kl: 0.014436188153922558
          model: {}
          policy_loss: -0.18459700047969818
          total_loss: -0.1325031965970993
          vf_explained_var: 0.9933021664619446
          vf_loss: 0.022918054834008217
    num_agent_steps_sampled: 345870
    num_agent_steps_trained: 345870
    num_steps_sampled: 345870
    num_steps_trained: 345870
  iterations_since_restore: 183
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.06572295247725
    ram_util_percent: 9.128614762386247
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09999075241994018
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3008.217707945552
    mean_inference_ms: 2.379314584647026
    mean_raw_obs_processing_ms: 221.13123673913526
  time_since_restore: 236695.65997552872
  time_this_iter_s: 711.10125041008
  time_total_s: 236695.65997552872
  timers:
    learn_throughput: 705.556
    learn_time_ms: 2678.739
    load_throughput: 179472.636
    load_time_ms: 10.531
    sample_throughput: 2.227
    sample_time_ms: 848786.173
    update_time_ms: 30.265
  timestamp: 1632245999
  timesteps_since_restore: 0
  timesteps_total: 345870
  training_iteration: 183
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    183 |           236696 | 345870 |  4.80516 |              8.10907 |              1.81059 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 347760
  custom_metrics: {}
  date: 2021-09-21_10-54-29
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.292159407008855
  episode_reward_mean: 4.888111136776598
  episode_reward_min: 1.8375033042650868
  episodes_this_iter: 270
  episodes_total: 49680
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.523000047309324e-05
          entropy: 11.679116249084473
          entropy_coeff: 0.00031703661079518497
          kl: 0.01555717270821333
          model: {}
          policy_loss: -0.18353143334388733
          total_loss: -0.13116960227489471
          vf_explained_var: 0.9939554333686829
          vf_loss: 0.020623359829187393
    num_agent_steps_sampled: 347760
    num_agent_steps_trained: 347760
    num_steps_sampled: 347760
    num_steps_trained: 347760
  iterations_since_restore: 184
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.78512396694215
    ram_util_percent: 9.3
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09999237789176586
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3004.775233137774
    mean_inference_ms: 2.378786488604855
    mean_raw_obs_processing_ms: 221.02472824042187
  time_since_restore: 237565.06151890755
  time_this_iter_s: 869.40154337883
  time_total_s: 237565.06151890755
  timers:
    learn_throughput: 706.152
    learn_time_ms: 2676.477
    load_throughput: 178176.138
    load_time_ms: 10.607
    sample_throughput: 2.189
    sample_time_ms: 863453.822
    update_time_ms: 29.568
  timestamp: 1632246869
  timesteps_since_restore: 0
  timesteps_total: 347760
  training_iteration: 184
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    184 |           237565 | 347760 |  4.88811 |              8.29216 |               1.8375 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 349650
  custom_metrics: {}
  date: 2021-09-21_11-10-28
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.181131347945172
  episode_reward_mean: 4.850658751387812
  episode_reward_min: 1.6950788989610253
  episodes_this_iter: 270
  episodes_total: 49950
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.504000339191407e-05
          entropy: 11.718159675598145
          entropy_coeff: 0.0003160368069075048
          kl: 0.013992106541991234
          model: {}
          policy_loss: -0.1804429441690445
          total_loss: -0.12983542680740356
          vf_explained_var: 0.9936701059341431
          vf_loss: 0.02243511937558651
    num_agent_steps_sampled: 349650
    num_agent_steps_trained: 349650
    num_steps_sampled: 349650
    num_steps_trained: 349650
  iterations_since_restore: 185
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.881573033707866
    ram_util_percent: 9.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0999835414164781
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3000.297316804592
    mean_inference_ms: 2.378290422781664
    mean_raw_obs_processing_ms: 220.92664771030465
  time_since_restore: 238524.53935837746
  time_this_iter_s: 959.4778394699097
  time_total_s: 238524.53935837746
  timers:
    learn_throughput: 705.949
    learn_time_ms: 2677.249
    load_throughput: 176777.919
    load_time_ms: 10.691
    sample_throughput: 2.204
    sample_time_ms: 857478.961
    update_time_ms: 29.547
  timestamp: 1632247828
  timesteps_since_restore: 0
  timesteps_total: 349650
  training_iteration: 185
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    185 |           238525 | 349650 |  4.85066 |              8.18113 |              1.69508 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 351540
  custom_metrics: {}
  date: 2021-09-21_11-14-49
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.614473797898139
  episode_reward_mean: 4.825778769727701
  episode_reward_min: 2.2626124769435347
  episodes_this_iter: 270
  episodes_total: 50220
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.484999903477728e-05
          entropy: 11.714510917663574
          entropy_coeff: 0.0003150370030198246
          kl: 0.01566954329609871
          model: {}
          policy_loss: -0.1787940263748169
          total_loss: -0.12697553634643555
          vf_explained_var: 0.9940871596336365
          vf_loss: 0.019811823964118958
    num_agent_steps_sampled: 351540
    num_agent_steps_trained: 351540
    num_steps_sampled: 351540
    num_steps_trained: 351540
  iterations_since_restore: 186
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.2305785123967
    ram_util_percent: 9.299999999999997
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09996414794726624
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2990.6725400504492
    mean_inference_ms: 2.3776763946659027
    mean_raw_obs_processing_ms: 220.84737885233062
  time_since_restore: 238784.98416996002
  time_this_iter_s: 260.4448115825653
  time_total_s: 238784.98416996002
  timers:
    learn_throughput: 705.958
    learn_time_ms: 2677.214
    load_throughput: 176671.939
    load_time_ms: 10.698
    sample_throughput: 2.231
    sample_time_ms: 847103.864
    update_time_ms: 29.003
  timestamp: 1632248089
  timesteps_since_restore: 0
  timesteps_total: 351540
  training_iteration: 186
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    186 |           238785 | 351540 |  4.82578 |              7.61447 |              2.26261 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 353430
  custom_metrics: {}
  date: 2021-09-21_11-25-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.594330273716776
  episode_reward_mean: 4.8549425801909445
  episode_reward_min: 2.1927350692019894
  episodes_this_iter: 270
  episodes_total: 50490
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.466000195359811e-05
          entropy: 11.721397399902344
          entropy_coeff: 0.00031403719913214445
          kl: 0.016534127295017242
          model: {}
          policy_loss: -0.18719008564949036
          total_loss: -0.13180966675281525
          vf_explained_var: 0.99362713098526
          vf_loss: 0.021394578740000725
    num_agent_steps_sampled: 353430
    num_agent_steps_trained: 353430
    num_steps_sampled: 353430
    num_steps_trained: 353430
  iterations_since_restore: 187
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.73193657984145
    ram_util_percent: 9.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09994595982024239
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2984.0278274237858
    mean_inference_ms: 2.377432787482258
    mean_raw_obs_processing_ms: 220.74977731717522
  time_since_restore: 239419.835801363
  time_this_iter_s: 634.8516314029694
  time_total_s: 239419.835801363
  timers:
    learn_throughput: 705.556
    learn_time_ms: 2678.738
    load_throughput: 177935.776
    load_time_ms: 10.622
    sample_throughput: 2.501
    sample_time_ms: 755599.901
    update_time_ms: 28.779
  timestamp: 1632248724
  timesteps_since_restore: 0
  timesteps_total: 353430
  training_iteration: 187
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    187 |           239420 | 353430 |  4.85494 |              7.59433 |              2.19274 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 355320
  custom_metrics: {}
  date: 2021-09-21_11-28-21
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.147376952314067
  episode_reward_mean: 4.958757935209658
  episode_reward_min: 1.5671217155889832
  episodes_this_iter: 270
  episodes_total: 50760
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.446999759646133e-05
          entropy: 11.619343757629395
          entropy_coeff: 0.0003130373952444643
          kl: 0.015144094824790955
          model: {}
          policy_loss: -0.19358907639980316
          total_loss: -0.14075741171836853
          vf_explained_var: 0.993918240070343
          vf_loss: 0.021968798711895943
    num_agent_steps_sampled: 355320
    num_agent_steps_trained: 355320
    num_steps_sampled: 355320
    num_steps_trained: 355320
  iterations_since_restore: 188
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.30485829959515
    ram_util_percent: 9.200404858299596
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09993580138862242
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2974.3398312323993
    mean_inference_ms: 2.376810460119412
    mean_raw_obs_processing_ms: 220.6453070071976
  time_since_restore: 239597.08866000175
  time_this_iter_s: 177.25285863876343
  time_total_s: 239597.08866000175
  timers:
    learn_throughput: 707.781
    learn_time_ms: 2670.319
    load_throughput: 176443.862
    load_time_ms: 10.712
    sample_throughput: 3.053
    sample_time_ms: 618992.702
    update_time_ms: 28.688
  timestamp: 1632248901
  timesteps_since_restore: 0
  timesteps_total: 355320
  training_iteration: 188
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    188 |           239597 | 355320 |  4.95876 |              8.14738 |              1.56712 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 357210
  custom_metrics: {}
  date: 2021-09-21_11-37-37
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.558378313494292
  episode_reward_mean: 4.833479922878844
  episode_reward_min: 1.7460774944444724
  episodes_this_iter: 270
  episodes_total: 51030
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.428000051528215e-05
          entropy: 11.76265811920166
          entropy_coeff: 0.0003120375913567841
          kl: 0.014391119591891766
          model: {}
          policy_loss: -0.202941432595253
          total_loss: -0.1535479575395584
          vf_explained_var: 0.9940634369850159
          vf_loss: 0.020279083400964737
    num_agent_steps_sampled: 357210
    num_agent_steps_trained: 357210
    num_steps_sampled: 357210
    num_steps_trained: 357210
  iterations_since_restore: 189
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.6869340232859
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09993149495671581
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2967.0273473753205
    mean_inference_ms: 2.376268043742778
    mean_raw_obs_processing_ms: 220.55029471431737
  time_since_restore: 240153.0785973072
  time_this_iter_s: 555.9899373054504
  time_total_s: 240153.0785973072
  timers:
    learn_throughput: 710.035
    learn_time_ms: 2661.84
    load_throughput: 178044.079
    load_time_ms: 10.615
    sample_throughput: 3.002
    sample_time_ms: 629645.599
    update_time_ms: 28.695
  timestamp: 1632249457
  timesteps_since_restore: 0
  timesteps_total: 357210
  training_iteration: 189
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    189 |           240153 | 357210 |  4.83348 |              7.55838 |              1.74608 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 359100
  custom_metrics: {}
  date: 2021-09-21_11-56-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.744296146184644
  episode_reward_mean: 4.891188694112826
  episode_reward_min: 2.161904012992029
  episodes_this_iter: 270
  episodes_total: 51300
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.409000343410298e-05
          entropy: 11.654672622680664
          entropy_coeff: 0.00031103778746910393
          kl: 0.015726659446954727
          model: {}
          policy_loss: -0.18158626556396484
          total_loss: -0.1280106157064438
          vf_explained_var: 0.9936774373054504
          vf_loss: 0.021373411640524864
    num_agent_steps_sampled: 359100
    num_agent_steps_trained: 359100
    num_steps_sampled: 359100
    num_steps_trained: 359100
  iterations_since_restore: 190
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.10445736434109
    ram_util_percent: 9.273062015503873
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09992826254908366
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2963.14376953
    mean_inference_ms: 2.375868929202836
    mean_raw_obs_processing_ms: 220.45643984178864
  time_since_restore: 241266.02767944336
  time_this_iter_s: 1112.9490821361542
  time_total_s: 241266.02767944336
  timers:
    learn_throughput: 710.918
    learn_time_ms: 2658.535
    load_throughput: 177905.826
    load_time_ms: 10.624
    sample_throughput: 2.885
    sample_time_ms: 655010.218
    update_time_ms: 28.701
  timestamp: 1632250570
  timesteps_since_restore: 0
  timesteps_total: 359100
  training_iteration: 190
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    190 |           241266 | 359100 |  4.89119 |               7.7443 |               2.1619 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 360990
  custom_metrics: {}
  date: 2021-09-21_12-06-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.771423556077608
  episode_reward_mean: 4.849864079216284
  episode_reward_min: 1.8876547612603378
  episodes_this_iter: 270
  episodes_total: 51570
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.38999990769662e-05
          entropy: 11.70818042755127
          entropy_coeff: 0.0003100380126852542
          kl: 0.014770451001822948
          model: {}
          policy_loss: -0.1677740067243576
          total_loss: -0.11559214442968369
          vf_explained_var: 0.9936232566833496
          vf_loss: 0.0221629049628973
    num_agent_steps_sampled: 360990
    num_agent_steps_trained: 360990
    num_steps_sampled: 360990
    num_steps_trained: 360990
  iterations_since_restore: 191
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.01392251815981
    ram_util_percent: 9.299999999999997
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09991876622639383
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2958.5165687922827
    mean_inference_ms: 2.375161245233173
    mean_raw_obs_processing_ms: 220.36354684351062
  time_since_restore: 241859.7171087265
  time_this_iter_s: 593.6894292831421
  time_total_s: 241859.7171087265
  timers:
    learn_throughput: 710.945
    learn_time_ms: 2658.432
    load_throughput: 178249.055
    load_time_ms: 10.603
    sample_throughput: 2.866
    sample_time_ms: 659545.853
    update_time_ms: 29.848
  timestamp: 1632251164
  timesteps_since_restore: 0
  timesteps_total: 360990
  training_iteration: 191
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    191 |           241860 | 360990 |  4.84986 |              7.77142 |              1.88765 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 362880
  custom_metrics: {}
  date: 2021-09-21_12-17-03
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.935264384760245
  episode_reward_mean: 4.824745339871825
  episode_reward_min: 1.8744219637278738
  episodes_this_iter: 270
  episodes_total: 51840
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.371000199578702e-05
          entropy: 11.743361473083496
          entropy_coeff: 0.00030903820879757404
          kl: 0.015637323260307312
          model: {}
          policy_loss: -0.17448873817920685
          total_loss: -0.12361086905002594
          vf_explained_var: 0.9943662285804749
          vf_loss: 0.0188832376152277
    num_agent_steps_sampled: 362880
    num_agent_steps_trained: 362880
    num_steps_sampled: 362880
    num_steps_trained: 362880
  iterations_since_restore: 192
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.03991275899673
    ram_util_percent: 9.243729552889858
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09990857015054662
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2953.965640379693
    mean_inference_ms: 2.3745729153411403
    mean_raw_obs_processing_ms: 220.26193619312966
  time_since_restore: 242519.32021069527
  time_this_iter_s: 659.6031019687653
  time_total_s: 242519.32021069527
  timers:
    learn_throughput: 712.215
    learn_time_ms: 2653.692
    load_throughput: 177583.806
    load_time_ms: 10.643
    sample_throughput: 2.905
    sample_time_ms: 650650.752
    update_time_ms: 29.729
  timestamp: 1632251823
  timesteps_since_restore: 0
  timesteps_total: 362880
  training_iteration: 192
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    192 |           242519 | 362880 |  4.82475 |              7.93526 |              1.87442 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 364770
  custom_metrics: {}
  date: 2021-09-21_12-37-51
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6797941313747495
  episode_reward_mean: 4.951966229827178
  episode_reward_min: 1.9043779756263577
  episodes_this_iter: 270
  episodes_total: 52110
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.351999763865024e-05
          entropy: 11.59394645690918
          entropy_coeff: 0.00030803840490989387
          kl: 0.014103977009654045
          model: {}
          policy_loss: -0.19007647037506104
          total_loss: -0.13949353992938995
          vf_explained_var: 0.9938128590583801
          vf_loss: 0.022023702040314674
    num_agent_steps_sampled: 364770
    num_agent_steps_trained: 364770
    num_steps_sampled: 364770
    num_steps_trained: 364770
  iterations_since_restore: 193
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.50224783861671
    ram_util_percent: 9.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09989614781103598
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2956.294889441318
    mean_inference_ms: 2.374158107666463
    mean_raw_obs_processing_ms: 220.16670575925102
  time_since_restore: 243766.47810959816
  time_this_iter_s: 1247.157898902893
  time_total_s: 243766.47810959816
  timers:
    learn_throughput: 712.222
    learn_time_ms: 2653.666
    load_throughput: 177114.827
    load_time_ms: 10.671
    sample_throughput: 2.684
    sample_time_ms: 704256.49
    update_time_ms: 29.81
  timestamp: 1632253071
  timesteps_since_restore: 0
  timesteps_total: 364770
  training_iteration: 193
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    193 |           243766 | 364770 |  4.95197 |              7.67979 |              1.90438 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 366660
  custom_metrics: {}
  date: 2021-09-21_12-45-11
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.682875400089696
  episode_reward_mean: 4.83859815283805
  episode_reward_min: 2.0997823750645193
  episodes_this_iter: 270
  episodes_total: 52380
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.333000055747107e-05
          entropy: 11.719076156616211
          entropy_coeff: 0.0003070386010222137
          kl: 0.015947837382555008
          model: {}
          policy_loss: -0.17278258502483368
          total_loss: -0.11783459037542343
          vf_explained_var: 0.9932808876037598
          vf_loss: 0.022215047851204872
    num_agent_steps_sampled: 366660
    num_agent_steps_trained: 366660
    num_steps_sampled: 366660
    num_steps_trained: 366660
  iterations_since_restore: 194
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.237745098039216
    ram_util_percent: 9.01764705882353
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09988602775642683
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2950.848116100734
    mean_inference_ms: 2.3737010435286554
    mean_raw_obs_processing_ms: 220.07071092209063
  time_since_restore: 244206.78146505356
  time_this_iter_s: 440.30335545539856
  time_total_s: 244206.78146505356
  timers:
    learn_throughput: 713.658
    learn_time_ms: 2648.327
    load_throughput: 177490.765
    load_time_ms: 10.648
    sample_throughput: 2.858
    sample_time_ms: 661351.833
    update_time_ms: 29.967
  timestamp: 1632253511
  timesteps_since_restore: 0
  timesteps_total: 366660
  training_iteration: 194
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    194 |           244207 | 366660 |   4.8386 |              7.68288 |              2.09978 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 368550
  custom_metrics: {}
  date: 2021-09-21_13-01-23
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.00322660119183
  episode_reward_mean: 4.862595908297921
  episode_reward_min: 1.7120361782495959
  episodes_this_iter: 270
  episodes_total: 52650
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.31400034762919e-05
          entropy: 11.676339149475098
          entropy_coeff: 0.0003060387971345335
          kl: 0.015228837728500366
          model: {}
          policy_loss: -0.19599083065986633
          total_loss: -0.14347222447395325
          vf_explained_var: 0.9938703775405884
          vf_loss: 0.021398860961198807
    num_agent_steps_sampled: 368550
    num_agent_steps_trained: 368550
    num_steps_sampled: 368550
    num_steps_trained: 368550
  iterations_since_restore: 195
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.32048816568047
    ram_util_percent: 9.258727810650885
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987617183410957
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2948.959578413955
    mean_inference_ms: 2.3732782650212467
    mean_raw_obs_processing_ms: 219.99237369669427
  time_since_restore: 245179.26592636108
  time_this_iter_s: 972.4844613075256
  time_total_s: 245179.26592636108
  timers:
    learn_throughput: 714.351
    learn_time_ms: 2645.758
    load_throughput: 177394.249
    load_time_ms: 10.654
    sample_throughput: 2.852
    sample_time_ms: 662654.874
    update_time_ms: 30.084
  timestamp: 1632254483
  timesteps_since_restore: 0
  timesteps_total: 368550
  training_iteration: 195
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    195 |           245179 | 368550 |   4.8626 |              8.00323 |              1.71204 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 370440
  custom_metrics: {}
  date: 2021-09-21_13-16-30
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.574907251919883
  episode_reward_mean: 4.826964590178624
  episode_reward_min: 2.48163808221492
  episodes_this_iter: 270
  episodes_total: 52920
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.294999911915511e-05
          entropy: 11.559727668762207
          entropy_coeff: 0.00030503899324685335
          kl: 0.014294587075710297
          model: {}
          policy_loss: -0.19157804548740387
          total_loss: -0.14244627952575684
          vf_explained_var: 0.9939069151878357
          vf_loss: 0.020093068480491638
    num_agent_steps_sampled: 370440
    num_agent_steps_trained: 370440
    num_steps_sampled: 370440
    num_steps_trained: 370440
  iterations_since_restore: 196
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.38605388272583
    ram_util_percent: 9.239857369255148
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986555303694433
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2944.2463985651057
    mean_inference_ms: 2.373003677535956
    mean_raw_obs_processing_ms: 219.91414918740625
  time_since_restore: 246086.2706103325
  time_this_iter_s: 907.004683971405
  time_total_s: 246086.2706103325
  timers:
    learn_throughput: 716.351
    learn_time_ms: 2638.371
    load_throughput: 177673.361
    load_time_ms: 10.637
    sample_throughput: 2.599
    sample_time_ms: 727318.485
    update_time_ms: 30.009
  timestamp: 1632255390
  timesteps_since_restore: 0
  timesteps_total: 370440
  training_iteration: 196
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    196 |           246086 | 370440 |  4.82696 |              7.57491 |              2.48164 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 372330
  custom_metrics: {}
  date: 2021-09-21_13-25-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.673444957426537
  episode_reward_mean: 4.846992633318227
  episode_reward_min: 1.5064081986480629
  episodes_this_iter: 270
  episodes_total: 53190
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.276000203797594e-05
          entropy: 11.611578941345215
          entropy_coeff: 0.0003040391893591732
          kl: 0.015355899930000305
          model: {}
          policy_loss: -0.18505653738975525
          total_loss: -0.1300099939107895
          vf_explained_var: 0.9930791854858398
          vf_loss: 0.023594249039888382
    num_agent_steps_sampled: 372330
    num_agent_steps_trained: 372330
    num_steps_sampled: 372330
    num_steps_trained: 372330
  iterations_since_restore: 197
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.402422611036336
    ram_util_percent: 9.14979811574697
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986654286250607
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2940.222620938126
    mean_inference_ms: 2.372813886346188
    mean_raw_obs_processing_ms: 219.83156245894796
  time_since_restore: 246620.18457436562
  time_this_iter_s: 533.9139640331268
  time_total_s: 246620.18457436562
  timers:
    learn_throughput: 717.681
    learn_time_ms: 2633.481
    load_throughput: 176824.844
    load_time_ms: 10.689
    sample_throughput: 2.635
    sample_time_ms: 717229.911
    update_time_ms: 29.867
  timestamp: 1632255924
  timesteps_since_restore: 0
  timesteps_total: 372330
  training_iteration: 197
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    197 |           246620 | 372330 |  4.84699 |              7.67344 |              1.50641 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 374220
  custom_metrics: {}
  date: 2021-09-21_14-16-56
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.421640377496302
  episode_reward_mean: 4.827040578525277
  episode_reward_min: 1.4057280300575783
  episodes_this_iter: 270
  episodes_total: 53460
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.256999768083915e-05
          entropy: 11.681879997253418
          entropy_coeff: 0.000303039385471493
          kl: 0.016094742342829704
          model: {}
          policy_loss: -0.19256308674812317
          total_loss: -0.13700804114341736
          vf_explained_var: 0.9932805299758911
          vf_loss: 0.02242930233478546
    num_agent_steps_sampled: 374220
    num_agent_steps_trained: 374220
    num_steps_sampled: 374220
    num_steps_trained: 374220
  iterations_since_restore: 198
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.863124854684955
    ram_util_percent: 9.228225993954892
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985705687219516
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2942.942933121315
    mean_inference_ms: 2.372323696241789
    mean_raw_obs_processing_ms: 219.74262548230803
  time_since_restore: 249711.445032835
  time_this_iter_s: 3091.260458469391
  time_total_s: 249711.445032835
  timers:
    learn_throughput: 716.947
    learn_time_ms: 2636.178
    load_throughput: 178771.628
    load_time_ms: 10.572
    sample_throughput: 1.874
    sample_time_ms: 1008627.606
    update_time_ms: 29.855
  timestamp: 1632259016
  timesteps_since_restore: 0
  timesteps_total: 374220
  training_iteration: 198
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    198 |           249711 | 374220 |  4.82704 |              7.42164 |              1.40573 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 376110
  custom_metrics: {}
  date: 2021-09-21_14-53-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.697354825643707
  episode_reward_mean: 4.906221716329663
  episode_reward_min: 1.8389994487465202
  episodes_this_iter: 270
  episodes_total: 53730
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.238000059965998e-05
          entropy: 11.593689918518066
          entropy_coeff: 0.0003020396106876433
          kl: 0.015043719671666622
          model: {}
          policy_loss: -0.182336688041687
          total_loss: -0.12776610255241394
          vf_explained_var: 0.9930576682090759
          vf_loss: 0.023800864815711975
    num_agent_steps_sampled: 376110
    num_agent_steps_trained: 376110
    num_steps_sampled: 376110
    num_steps_trained: 376110
  iterations_since_restore: 199
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.396655629139076
    ram_util_percent: 9.225761589403971
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998487865288463
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2943.754707561838
    mean_inference_ms: 2.3719950763678206
    mean_raw_obs_processing_ms: 219.6552020478251
  time_since_restore: 251881.34051704407
  time_this_iter_s: 2169.8954842090607
  time_total_s: 251881.34051704407
  timers:
    learn_throughput: 715.305
    learn_time_ms: 2642.228
    load_throughput: 175169.697
    load_time_ms: 10.79
    sample_throughput: 1.615
    sample_time_ms: 1170012.57
    update_time_ms: 29.829
  timestamp: 1632261186
  timesteps_since_restore: 0
  timesteps_total: 376110
  training_iteration: 199
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    199 |           251881 | 376110 |  4.90622 |              7.69735 |                1.839 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 378000
  custom_metrics: {}
  date: 2021-09-21_15-14-50
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.334791849383283
  episode_reward_mean: 4.88104262906936
  episode_reward_min: 1.7572995122684865
  episodes_this_iter: 270
  episodes_total: 54000
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.219000351848081e-05
          entropy: 11.5316801071167
          entropy_coeff: 0.0003010398067999631
          kl: 0.01622306928038597
          model: {}
          policy_loss: -0.19228290021419525
          total_loss: -0.13766859471797943
          vf_explained_var: 0.9938381910324097
          vf_loss: 0.0211276076734066
    num_agent_steps_sampled: 378000
    num_agent_steps_trained: 378000
    num_steps_sampled: 378000
    num_steps_trained: 378000
  iterations_since_restore: 200
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.26545454545455
    ram_util_percent: 9.250247933884296
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998445111851171
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2941.9480469540613
    mean_inference_ms: 2.372112058787091
    mean_raw_obs_processing_ms: 219.5966095415996
  time_since_restore: 253186.0725800991
  time_this_iter_s: 1304.7320630550385
  time_total_s: 253186.0725800991
  timers:
    learn_throughput: 716.379
    learn_time_ms: 2638.268
    load_throughput: 174909.196
    load_time_ms: 10.806
    sample_throughput: 1.589
    sample_time_ms: 1189194.653
    update_time_ms: 29.804
  timestamp: 1632262490
  timesteps_since_restore: 0
  timesteps_total: 378000
  training_iteration: 200
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    200 |           253186 | 378000 |  4.88104 |              7.33479 |               1.7573 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 379890
  custom_metrics: {}
  date: 2021-09-21_15-27-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.625813863780009
  episode_reward_mean: 4.902555728925143
  episode_reward_min: 2.182539902003204
  episodes_this_iter: 270
  episodes_total: 54270
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.199999916134402e-05
          entropy: 11.590310096740723
          entropy_coeff: 0.00030004000291228294
          kl: 0.015323684550821781
          model: {}
          policy_loss: -0.19338034093379974
          total_loss: -0.14046761393547058
          vf_explained_var: 0.9937772750854492
          vf_loss: 0.02148103341460228
    num_agent_steps_sampled: 379890
    num_agent_steps_trained: 379890
    num_steps_sampled: 379890
    num_steps_trained: 379890
  iterations_since_restore: 201
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.701117318435756
    ram_util_percent: 9.163128491620112
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09983763397700629
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2935.7390640111485
    mean_inference_ms: 2.371681243618432
    mean_raw_obs_processing_ms: 219.51153354922076
  time_since_restore: 253957.86859679222
  time_this_iter_s: 771.7960166931152
  time_total_s: 253957.86859679222
  timers:
    learn_throughput: 717.0
    learn_time_ms: 2635.982
    load_throughput: 172765.353
    load_time_ms: 10.94
    sample_throughput: 1.566
    sample_time_ms: 1207008.612
    update_time_ms: 28.801
  timestamp: 1632263262
  timesteps_since_restore: 0
  timesteps_total: 379890
  training_iteration: 201
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    201 |           253958 | 379890 |  4.90256 |              7.62581 |              2.18254 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 381780
  custom_metrics: {}
  date: 2021-09-21_15-37-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.71558052998286
  episode_reward_mean: 4.86109455639881
  episode_reward_min: 1.541005671135196
  episodes_this_iter: 270
  episodes_total: 54540
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.181000208016485e-05
          entropy: 11.573648452758789
          entropy_coeff: 0.00029904019902460277
          kl: 0.014004675671458244
          model: {}
          policy_loss: -0.1858222931623459
          total_loss: -0.1355329304933548
          vf_explained_var: 0.9934234023094177
          vf_loss: 0.021845953539013863
    num_agent_steps_sampled: 381780
    num_agent_steps_trained: 381780
    num_steps_sampled: 381780
    num_steps_trained: 381780
  iterations_since_restore: 202
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.400757575757574
    ram_util_percent: 9.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09983085792237556
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2930.419058447842
    mean_inference_ms: 2.3712813538180173
    mean_raw_obs_processing_ms: 219.42703286449347
  time_since_restore: 254527.02726721764
  time_this_iter_s: 569.158670425415
  time_total_s: 254527.02726721764
  timers:
    learn_throughput: 716.777
    learn_time_ms: 2636.803
    load_throughput: 173756.366
    load_time_ms: 10.877
    sample_throughput: 1.578
    sample_time_ms: 1197963.436
    update_time_ms: 29.209
  timestamp: 1632263832
  timesteps_since_restore: 0
  timesteps_total: 381780
  training_iteration: 202
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    202 |           254527 | 381780 |  4.86109 |              7.71558 |              1.54101 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 383670
  custom_metrics: {}
  date: 2021-09-21_15-44-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.949186469451436
  episode_reward_mean: 4.885705531261857
  episode_reward_min: 1.7557876724591228
  episodes_this_iter: 270
  episodes_total: 54810
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.161999772302806e-05
          entropy: 11.5720796585083
          entropy_coeff: 0.0002980403951369226
          kl: 0.015351983718574047
          model: {}
          policy_loss: -0.1861562281847
          total_loss: -0.1342869997024536
          vf_explained_var: 0.9940858483314514
          vf_loss: 0.020344436168670654
    num_agent_steps_sampled: 383670
    num_agent_steps_trained: 383670
    num_steps_sampled: 383670
    num_steps_trained: 383670
  iterations_since_restore: 203
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.23304794520548
    ram_util_percent: 9.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998183491066605
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2924.7921042941084
    mean_inference_ms: 2.3711662369757396
    mean_raw_obs_processing_ms: 219.34269931752115
  time_since_restore: 254947.17069768906
  time_this_iter_s: 420.1434304714203
  time_total_s: 254947.17069768906
  timers:
    learn_throughput: 716.811
    learn_time_ms: 2636.677
    load_throughput: 173586.671
    load_time_ms: 10.888
    sample_throughput: 1.695
    sample_time_ms: 1115262.437
    update_time_ms: 29.471
  timestamp: 1632264252
  timesteps_since_restore: 0
  timesteps_total: 383670
  training_iteration: 203
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    203 |           254947 | 383670 |  4.88571 |              7.94919 |              1.75579 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 385560
  custom_metrics: {}
  date: 2021-09-21_16-00-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.667460927485264
  episode_reward_mean: 4.943163556099599
  episode_reward_min: 1.525282318267656
  episodes_this_iter: 270
  episodes_total: 55080
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.143000064184889e-05
          entropy: 11.498968124389648
          entropy_coeff: 0.0002970405912492424
          kl: 0.014654206112027168
          model: {}
          policy_loss: -0.16053441166877747
          total_loss: -0.11163592338562012
          vf_explained_var: 0.9945018887519836
          vf_loss: 0.018930058926343918
    num_agent_steps_sampled: 385560
    num_agent_steps_trained: 385560
    num_steps_sampled: 385560
    num_steps_trained: 385560
  iterations_since_restore: 204
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.854490566037732
    ram_util_percent: 9.117584905660376
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09981258454000942
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2922.4756501050115
    mean_inference_ms: 2.3708506617931007
    mean_raw_obs_processing_ms: 219.26238562086272
  time_since_restore: 255899.86081933975
  time_this_iter_s: 952.6901216506958
  time_total_s: 255899.86081933975
  timers:
    learn_throughput: 716.045
    learn_time_ms: 2639.497
    load_throughput: 174680.256
    load_time_ms: 10.82
    sample_throughput: 1.62
    sample_time_ms: 1166497.749
    update_time_ms: 29.997
  timestamp: 1632265204
  timesteps_since_restore: 0
  timesteps_total: 385560
  training_iteration: 204
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    204 |           255900 | 385560 |  4.94316 |              7.66746 |              1.52528 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 387450
  custom_metrics: {}
  date: 2021-09-21_16-09-18
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.879888687168283
  episode_reward_mean: 4.851820807524594
  episode_reward_min: 1.8818547265483534
  episodes_this_iter: 270
  episodes_total: 55350
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.124000356066972e-05
          entropy: 11.414164543151855
          entropy_coeff: 0.00029604078736156225
          kl: 0.014829582534730434
          model: {}
          policy_loss: -0.20741209387779236
          total_loss: -0.15542054176330566
          vf_explained_var: 0.9935883283615112
          vf_loss: 0.021586963906884193
    num_agent_steps_sampled: 387450
    num_agent_steps_trained: 387450
    num_steps_sampled: 387450
    num_steps_trained: 387450
  iterations_since_restore: 205
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.309480519480523
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998027633753013
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2917.957983171737
    mean_inference_ms: 2.3706308368288336
    mean_raw_obs_processing_ms: 219.17501091426536
  time_since_restore: 256453.5703690052
  time_this_iter_s: 553.709549665451
  time_total_s: 256453.5703690052
  timers:
    learn_throughput: 717.776
    learn_time_ms: 2633.132
    load_throughput: 174361.742
    load_time_ms: 10.84
    sample_throughput: 1.681
    sample_time_ms: 1124626.803
    update_time_ms: 29.769
  timestamp: 1632265758
  timesteps_since_restore: 0
  timesteps_total: 387450
  training_iteration: 205
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    205 |           256454 | 387450 |  4.85182 |              7.87989 |              1.88185 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 389340
  custom_metrics: {}
  date: 2021-09-21_16-44-25
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.51621360577273
  episode_reward_mean: 4.934859238528014
  episode_reward_min: 1.885372634440305
  episodes_this_iter: 270
  episodes_total: 55620
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.104999920353293e-05
          entropy: 11.460896492004395
          entropy_coeff: 0.00029504101257771254
          kl: 0.01418279018253088
          model: {}
          policy_loss: -0.1963234692811966
          total_loss: -0.1463783085346222
          vf_explained_var: 0.9939190745353699
          vf_loss: 0.021016422659158707
    num_agent_steps_sampled: 389340
    num_agent_steps_trained: 389340
    num_steps_sampled: 389340
    num_steps_trained: 389340
  iterations_since_restore: 206
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.0798703514159
    ram_util_percent: 9.273524394404639
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09980152871322706
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2922.3408675222863
    mean_inference_ms: 2.3702253445373653
    mean_raw_obs_processing_ms: 219.08394665029098
  time_since_restore: 258560.433252573
  time_this_iter_s: 2106.86288356781
  time_total_s: 258560.433252573
  timers:
    learn_throughput: 715.349
    learn_time_ms: 2642.067
    load_throughput: 158290.793
    load_time_ms: 11.94
    sample_throughput: 1.519
    sample_time_ms: 1244602.034
    update_time_ms: 29.818
  timestamp: 1632267865
  timesteps_since_restore: 0
  timesteps_total: 389340
  training_iteration: 206
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    206 |           258560 | 389340 |  4.93486 |              7.51621 |              1.88537 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 391230
  custom_metrics: {}
  date: 2021-09-21_17-12-59
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.7380800741584075
  episode_reward_mean: 4.887542458236758
  episode_reward_min: 2.033843268880985
  episodes_this_iter: 270
  episodes_total: 55890
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.0859998484374955e-05
          entropy: 11.396368980407715
          entropy_coeff: 0.00029404120869003236
          kl: 0.017795031890273094
          model: {}
          policy_loss: -0.19519905745983124
          total_loss: -0.13487353920936584
          vf_explained_var: 0.993279755115509
          vf_loss: 0.023137222975492477
    num_agent_steps_sampled: 391230
    num_agent_steps_trained: 391230
    num_steps_sampled: 391230
    num_steps_trained: 391230
  iterations_since_restore: 207
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.442335153296934
    ram_util_percent: 6.79302813943721
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09978502425688371
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2921.97385244645
    mean_inference_ms: 2.3699063685648443
    mean_raw_obs_processing_ms: 219.01075112340388
  time_since_restore: 260274.2351064682
  time_this_iter_s: 1713.8018538951874
  time_total_s: 260274.2351064682
  timers:
    learn_throughput: 715.58
    learn_time_ms: 2641.215
    load_throughput: 157869.012
    load_time_ms: 11.972
    sample_throughput: 1.387
    sample_time_ms: 1362591.257
    update_time_ms: 30.025
  timestamp: 1632269579
  timesteps_since_restore: 0
  timesteps_total: 391230
  training_iteration: 207
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    207 |           260274 | 391230 |  4.88754 |              7.73808 |              2.03384 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 393120
  custom_metrics: {}
  date: 2021-09-21_18-02-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.741272645703923
  episode_reward_mean: 4.8186138380439605
  episode_reward_min: 2.228254843904642
  episodes_this_iter: 270
  episodes_total: 56160
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.0670001403195783e-05
          entropy: 11.46349811553955
          entropy_coeff: 0.0002930414048023522
          kl: 0.01558650191873312
          model: {}
          policy_loss: -0.19327938556671143
          total_loss: -0.13888321816921234
          vf_explained_var: 0.9932700991630554
          vf_loss: 0.0222474355250597
    num_agent_steps_sampled: 393120
    num_agent_steps_trained: 393120
    num_steps_sampled: 393120
    num_steps_trained: 393120
  iterations_since_restore: 208
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 18.698882952889754
    ram_util_percent: 6.231811559009229
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09981177815360212
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2923.8712884788943
    mean_inference_ms: 2.369963363571023
    mean_raw_obs_processing_ms: 218.94906049426396
  time_since_restore: 263239.56573343277
  time_this_iter_s: 2965.330626964569
  time_total_s: 263239.56573343277
  timers:
    learn_throughput: 714.55
    learn_time_ms: 2645.02
    load_throughput: 158060.709
    load_time_ms: 11.957
    sample_throughput: 1.4
    sample_time_ms: 1349994.399
    update_time_ms: 30.311
  timestamp: 1632272544
  timesteps_since_restore: 0
  timesteps_total: 393120
  training_iteration: 208
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    208 |           263240 | 393120 |  4.81861 |              7.74127 |              2.22825 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 395010
  custom_metrics: {}
  date: 2021-09-21_18-46-37
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.474828841797936
  episode_reward_mean: 4.877582420447375
  episode_reward_min: 2.0017022778250055
  episodes_this_iter: 270
  episodes_total: 56430
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.0480000684037805e-05
          entropy: 11.349051475524902
          entropy_coeff: 0.000292041600914672
          kl: 0.015312423929572105
          model: {}
          policy_loss: -0.18061330914497375
          total_loss: -0.12797072529792786
          vf_explained_var: 0.9939818978309631
          vf_loss: 0.021073361858725548
    num_agent_steps_sampled: 395010
    num_agent_steps_trained: 395010
    num_steps_sampled: 395010
    num_steps_trained: 395010
  iterations_since_restore: 209
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 17.44751696065129
    ram_util_percent: 6.241194029850746
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986198915808521
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2924.989187229161
    mean_inference_ms: 2.37027613452268
    mean_raw_obs_processing_ms: 218.93565026616457
  time_since_restore: 265892.62043070793
  time_this_iter_s: 2653.0546972751617
  time_total_s: 265892.62043070793
  timers:
    learn_throughput: 714.332
    learn_time_ms: 2645.83
    load_throughput: 160469.036
    load_time_ms: 11.778
    sample_throughput: 1.352
    sample_time_ms: 1398309.387
    update_time_ms: 30.316
  timestamp: 1632275197
  timesteps_since_restore: 0
  timesteps_total: 395010
  training_iteration: 209
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    209 |           265893 | 395010 |  4.87758 |              7.47483 |               2.0017 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 396900
  custom_metrics: {}
  date: 2021-09-21_18-50-32
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.705109685724106
  episode_reward_mean: 4.9196197386515905
  episode_reward_min: 2.1935108628231683
  episodes_this_iter: 270
  episodes_total: 56700
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.0289999964879826e-05
          entropy: 11.392470359802246
          entropy_coeff: 0.00029104179702699184
          kl: 0.015337868593633175
          model: {}
          policy_loss: -0.18993933498859406
          total_loss: -0.13747751712799072
          vf_explained_var: 0.9940329194068909
          vf_loss: 0.020835911855101585
    num_agent_steps_sampled: 396900
    num_agent_steps_trained: 396900
    num_steps_sampled: 396900
    num_steps_trained: 396900
  iterations_since_restore: 210
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 15.347384615384614
    ram_util_percent: 5.3
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987226402592762
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2917.69123682454
    mean_inference_ms: 2.3706404066632865
    mean_raw_obs_processing_ms: 218.89441070773782
  time_since_restore: 266126.83913350105
  time_this_iter_s: 234.21870279312134
  time_total_s: 266126.83913350105
  timers:
    learn_throughput: 714.133
    learn_time_ms: 2646.566
    load_throughput: 160290.9
    load_time_ms: 11.791
    sample_throughput: 1.464
    sample_time_ms: 1291257.387
    update_time_ms: 30.466
  timestamp: 1632275432
  timesteps_since_restore: 0
  timesteps_total: 396900
  training_iteration: 210
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    210 |           266127 | 396900 |  4.91962 |              7.70511 |              2.19351 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 398790
  custom_metrics: {}
  date: 2021-09-21_18-56-19
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.856597531274911
  episode_reward_mean: 4.882054135362171
  episode_reward_min: 2.2081171005943916
  episodes_this_iter: 270
  episodes_total: 56970
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.009999924572185e-05
          entropy: 11.505575180053711
          entropy_coeff: 0.00029004199313931167
          kl: 0.014583704993128777
          model: {}
          policy_loss: -0.1821030229330063
          total_loss: -0.1306329220533371
          vf_explained_var: 0.9937434792518616
          vf_loss: 0.021583663299679756
    num_agent_steps_sampled: 398790
    num_agent_steps_trained: 398790
    num_steps_sampled: 398790
    num_steps_trained: 398790
  iterations_since_restore: 211
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 17.220082815734997
    ram_util_percent: 5.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987657079274115
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2911.408903032061
    mean_inference_ms: 2.371061340253619
    mean_raw_obs_processing_ms: 218.8487143437949
  time_since_restore: 266473.83267378807
  time_this_iter_s: 346.9935402870178
  time_total_s: 266473.83267378807
  timers:
    learn_throughput: 715.233
    learn_time_ms: 2642.496
    load_throughput: 162577.257
    load_time_ms: 11.625
    sample_throughput: 1.513
    sample_time_ms: 1248781.153
    update_time_ms: 30.267
  timestamp: 1632275779
  timesteps_since_restore: 0
  timesteps_total: 398790
  training_iteration: 211
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    211 |           266474 | 398790 |  4.88205 |               7.8566 |              2.20812 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 400680
  custom_metrics: {}
  date: 2021-09-21_19-11-32
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.72500791682961
  episode_reward_mean: 4.927065893847891
  episode_reward_min: 1.9707165518733902
  episodes_this_iter: 270
  episodes_total: 57240
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.990999852656387e-05
          entropy: 11.393133163452148
          entropy_coeff: 0.0002890421892516315
          kl: 0.014155484735965729
          model: {}
          policy_loss: -0.18145377933979034
          total_loss: -0.1311493217945099
          vf_explained_var: 0.9938388466835022
          vf_loss: 0.021349569782614708
    num_agent_steps_sampled: 400680
    num_agent_steps_trained: 400680
    num_steps_sampled: 400680
    num_steps_trained: 400680
  iterations_since_restore: 212
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 14.578818897637793
    ram_util_percent: 5.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09988180533388502
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2909.7822980080214
    mean_inference_ms: 2.37106106817613
    mean_raw_obs_processing_ms: 218.78406155135133
  time_since_restore: 267387.22213459015
  time_this_iter_s: 913.3894608020782
  time_total_s: 267387.22213459015
  timers:
    learn_throughput: 717.009
    learn_time_ms: 2635.949
    load_throughput: 161149.512
    load_time_ms: 11.728
    sample_throughput: 1.473
    sample_time_ms: 1283210.607
    update_time_ms: 29.911
  timestamp: 1632276692
  timesteps_since_restore: 0
  timesteps_total: 400680
  training_iteration: 212
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    212 |           267387 | 400680 |  4.92707 |              7.72501 |              1.97072 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 402570
  custom_metrics: {}
  date: 2021-09-21_19-21-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6741230238732525
  episode_reward_mean: 4.977893236788927
  episode_reward_min: 2.0801614648851303
  episodes_this_iter: 270
  episodes_total: 57510
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.9720001445384696e-05
          entropy: 11.322232246398926
          entropy_coeff: 0.0002880424144677818
          kl: 0.014916053041815758
          model: {}
          policy_loss: -0.184761181473732
          total_loss: -0.13380040228366852
          vf_explained_var: 0.994335949420929
          vf_loss: 0.020241443067789078
    num_agent_steps_sampled: 402570
    num_agent_steps_trained: 402570
    num_steps_sampled: 402570
    num_steps_trained: 402570
  iterations_since_restore: 213
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 11.333294255568584
    ram_util_percent: 5.2114888628370455
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987396935245212
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2905.147886674098
    mean_inference_ms: 2.37092827402529
    mean_raw_obs_processing_ms: 218.7334132347562
  time_since_restore: 268001.1913073063
  time_this_iter_s: 613.9691727161407
  time_total_s: 268001.1913073063
  timers:
    learn_throughput: 719.267
    learn_time_ms: 2627.674
    load_throughput: 161668.779
    load_time_ms: 11.691
    sample_throughput: 1.451
    sample_time_ms: 1302601.194
    update_time_ms: 29.381
  timestamp: 1632277306
  timesteps_since_restore: 0
  timesteps_total: 402570
  training_iteration: 213
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    213 |           268001 | 402570 |  4.97789 |              7.67412 |              2.08016 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 404460
  custom_metrics: {}
  date: 2021-09-21_19-36-05
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.725507483743726
  episode_reward_mean: 4.877185990291104
  episode_reward_min: 1.9316482070772503
  episodes_this_iter: 270
  episodes_total: 57780
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.953000072622672e-05
          entropy: 11.29017162322998
          entropy_coeff: 0.0002870426105801016
          kl: 0.015328201465308666
          model: {}
          policy_loss: -0.18773852288722992
          total_loss: -0.1345393806695938
          vf_explained_var: 0.993752658367157
          vf_loss: 0.02152036502957344
    num_agent_steps_sampled: 404460
    num_agent_steps_trained: 404460
    num_steps_sampled: 404460
    num_steps_trained: 404460
  iterations_since_restore: 214
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 14.14463986599665
    ram_util_percent: 5.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986694553357188
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2901.658684400146
    mean_inference_ms: 2.371006064172844
    mean_raw_obs_processing_ms: 218.66752250756807
  time_since_restore: 268860.0584459305
  time_this_iter_s: 858.8671386241913
  time_total_s: 268860.0584459305
  timers:
    learn_throughput: 719.272
    learn_time_ms: 2627.659
    load_throughput: 160779.527
    load_time_ms: 11.755
    sample_throughput: 1.461
    sample_time_ms: 1293219.061
    update_time_ms: 28.976
  timestamp: 1632278165
  timesteps_since_restore: 0
  timesteps_total: 404460
  training_iteration: 214
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    214 |           268860 | 404460 |  4.87719 |              7.72551 |              1.93165 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 406350
  custom_metrics: {}
  date: 2021-09-21_19-41-38
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.696046776905358
  episode_reward_mean: 4.821279959288399
  episode_reward_min: 1.7553820260919155
  episodes_this_iter: 270
  episodes_total: 58050
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.934000000706874e-05
          entropy: 11.393197059631348
          entropy_coeff: 0.00028604280669242144
          kl: 0.013057852163910866
          model: {}
          policy_loss: -0.1622755527496338
          total_loss: -0.11387217789888382
          vf_explained_var: 0.9932597279548645
          vf_loss: 0.021914886310696602
    num_agent_steps_sampled: 406350
    num_agent_steps_trained: 406350
    num_steps_sampled: 406350
    num_steps_trained: 406350
  iterations_since_restore: 215
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 14.461637931034486
    ram_util_percent: 5.3
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09984929387090842
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2894.1528376485167
    mean_inference_ms: 2.371112008504601
    mean_raw_obs_processing_ms: 218.60447383852787
  time_since_restore: 269193.2601284981
  time_this_iter_s: 333.20168256759644
  time_total_s: 269193.2601284981
  timers:
    learn_throughput: 719.272
    learn_time_ms: 2627.655
    load_throughput: 161163.927
    load_time_ms: 11.727
    sample_throughput: 1.487
    sample_time_ms: 1271168.223
    update_time_ms: 29.064
  timestamp: 1632278498
  timesteps_since_restore: 0
  timesteps_total: 406350
  training_iteration: 215
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    215 |           269193 | 406350 |  4.82128 |              7.69605 |              1.75538 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 408240
  custom_metrics: {}
  date: 2021-09-21_19-55-25
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.489122495247632
  episode_reward_mean: 4.830920676268193
  episode_reward_min: 2.1503819793665735
  episodes_this_iter: 270
  episodes_total: 58320
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.914999928791076e-05
          entropy: 11.365888595581055
          entropy_coeff: 0.00028504300280474126
          kl: 0.014370588585734367
          model: {}
          policy_loss: -0.19105933606624603
          total_loss: -0.13911230862140656
          vf_explained_var: 0.993488609790802
          vf_loss: 0.02244877815246582
    num_agent_steps_sampled: 408240
    num_agent_steps_trained: 408240
    num_steps_sampled: 408240
    num_steps_trained: 408240
  iterations_since_restore: 216
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 14.317232375979112
    ram_util_percent: 5.3
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09983786567475476
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2893.91706227833
    mean_inference_ms: 2.3709295726457356
    mean_raw_obs_processing_ms: 218.54484024783466
  time_since_restore: 270020.2163989544
  time_this_iter_s: 826.9562704563141
  time_total_s: 270020.2163989544
  timers:
    learn_throughput: 722.069
    learn_time_ms: 2617.479
    load_throughput: 177829.201
    load_time_ms: 10.628
    sample_throughput: 1.653
    sample_time_ms: 1143189.465
    update_time_ms: 28.959
  timestamp: 1632279325
  timesteps_since_restore: 0
  timesteps_total: 408240
  training_iteration: 216
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    216 |           270020 | 408240 |  4.83092 |              7.48912 |              2.15038 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 410130
  custom_metrics: {}
  date: 2021-09-21_20-28-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.562951366331159
  episode_reward_mean: 4.849000153826192
  episode_reward_min: 1.8980140151015612
  episodes_this_iter: 270
  episodes_total: 58590
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.895999856875278e-05
          entropy: 11.350310325622559
          entropy_coeff: 0.0002840431989170611
          kl: 0.014144605956971645
          model: {}
          policy_loss: -0.20507153868675232
          total_loss: -0.15408693253993988
          vf_explained_var: 0.993619978427887
          vf_loss: 0.0219853688031435
    num_agent_steps_sampled: 410130
    num_agent_steps_trained: 410130
    num_steps_sampled: 410130
    num_steps_trained: 410130
  iterations_since_restore: 217
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 12.56437454279444
    ram_util_percent: 5.285771762984637
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998277265411024
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2897.101831194987
    mean_inference_ms: 2.371030777066987
    mean_raw_obs_processing_ms: 218.47872813801288
  time_since_restore: 271986.82631993294
  time_this_iter_s: 1966.6099209785461
  time_total_s: 271986.82631993294
  timers:
    learn_throughput: 722.286
    learn_time_ms: 2616.691
    load_throughput: 178935.057
    load_time_ms: 10.562
    sample_throughput: 1.617
    sample_time_ms: 1168470.8
    update_time_ms: 28.967
  timestamp: 1632281292
  timesteps_since_restore: 0
  timesteps_total: 410130
  training_iteration: 217
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    217 |           271987 | 410130 |    4.849 |              7.56295 |              1.89801 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 412020
  custom_metrics: {}
  date: 2021-09-21_20-36-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.607191526571304
  episode_reward_mean: 4.856140398589815
  episode_reward_min: 2.1908069415524984
  episodes_this_iter: 270
  episodes_total: 58860
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.877000148757361e-05
          entropy: 11.288680076599121
          entropy_coeff: 0.0002830433950293809
          kl: 0.014678552746772766
          model: {}
          policy_loss: -0.1970597505569458
          total_loss: -0.1436835527420044
          vf_explained_var: 0.9931658506393433
          vf_loss: 0.02313181199133396
    num_agent_steps_sampled: 412020
    num_agent_steps_trained: 412020
    num_steps_sampled: 412020
    num_steps_trained: 412020
  iterations_since_restore: 218
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 14.073834586466166
    ram_util_percent: 5.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09983910165409975
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2891.218980000686
    mean_inference_ms: 2.37080289984598
    mean_raw_obs_processing_ms: 218.42551047662764
  time_since_restore: 272464.86980748177
  time_this_iter_s: 478.0434875488281
  time_total_s: 272464.86980748177
  timers:
    learn_throughput: 724.312
    learn_time_ms: 2609.373
    load_throughput: 179167.598
    load_time_ms: 10.549
    sample_throughput: 2.055
    sample_time_ms: 919749.575
    update_time_ms: 28.937
  timestamp: 1632281770
  timesteps_since_restore: 0
  timesteps_total: 412020
  training_iteration: 218
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    218 |           272465 | 412020 |  4.85614 |              7.60719 |              2.19081 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 413910
  custom_metrics: {}
  date: 2021-09-21_20-44-54
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.7415898446282565
  episode_reward_mean: 4.93843857338302
  episode_reward_min: 1.7403792069612773
  episodes_this_iter: 270
  episodes_total: 59130
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.858000076841563e-05
          entropy: 11.335183143615723
          entropy_coeff: 0.00028204359114170074
          kl: 0.01705830916762352
          model: {}
          policy_loss: -0.1825716197490692
          total_loss: -0.12333165109157562
          vf_explained_var: 0.9932354092597961
          vf_loss: 0.023576026782393456
    num_agent_steps_sampled: 413910
    num_agent_steps_trained: 413910
    num_steps_sampled: 413910
    num_steps_trained: 413910
  iterations_since_restore: 219
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 15.241346153846155
    ram_util_percent: 5.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09981897648926724
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2887.0971652767553
    mean_inference_ms: 2.3706270698895895
    mean_raw_obs_processing_ms: 218.3576969021935
  time_since_restore: 272988.37505078316
  time_this_iter_s: 523.5052433013916
  time_total_s: 272988.37505078316
  timers:
    learn_throughput: 724.73
    learn_time_ms: 2607.867
    load_throughput: 178140.102
    load_time_ms: 10.61
    sample_throughput: 2.674
    sample_time_ms: 706796.338
    update_time_ms: 28.947
  timestamp: 1632282294
  timesteps_since_restore: 0
  timesteps_total: 413910
  training_iteration: 219
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    219 |           272988 | 413910 |  4.93844 |              7.74159 |              1.74038 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 415800
  custom_metrics: {}
  date: 2021-09-21_20-56-25
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.823010391704917
  episode_reward_mean: 4.892873679354083
  episode_reward_min: 2.0373417108734597
  episodes_this_iter: 270
  episodes_total: 59400
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.839000004925765e-05
          entropy: 11.2963228225708
          entropy_coeff: 0.00028104378725402057
          kl: 0.0174220260232687
          model: {}
          policy_loss: -0.19016419351100922
          total_loss: -0.1307828277349472
          vf_explained_var: 0.9933846592903137
          vf_loss: 0.022866563871502876
    num_agent_steps_sampled: 415800
    num_agent_steps_trained: 415800
    num_steps_sampled: 415800
    num_steps_trained: 415800
  iterations_since_restore: 220
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 15.070551508844956
    ram_util_percent: 5.299999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09980905280555225
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2882.2429313302205
    mean_inference_ms: 2.3705404749746286
    mean_raw_obs_processing_ms: 218.29809043499353
  time_since_restore: 273679.46704125404
  time_this_iter_s: 691.0919904708862
  time_total_s: 273679.46704125404
  timers:
    learn_throughput: 725.381
    learn_time_ms: 2605.527
    load_throughput: 179004.149
    load_time_ms: 10.558
    sample_throughput: 2.512
    sample_time_ms: 752485.766
    update_time_ms: 30.02
  timestamp: 1632282985
  timesteps_since_restore: 0
  timesteps_total: 415800
  training_iteration: 220
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    220 |           273679 | 415800 |  4.89287 |              7.82301 |              2.03734 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 417690
  custom_metrics: {}
  date: 2021-09-21_21-07-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.527248938233671
  episode_reward_mean: 4.916571364033841
  episode_reward_min: 2.1368975266098094
  episodes_this_iter: 270
  episodes_total: 59670
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.819999933009967e-05
          entropy: 11.28679370880127
          entropy_coeff: 0.00028004401247017086
          kl: 0.014851391315460205
          model: {}
          policy_loss: -0.1917543262243271
          total_loss: -0.13991199433803558
          vf_explained_var: 0.9939176440238953
          vf_loss: 0.02116980031132698
    num_agent_steps_sampled: 417690
    num_agent_steps_trained: 417690
    num_steps_sampled: 417690
    num_steps_trained: 417690
  iterations_since_restore: 221
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 16.493370165745855
    ram_util_percent: 5.678563535911602
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09980657474004495
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2878.783186007137
    mean_inference_ms: 2.3707395583375144
    mean_raw_obs_processing_ms: 218.24361810334418
  time_since_restore: 274330.68665647507
  time_this_iter_s: 651.2196152210236
  time_total_s: 274330.68665647507
  timers:
    learn_throughput: 724.456
    learn_time_ms: 2608.854
    load_throughput: 178160.921
    load_time_ms: 10.608
    sample_throughput: 2.414
    sample_time_ms: 782905.268
    update_time_ms: 29.914
  timestamp: 1632283636
  timesteps_since_restore: 0
  timesteps_total: 417690
  training_iteration: 221
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    221 |           274331 | 417690 |  4.91657 |              7.52725 |               2.1369 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 419580
  custom_metrics: {}
  date: 2021-09-21_21-18-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.7999063235539365
  episode_reward_mean: 4.91167237218035
  episode_reward_min: 2.1127037771206747
  episodes_this_iter: 270
  episodes_total: 59940
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.800999861094169e-05
          entropy: 11.295254707336426
          entropy_coeff: 0.0002790442085824907
          kl: 0.015119676478207111
          model: {}
          policy_loss: -0.19463671743869781
          total_loss: -0.140837162733078
          vf_explained_var: 0.9933239817619324
          vf_loss: 0.02250691130757332
    num_agent_steps_sampled: 419580
    num_agent_steps_trained: 419580
    num_steps_sampled: 419580
    num_steps_trained: 419580
  iterations_since_restore: 222
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 9.299371727748692
    ram_util_percent: 5.875183246073299
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09979235754300154
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2875.234049819587
    mean_inference_ms: 2.3703883045099694
    mean_raw_obs_processing_ms: 218.19615580217254
  time_since_restore: 275018.2115166187
  time_this_iter_s: 687.5248601436615
  time_total_s: 275018.2115166187
  timers:
    learn_throughput: 724.317
    learn_time_ms: 2609.355
    load_throughput: 178714.398
    load_time_ms: 10.576
    sample_throughput: 2.486
    sample_time_ms: 760318.208
    update_time_ms: 30.011
  timestamp: 1632284324
  timesteps_since_restore: 0
  timesteps_total: 419580
  training_iteration: 222
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    222 |           275018 | 419580 |  4.91167 |              7.79991 |               2.1127 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 421470
  custom_metrics: {}
  date: 2021-09-21_21-31-05
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.805854495553919
  episode_reward_mean: 4.952501168544107
  episode_reward_min: 2.1817484385813533
  episodes_this_iter: 270
  episodes_total: 60210
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.782000152976252e-05
          entropy: 11.278501510620117
          entropy_coeff: 0.0002780444046948105
          kl: 0.014967813156545162
          model: {}
          policy_loss: -0.1947081834077835
          total_loss: -0.14162126183509827
          vf_explained_var: 0.9935199618339539
          vf_loss: 0.022124290466308594
    num_agent_steps_sampled: 421470
    num_agent_steps_trained: 421470
    num_steps_sampled: 421470
    num_steps_trained: 421470
  iterations_since_restore: 223
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 10.705247813411079
    ram_util_percent: 5.9524781341107875
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09978288344304534
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2874.3926804581524
    mean_inference_ms: 2.3710099842769448
    mean_raw_obs_processing_ms: 218.1378735345853
  time_since_restore: 275759.85304427147
  time_this_iter_s: 741.6415276527405
  time_total_s: 275759.85304427147
  timers:
    learn_throughput: 723.95
    learn_time_ms: 2610.678
    load_throughput: 177956.547
    load_time_ms: 10.621
    sample_throughput: 2.445
    sample_time_ms: 773083.992
    update_time_ms: 30.157
  timestamp: 1632285065
  timesteps_since_restore: 0
  timesteps_total: 421470
  training_iteration: 223
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    223 |           275760 | 421470 |   4.9525 |              7.80585 |              2.18175 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 423360
  custom_metrics: {}
  date: 2021-09-21_21-46-32
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.793014708540234
  episode_reward_mean: 4.862424171725729
  episode_reward_min: 2.1045889472542196
  episodes_this_iter: 270
  episodes_total: 60480
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.763000081060454e-05
          entropy: 11.384384155273438
          entropy_coeff: 0.00027704460080713034
          kl: 0.015140390954911709
          model: {}
          policy_loss: -0.19360336661338806
          total_loss: -0.1384190022945404
          vf_explained_var: 0.993007481098175
          vf_loss: 0.023846624419093132
    num_agent_steps_sampled: 423360
    num_agent_steps_trained: 423360
    num_steps_sampled: 423360
    num_steps_trained: 423360
  iterations_since_restore: 224
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 18.370940170940173
    ram_util_percent: 5.933100233100234
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09980926218220758
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2875.3546667486885
    mean_inference_ms: 2.3710301877233477
    mean_raw_obs_processing_ms: 218.0792055031033
  time_since_restore: 276686.94547200203
  time_this_iter_s: 927.0924277305603
  time_total_s: 276686.94547200203
  timers:
    learn_throughput: 724.763
    learn_time_ms: 2607.749
    load_throughput: 177342.658
    load_time_ms: 10.657
    sample_throughput: 2.423
    sample_time_ms: 779909.779
    update_time_ms: 29.992
  timestamp: 1632285992
  timesteps_since_restore: 0
  timesteps_total: 423360
  training_iteration: 224
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    224 |           276687 | 423360 |  4.86242 |              7.79301 |              2.10459 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 425250
  custom_metrics: {}
  date: 2021-09-21_22-07-50
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.742432903597741
  episode_reward_mean: 4.884322383052357
  episode_reward_min: 1.3995743811902905
  episodes_this_iter: 270
  episodes_total: 60750
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.7440000091446564e-05
          entropy: 11.453349113464355
          entropy_coeff: 0.00027604479691945016
          kl: 0.013340441510081291
          model: {}
          policy_loss: -0.17819549143314362
          total_loss: -0.12675708532333374
          vf_explained_var: 0.9929016828536987
          vf_loss: 0.024208860471844673
    num_agent_steps_sampled: 425250
    num_agent_steps_trained: 425250
    num_steps_sampled: 425250
    num_steps_trained: 425250
  iterations_since_restore: 225
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.1564261555806088
    ram_util_percent: 5.198478015783541
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09979916158075518
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2872.4478609573666
    mean_inference_ms: 2.3714365793770673
    mean_raw_obs_processing_ms: 218.03080001225382
  time_since_restore: 277964.8076262474
  time_this_iter_s: 1277.8621542453766
  time_total_s: 277964.8076262474
  timers:
    learn_throughput: 724.809
    learn_time_ms: 2607.582
    load_throughput: 176829.971
    load_time_ms: 10.688
    sample_throughput: 2.162
    sample_time_ms: 874375.613
    update_time_ms: 29.919
  timestamp: 1632287270
  timesteps_since_restore: 0
  timesteps_total: 425250
  training_iteration: 225
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    225 |           277965 | 425250 |  4.88432 |              7.74243 |              1.39957 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 427140
  custom_metrics: {}
  date: 2021-09-21_22-19-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.162287160495872
  episode_reward_mean: 4.938511372325175
  episode_reward_min: 1.1718147129938528
  episodes_this_iter: 270
  episodes_total: 61020
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.7249999372288585e-05
          entropy: 11.287370681762695
          entropy_coeff: 0.00027504499303177
          kl: 0.014737995341420174
          model: {}
          policy_loss: -0.19814352691173553
          total_loss: -0.1468854695558548
          vf_explained_var: 0.993986189365387
          vf_loss: 0.020787598565220833
    num_agent_steps_sampled: 427140
    num_agent_steps_trained: 427140
    num_steps_sampled: 427140
    num_steps_trained: 427140
  iterations_since_restore: 226
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.1610874200426438
    ram_util_percent: 5.197547974413647
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09978596807453857
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2868.42966437277
    mean_inference_ms: 2.3717606629142916
    mean_raw_obs_processing_ms: 217.9747550155928
  time_since_restore: 278640.3502702713
  time_this_iter_s: 675.5426440238953
  time_total_s: 278640.3502702713
  timers:
    learn_throughput: 724.989
    learn_time_ms: 2606.934
    load_throughput: 176877.712
    load_time_ms: 10.685
    sample_throughput: 2.2
    sample_time_ms: 859234.765
    update_time_ms: 30.529
  timestamp: 1632287946
  timesteps_since_restore: 0
  timesteps_total: 427140
  training_iteration: 226
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    226 |           278640 | 427140 |  4.93851 |              8.16229 |              1.17181 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 429030
  custom_metrics: {}
  date: 2021-09-21_22-27-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.944721091539391
  episode_reward_mean: 4.9348738834380335
  episode_reward_min: 1.8382459530459347
  episodes_this_iter: 270
  episodes_total: 61290
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.7059998653130606e-05
          entropy: 11.272299766540527
          entropy_coeff: 0.0002740451891440898
          kl: 0.01351736020296812
          model: {}
          policy_loss: -0.16545383632183075
          total_loss: -0.11717633903026581
          vf_explained_var: 0.9941423535346985
          vf_loss: 0.020572369918227196
    num_agent_steps_sampled: 429030
    num_agent_steps_trained: 429030
    num_steps_sampled: 429030
    num_steps_trained: 429030
  iterations_since_restore: 227
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.4655325443786984
    ram_util_percent: 5.14940828402367
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09977788008417811
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2861.838618537352
    mean_inference_ms: 2.37190536951989
    mean_raw_obs_processing_ms: 217.9277382717173
  time_since_restore: 279127.76174235344
  time_this_iter_s: 487.41147208213806
  time_total_s: 279127.76174235344
  timers:
    learn_throughput: 726.246
    learn_time_ms: 2602.423
    load_throughput: 175705.106
    load_time_ms: 10.757
    sample_throughput: 2.657
    sample_time_ms: 711319.523
    update_time_ms: 30.259
  timestamp: 1632288433
  timesteps_since_restore: 0
  timesteps_total: 429030
  training_iteration: 227
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    227 |           279128 | 429030 |  4.93487 |              7.94472 |              1.83825 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 430920
  custom_metrics: {}
  date: 2021-09-21_22-41-54
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.126984194780997
  episode_reward_mean: 4.899094666890062
  episode_reward_min: 2.229653669175143
  episodes_this_iter: 270
  episodes_total: 61560
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.6870001571951434e-05
          entropy: 11.281526565551758
          entropy_coeff: 0.0002730454143602401
          kl: 0.014791818335652351
          model: {}
          policy_loss: -0.20048703253269196
          total_loss: -0.14796344935894012
          vf_explained_var: 0.9937590956687927
          vf_loss: 0.02190631814301014
    num_agent_steps_sampled: 430920
    num_agent_steps_trained: 430920
    num_steps_sampled: 430920
    num_steps_trained: 430920
  iterations_since_restore: 228
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.0610474631751228
    ram_util_percent: 5.100981996726678
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09976961744036146
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2858.4933756465593
    mean_inference_ms: 2.372052719830026
    mean_raw_obs_processing_ms: 217.87680713593696
  time_since_restore: 280008.64261984825
  time_this_iter_s: 880.880877494812
  time_total_s: 280008.64261984825
  timers:
    learn_throughput: 726.829
    learn_time_ms: 2600.337
    load_throughput: 175630.753
    load_time_ms: 10.761
    sample_throughput: 2.515
    sample_time_ms: 751605.392
    update_time_ms: 30.014
  timestamp: 1632289314
  timesteps_since_restore: 0
  timesteps_total: 430920
  training_iteration: 228
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    228 |           280009 | 430920 |  4.89909 |              8.12698 |              2.22965 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 432810
  custom_metrics: {}
  date: 2021-09-21_23-12-59
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.885774085662746
  episode_reward_mean: 4.9061821421746945
  episode_reward_min: 1.51715341366691
  episodes_this_iter: 270
  episodes_total: 61830
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.6680000852793455e-05
          entropy: 11.362469673156738
          entropy_coeff: 0.00027204561047255993
          kl: 0.014161000959575176
          model: {}
          policy_loss: -0.17010954022407532
          total_loss: -0.11779985576868057
          vf_explained_var: 0.9932578206062317
          vf_loss: 0.02314027212560177
    num_agent_steps_sampled: 432810
    num_agent_steps_trained: 432810
    num_steps_sampled: 432810
    num_steps_trained: 432810
  iterations_since_restore: 229
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 3.886718146718146
    ram_util_percent: 5.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09976806715959423
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2858.2024860330316
    mean_inference_ms: 2.37227762045166
    mean_raw_obs_processing_ms: 217.81945366084008
  time_since_restore: 281873.7513387203
  time_this_iter_s: 1865.1087188720703
  time_total_s: 281873.7513387203
  timers:
    learn_throughput: 729.254
    learn_time_ms: 2591.689
    load_throughput: 175292.486
    load_time_ms: 10.782
    sample_throughput: 2.134
    sample_time_ms: 885772.007
    update_time_ms: 31.704
  timestamp: 1632291179
  timesteps_since_restore: 0
  timesteps_total: 432810
  training_iteration: 229
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    229 |           281874 | 432810 |  4.90618 |              7.88577 |              1.51715 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 434700
  custom_metrics: {}
  date: 2021-09-21_23-16-50
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.432209570726325
  episode_reward_mean: 4.9396801644839545
  episode_reward_min: 2.102809651725286
  episodes_this_iter: 270
  episodes_total: 62100
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.6490000133635476e-05
          entropy: 11.330846786499023
          entropy_coeff: 0.00027104580658487976
          kl: 0.013582760468125343
          model: {}
          policy_loss: -0.18780504167079926
          total_loss: -0.1394529789686203
          vf_explained_var: 0.9942070841789246
          vf_loss: 0.020480036735534668
    num_agent_steps_sampled: 434700
    num_agent_steps_trained: 434700
    num_steps_sampled: 434700
    num_steps_trained: 434700
  iterations_since_restore: 230
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 4.7875
    ram_util_percent: 5.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09976422421958292
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2851.9044723173206
    mean_inference_ms: 2.372203590456844
    mean_raw_obs_processing_ms: 217.77530108304492
  time_since_restore: 282104.6632769108
  time_this_iter_s: 230.9119381904602
  time_total_s: 282104.6632769108
  timers:
    learn_throughput: 730.266
    learn_time_ms: 2588.099
    load_throughput: 173784.554
    load_time_ms: 10.876
    sample_throughput: 2.251
    sample_time_ms: 839757.645
    update_time_ms: 30.581
  timestamp: 1632291410
  timesteps_since_restore: 0
  timesteps_total: 434700
  training_iteration: 230
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    230 |           282105 | 434700 |  4.93968 |              8.43221 |              2.10281 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 436590
  custom_metrics: {}
  date: 2021-09-21_23-53-00
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.579499398043566
  episode_reward_mean: 4.958954335460226
  episode_reward_min: 1.781592457914596
  episodes_this_iter: 270
  episodes_total: 62370
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.62999994144775e-05
          entropy: 11.287063598632812
          entropy_coeff: 0.0002700460026971996
          kl: 0.014992766082286835
          model: {}
          policy_loss: -0.18247611820697784
          total_loss: -0.12733270227909088
          vf_explained_var: 0.9930952787399292
          vf_loss: 0.024036036804318428
    num_agent_steps_sampled: 436590
    num_agent_steps_trained: 436590
    num_steps_sampled: 436590
    num_steps_trained: 436590
  iterations_since_restore: 231
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 0.6407641196013288
    ram_util_percent: 5.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09975529323016454
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2850.534291810862
    mean_inference_ms: 2.3724593872728916
    mean_raw_obs_processing_ms: 217.72503749227897
  time_since_restore: 284273.69413495064
  time_this_iter_s: 2169.030858039856
  time_total_s: 284273.69413495064
  timers:
    learn_throughput: 731.368
    learn_time_ms: 2584.199
    load_throughput: 171135.101
    load_time_ms: 11.044
    sample_throughput: 1.906
    sample_time_ms: 991541.739
    update_time_ms: 30.914
  timestamp: 1632293580
  timesteps_since_restore: 0
  timesteps_total: 436590
  training_iteration: 231
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    231 |           284274 | 436590 |  4.95895 |               7.5795 |              1.78159 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 438480
  custom_metrics: {}
  date: 2021-09-21_23-58-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.682323148462472
  episode_reward_mean: 4.870682432315212
  episode_reward_min: 2.0479156993952166
  episodes_this_iter: 270
  episodes_total: 62640
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.610999869531952e-05
          entropy: 11.347822189331055
          entropy_coeff: 0.0002690461988095194
          kl: 0.014047008939087391
          model: {}
          policy_loss: -0.18269237875938416
          total_loss: -0.1325683444738388
          vf_explained_var: 0.9936533570289612
          vf_loss: 0.021176259964704514
    num_agent_steps_sampled: 438480
    num_agent_steps_trained: 438480
    num_steps_sampled: 438480
    num_steps_trained: 438480
  iterations_since_restore: 232
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 2.6798672566371686
    ram_util_percent: 5.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09976036768244535
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2843.621668629657
    mean_inference_ms: 2.3726106609997073
    mean_raw_obs_processing_ms: 217.68381622382464
  time_since_restore: 284599.6302719116
  time_this_iter_s: 325.9361369609833
  time_total_s: 284599.6302719116
  timers:
    learn_throughput: 731.649
    learn_time_ms: 2583.205
    load_throughput: 172103.336
    load_time_ms: 10.982
    sample_throughput: 1.978
    sample_time_ms: 955383.723
    update_time_ms: 31.011
  timestamp: 1632293906
  timesteps_since_restore: 0
  timesteps_total: 438480
  training_iteration: 232
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    232 |           284600 | 438480 |  4.87068 |              7.68232 |              2.04792 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 440370
  custom_metrics: {}
  date: 2021-09-22_00-11-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.055668642252654
  episode_reward_mean: 4.921772570863581
  episode_reward_min: 2.070133840026632
  episodes_this_iter: 270
  episodes_total: 62910
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.592000161414035e-05
          entropy: 11.18061637878418
          entropy_coeff: 0.00026804639492183924
          kl: 0.013818776234984398
          model: {}
          policy_loss: -0.20208828151226044
          total_loss: -0.15408985316753387
          vf_explained_var: 0.9944861531257629
          vf_loss: 0.019514456391334534
    num_agent_steps_sampled: 440370
    num_agent_steps_trained: 440370
    num_steps_sampled: 440370
    num_steps_trained: 440370
  iterations_since_restore: 233
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.5801683816651078
    ram_util_percent: 5.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09974951538462812
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2840.3783761180775
    mean_inference_ms: 2.3727630830387785
    mean_raw_obs_processing_ms: 217.64695032805415
  time_since_restore: 285370.02840042114
  time_this_iter_s: 770.3981285095215
  time_total_s: 285370.02840042114
  timers:
    learn_throughput: 732.93
    learn_time_ms: 2578.691
    load_throughput: 171526.874
    load_time_ms: 11.019
    sample_throughput: 1.972
    sample_time_ms: 958263.836
    update_time_ms: 31.213
  timestamp: 1632294676
  timesteps_since_restore: 0
  timesteps_total: 440370
  training_iteration: 233
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 14.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    233 |           285370 | 440370 |  4.92177 |              8.05567 |              2.07013 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 442260
  custom_metrics: {}
  date: 2021-09-22_00-37-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.71369906675917
  episode_reward_mean: 4.8367545330889365
  episode_reward_min: 1.8406329188766273
  episodes_this_iter: 270
  episodes_total: 63180
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.573000089498237e-05
          entropy: 11.335990905761719
          entropy_coeff: 0.00026704659103415906
          kl: 0.013296705670654774
          model: {}
          policy_loss: -0.1770215481519699
          total_loss: -0.12651386857032776
          vf_explained_var: 0.9929781556129456
          vf_loss: 0.023243369534611702
    num_agent_steps_sampled: 442260
    num_agent_steps_trained: 442260
    num_steps_sampled: 442260
    num_steps_trained: 442260
  iterations_since_restore: 234
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 0.7218521897810217
    ram_util_percent: 5.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09975434427463234
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2838.320109409806
    mean_inference_ms: 2.3731549149750384
    mean_raw_obs_processing_ms: 217.6037520307301
  time_since_restore: 286949.5140988827
  time_this_iter_s: 1579.4856984615326
  time_total_s: 286949.5140988827
  timers:
    learn_throughput: 733.179
    learn_time_ms: 2577.817
    load_throughput: 170429.783
    load_time_ms: 11.09
    sample_throughput: 1.847
    sample_time_ms: 1023503.538
    update_time_ms: 31.618
  timestamp: 1632296256
  timesteps_since_restore: 0
  timesteps_total: 442260
  training_iteration: 234
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    234 |           286950 | 442260 |  4.83675 |               7.7137 |              1.84063 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 444150
  custom_metrics: {}
  date: 2021-09-22_00-43-39
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.772087592373755
  episode_reward_mean: 4.89815543088802
  episode_reward_min: 2.211316899343036
  episodes_this_iter: 270
  episodes_total: 63450
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.554000017582439e-05
          entropy: 11.203908920288086
          entropy_coeff: 0.0002660467871464789
          kl: 0.01601688750088215
          model: {}
          policy_loss: -0.19236411154270172
          total_loss: -0.13851305842399597
          vf_explained_var: 0.9941133856773376
          vf_loss: 0.020343361422419548
    num_agent_steps_sampled: 444150
    num_agent_steps_trained: 444150
    num_steps_sampled: 444150
    num_steps_trained: 444150
  iterations_since_restore: 235
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 3.3079365079365077
    ram_util_percent: 5.298611111111111
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09978303108965332
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2833.214244685617
    mean_inference_ms: 2.3733336852229177
    mean_raw_obs_processing_ms: 217.557311308921
  time_since_restore: 287312.8988683224
  time_this_iter_s: 363.38476943969727
  time_total_s: 287312.8988683224
  timers:
    learn_throughput: 732.878
    learn_time_ms: 2578.876
    load_throughput: 170212.412
    load_time_ms: 11.104
    sample_throughput: 2.028
    sample_time_ms: 932054.563
    update_time_ms: 32.255
  timestamp: 1632296619
  timesteps_since_restore: 0
  timesteps_total: 444150
  training_iteration: 235
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    235 |           287313 | 444150 |  4.89816 |              7.77209 |              2.21132 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 446040
  custom_metrics: {}
  date: 2021-09-22_00-57-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.623757083719971
  episode_reward_mean: 4.856203743985972
  episode_reward_min: 2.131553154559762
  episodes_this_iter: 270
  episodes_total: 63720
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.534999945666641e-05
          entropy: 11.264463424682617
          entropy_coeff: 0.0002650470123626292
          kl: 0.01582964137196541
          model: {}
          policy_loss: -0.19101811945438385
          total_loss: -0.1364012062549591
          vf_explained_var: 0.9936540722846985
          vf_loss: 0.021540606394410133
    num_agent_steps_sampled: 446040
    num_agent_steps_trained: 446040
    num_steps_sampled: 446040
    num_steps_trained: 446040
  iterations_since_restore: 236
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 13.684372331340736
    ram_util_percent: 5.988642186165671
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09979055623430534
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2830.7176492706753
    mean_inference_ms: 2.3734273502804917
    mean_raw_obs_processing_ms: 217.50900764655566
  time_since_restore: 288157.27901005745
  time_this_iter_s: 844.3801417350769
  time_total_s: 288157.27901005745
  timers:
    learn_throughput: 731.086
    learn_time_ms: 2585.194
    load_throughput: 170647.342
    load_time_ms: 11.075
    sample_throughput: 1.992
    sample_time_ms: 948931.953
    update_time_ms: 31.757
  timestamp: 1632297463
  timesteps_since_restore: 0
  timesteps_total: 446040
  training_iteration: 236
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    236 |           288157 | 446040 |   4.8562 |              7.62376 |              2.13155 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 447930
  custom_metrics: {}
  date: 2021-09-22_01-28-57
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.67705938796975
  episode_reward_mean: 4.953677268836098
  episode_reward_min: 1.867356847938358
  episodes_this_iter: 270
  episodes_total: 63990
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.515999873750843e-05
          entropy: 11.258374214172363
          entropy_coeff: 0.000264047208474949
          kl: 0.015324731357395649
          model: {}
          policy_loss: -0.19965271651744843
          total_loss: -0.1450650691986084
          vf_explained_var: 0.9934853911399841
          vf_loss: 0.022648733109235764
    num_agent_steps_sampled: 447930
    num_agent_steps_trained: 447930
    num_steps_sampled: 447930
    num_steps_trained: 447930
  iterations_since_restore: 237
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 12.046307692307693
    ram_util_percent: 5.961384615384616
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09978284379586781
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2831.522931483045
    mean_inference_ms: 2.3731086291181134
    mean_raw_obs_processing_ms: 217.451371587149
  time_since_restore: 290030.4094595909
  time_this_iter_s: 1873.1304495334625
  time_total_s: 290030.4094595909
  timers:
    learn_throughput: 729.863
    learn_time_ms: 2589.527
    load_throughput: 171463.432
    load_time_ms: 11.023
    sample_throughput: 1.738
    sample_time_ms: 1087499.038
    update_time_ms: 32.193
  timestamp: 1632299337
  timesteps_since_restore: 0
  timesteps_total: 447930
  training_iteration: 237
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    237 |           290030 | 447930 |  4.95368 |              7.67706 |              1.86736 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 449820
  custom_metrics: {}
  date: 2021-09-22_01-42-20
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.740696704486873
  episode_reward_mean: 4.835608958074523
  episode_reward_min: 2.2381416268036087
  episodes_this_iter: 270
  episodes_total: 64260
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.497000165632926e-05
          entropy: 11.250950813293457
          entropy_coeff: 0.00026304740458726883
          kl: 0.015627173706889153
          model: {}
          policy_loss: -0.17036475241184235
          total_loss: -0.11585582792758942
          vf_explained_var: 0.9934045672416687
          vf_loss: 0.021867785602808
    num_agent_steps_sampled: 449820
    num_agent_steps_trained: 449820
    num_steps_sampled: 449820
    num_steps_trained: 449820
  iterations_since_restore: 238
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 12.315515695067266
    ram_util_percent: 5.993004484304933
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09977385981107739
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2829.1834734313597
    mean_inference_ms: 2.3734538361291575
    mean_raw_obs_processing_ms: 217.41105127063827
  time_since_restore: 290834.0628809929
  time_this_iter_s: 803.6534214019775
  time_total_s: 290834.0628809929
  timers:
    learn_throughput: 729.653
    learn_time_ms: 2590.271
    load_throughput: 170349.942
    load_time_ms: 11.095
    sample_throughput: 1.75
    sample_time_ms: 1079775.988
    update_time_ms: 32.449
  timestamp: 1632300140
  timesteps_since_restore: 0
  timesteps_total: 449820
  training_iteration: 238
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    238 |           290834 | 449820 |  4.83561 |               7.7407 |              2.23814 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 451710
  custom_metrics: {}
  date: 2021-09-22_02-09-22
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.765166121810395
  episode_reward_mean: 4.947509371028162
  episode_reward_min: 2.240309101023864
  episodes_this_iter: 270
  episodes_total: 64530
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.478000093717128e-05
          entropy: 11.143218994140625
          entropy_coeff: 0.00026204760069958866
          kl: 0.013726537115871906
          model: {}
          policy_loss: -0.17470106482505798
          total_loss: -0.12410029023885727
          vf_explained_var: 0.9935457110404968
          vf_loss: 0.02225007303059101
    num_agent_steps_sampled: 451710
    num_agent_steps_trained: 451710
    num_steps_sampled: 451710
    num_steps_trained: 451710
  iterations_since_restore: 239
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.031895017793595
    ram_util_percent: 6.154492882562279
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09979381744270287
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2830.9348662454217
    mean_inference_ms: 2.3735124733033586
    mean_raw_obs_processing_ms: 217.3624196998269
  time_since_restore: 292456.14258885384
  time_this_iter_s: 1622.0797078609467
  time_total_s: 292456.14258885384
  timers:
    learn_throughput: 727.262
    learn_time_ms: 2598.787
    load_throughput: 171268.206
    load_time_ms: 11.035
    sample_throughput: 1.791
    sample_time_ms: 1055467.152
    update_time_ms: 31.042
  timestamp: 1632301762
  timesteps_since_restore: 0
  timesteps_total: 451710
  training_iteration: 239
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    239 |           292456 | 451710 |  4.94751 |              7.76517 |              2.24031 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 453600
  custom_metrics: {}
  date: 2021-09-22_02-27-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6374540108441655
  episode_reward_mean: 4.903397270595561
  episode_reward_min: 2.8839352654786374
  episodes_this_iter: 270
  episodes_total: 64800
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.45900002180133e-05
          entropy: 11.178580284118652
          entropy_coeff: 0.0002610477968119085
          kl: 0.017392437905073166
          model: {}
          policy_loss: -0.19126570224761963
          total_loss: -0.12934155762195587
          vf_explained_var: 0.9926077127456665
          vf_loss: 0.025220144540071487
    num_agent_steps_sampled: 453600
    num_agent_steps_trained: 453600
    num_steps_sampled: 453600
    num_steps_trained: 453600
  iterations_since_restore: 240
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 15.715364238410597
    ram_util_percent: 6.097218543046357
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09978986337380455
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2828.321628449071
    mean_inference_ms: 2.373334655186658
    mean_raw_obs_processing_ms: 217.32237774387812
  time_since_restore: 293544.98186159134
  time_this_iter_s: 1088.839272737503
  time_total_s: 293544.98186159134
  timers:
    learn_throughput: 725.328
    learn_time_ms: 2605.718
    load_throughput: 173225.179
    load_time_ms: 10.911
    sample_throughput: 1.656
    sample_time_ms: 1141253.195
    update_time_ms: 31.136
  timestamp: 1632302851
  timesteps_since_restore: 0
  timesteps_total: 453600
  training_iteration: 240
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    240 |           293545 | 453600 |   4.9034 |              7.63745 |              2.88394 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 455490
  custom_metrics: {}
  date: 2021-09-22_02-35-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.945946710153124
  episode_reward_mean: 4.964094913010182
  episode_reward_min: 1.890349506741306
  episodes_this_iter: 270
  episodes_total: 65070
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.439999949885532e-05
          entropy: 11.097471237182617
          entropy_coeff: 0.0002600479929242283
          kl: 0.014403476379811764
          model: {}
          policy_loss: -0.2084212750196457
          total_loss: -0.15547195076942444
          vf_explained_var: 0.9936865568161011
          vf_loss: 0.02302226796746254
    num_agent_steps_sampled: 455490
    num_agent_steps_trained: 455490
    num_steps_sampled: 455490
    num_steps_trained: 455490
  iterations_since_restore: 241
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.740583941605838
    ram_util_percent: 6.385839416058393
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09982189602322047
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2825.369836124373
    mean_inference_ms: 2.373127515293116
    mean_raw_obs_processing_ms: 217.26491067622095
  time_since_restore: 294038.72269034386
  time_this_iter_s: 493.7408287525177
  time_total_s: 294038.72269034386
  timers:
    learn_throughput: 724.148
    learn_time_ms: 2609.964
    load_throughput: 174670.249
    load_time_ms: 10.82
    sample_throughput: 1.941
    sample_time_ms: 973720.76
    update_time_ms: 31.262
  timestamp: 1632303345
  timesteps_since_restore: 0
  timesteps_total: 455490
  training_iteration: 241
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    241 |           294039 | 455490 |  4.96409 |              7.94595 |              1.89035 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 457380
  custom_metrics: {}
  date: 2021-09-22_02-45-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.038984400492982
  episode_reward_mean: 4.986811589270943
  episode_reward_min: 1.8748462389554124
  episodes_this_iter: 270
  episodes_total: 65340
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.4209998779697344e-05
          entropy: 11.147521018981934
          entropy_coeff: 0.00025904818903654814
          kl: 0.015079320408403873
          model: {}
          policy_loss: -0.18784181773662567
          total_loss: -0.13714976608753204
          vf_explained_var: 0.9946478605270386
          vf_loss: 0.01922720856964588
    num_agent_steps_sampled: 457380
    num_agent_steps_trained: 457380
    num_steps_sampled: 457380
    num_steps_trained: 457380
  iterations_since_restore: 242
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.123586040914567
    ram_util_percent: 6.531648616125151
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09982426888306721
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2821.481945762131
    mean_inference_ms: 2.3730610153898457
    mean_raw_obs_processing_ms: 217.2141031688801
  time_since_restore: 294637.50567126274
  time_this_iter_s: 598.7829809188843
  time_total_s: 294637.50567126274
  timers:
    learn_throughput: 723.491
    learn_time_ms: 2612.334
    load_throughput: 174025.666
    load_time_ms: 10.86
    sample_throughput: 1.888
    sample_time_ms: 1001003.336
    update_time_ms: 31.207
  timestamp: 1632303944
  timesteps_since_restore: 0
  timesteps_total: 457380
  training_iteration: 242
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    242 |           294638 | 457380 |  4.98681 |              8.03898 |              1.87485 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 459270
  custom_metrics: {}
  date: 2021-09-22_02-53-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.5199059727870186
  episode_reward_mean: 4.917133814886812
  episode_reward_min: 1.8577454504517845
  episodes_this_iter: 270
  episodes_total: 65610
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.402000169851817e-05
          entropy: 11.167508125305176
          entropy_coeff: 0.0002580484142526984
          kl: 0.014359035529196262
          model: {}
          policy_loss: -0.19154831767082214
          total_loss: -0.1410437375307083
          vf_explained_var: 0.9939664006233215
          vf_loss: 0.02067464031279087
    num_agent_steps_sampled: 459270
    num_agent_steps_trained: 459270
    num_steps_sampled: 459270
    num_steps_trained: 459270
  iterations_since_restore: 243
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.47172932330827
    ram_util_percent: 6.558045112781955
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09982629578521546
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2815.7697992421045
    mean_inference_ms: 2.373069976854967
    mean_raw_obs_processing_ms: 217.16146194579738
  time_since_restore: 295116.7380695343
  time_this_iter_s: 479.23239827156067
  time_total_s: 295116.7380695343
  timers:
    learn_throughput: 721.684
    learn_time_ms: 2618.874
    load_throughput: 174311.9
    load_time_ms: 10.843
    sample_throughput: 1.945
    sample_time_ms: 971880.43
    update_time_ms: 31.027
  timestamp: 1632304423
  timesteps_since_restore: 0
  timesteps_total: 459270
  training_iteration: 243
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 20.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    243 |           295117 | 459270 |  4.91713 |              7.51991 |              1.85775 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 461160
  custom_metrics: {}
  date: 2021-09-22_03-01-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.718742750823473
  episode_reward_mean: 4.874614560130087
  episode_reward_min: 2.2493845877441068
  episodes_this_iter: 270
  episodes_total: 65880
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.383000097936019e-05
          entropy: 11.110624313354492
          entropy_coeff: 0.00025704861036501825
          kl: 0.015336750075221062
          model: {}
          policy_loss: -0.16878999769687653
          total_loss: -0.1142667755484581
          vf_explained_var: 0.9935030341148376
          vf_loss: 0.022440161556005478
    num_agent_steps_sampled: 461160
    num_agent_steps_trained: 461160
    num_steps_sampled: 461160
    num_steps_trained: 461160
  iterations_since_restore: 244
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.607619047619053
    ram_util_percent: 6.610476190476191
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987010860090627
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2809.432841872503
    mean_inference_ms: 2.373309613984931
    mean_raw_obs_processing_ms: 217.10468734667035
  time_since_restore: 295570.7900002003
  time_this_iter_s: 454.05193066596985
  time_total_s: 295570.7900002003
  timers:
    learn_throughput: 720.411
    learn_time_ms: 2623.502
    load_throughput: 174855.568
    load_time_ms: 10.809
    sample_throughput: 2.199
    sample_time_ms: 859332.835
    update_time_ms: 30.691
  timestamp: 1632304877
  timesteps_since_restore: 0
  timesteps_total: 461160
  training_iteration: 244
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    244 |           295571 | 461160 |  4.87461 |              7.71874 |              2.24938 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 463050
  custom_metrics: {}
  date: 2021-09-22_03-13-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.700898978101473
  episode_reward_mean: 4.9878943809479885
  episode_reward_min: 2.308524462441566
  episodes_this_iter: 270
  episodes_total: 66150
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.3640000260202214e-05
          entropy: 11.08388614654541
          entropy_coeff: 0.0002560488064773381
          kl: 0.01422607246786356
          model: {}
          policy_loss: -0.19360771775245667
          total_loss: -0.14176754653453827
          vf_explained_var: 0.9936148524284363
          vf_loss: 0.02226943150162697
    num_agent_steps_sampled: 463050
    num_agent_steps_trained: 463050
    num_steps_sampled: 463050
    num_steps_trained: 463050
  iterations_since_restore: 245
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.177531340405018
    ram_util_percent: 6.732304725168757
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987133276120641
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2804.619129670464
    mean_inference_ms: 2.3734638941290958
    mean_raw_obs_processing_ms: 217.05144877950417
  time_since_restore: 296317.3003125191
  time_this_iter_s: 746.5103123188019
  time_total_s: 296317.3003125191
  timers:
    learn_throughput: 720.023
    learn_time_ms: 2624.916
    load_throughput: 175356.854
    load_time_ms: 10.778
    sample_throughput: 2.106
    sample_time_ms: 897644.526
    update_time_ms: 30.061
  timestamp: 1632305624
  timesteps_since_restore: 0
  timesteps_total: 463050
  training_iteration: 245
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    245 |           296317 | 463050 |  4.98789 |               7.7009 |              2.30852 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 464940
  custom_metrics: {}
  date: 2021-09-22_03-30-01
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6455414140513716
  episode_reward_mean: 4.910477306928044
  episode_reward_min: 2.3363606499155556
  episodes_this_iter: 270
  episodes_total: 66420
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.3449999541044235e-05
          entropy: 11.085443496704102
          entropy_coeff: 0.0002550490025896579
          kl: 0.01459946297109127
          model: {}
          policy_loss: -0.1940551996231079
          total_loss: -0.14017410576343536
          vf_explained_var: 0.9932788610458374
          vf_loss: 0.023449022322893143
    num_agent_steps_sampled: 464940
    num_agent_steps_trained: 464940
    num_steps_sampled: 464940
    num_steps_trained: 464940
  iterations_since_restore: 246
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.790044247787613
    ram_util_percent: 6.645353982300884
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987001347809665
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2803.093518781731
    mean_inference_ms: 2.373233984434713
    mean_raw_obs_processing_ms: 217.00138127259757
  time_since_restore: 297294.0947134495
  time_this_iter_s: 976.7944009304047
  time_total_s: 297294.0947134495
  timers:
    learn_throughput: 720.289
    learn_time_ms: 2623.947
    load_throughput: 173839.813
    load_time_ms: 10.872
    sample_throughput: 2.075
    sample_time_ms: 910886.796
    update_time_ms: 30.274
  timestamp: 1632306601
  timesteps_since_restore: 0
  timesteps_total: 464940
  training_iteration: 246
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    246 |           297294 | 464940 |  4.91048 |              7.64554 |              2.33636 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 466830
  custom_metrics: {}
  date: 2021-09-22_03-46-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.858405651443399
  episode_reward_mean: 5.0097581103812185
  episode_reward_min: 1.991600767624937
  episodes_this_iter: 270
  episodes_total: 66690
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.3259998821886256e-05
          entropy: 11.071343421936035
          entropy_coeff: 0.00025404919870197773
          kl: 0.01397548709064722
          model: {}
          policy_loss: -0.17785175144672394
          total_loss: -0.1284208595752716
          vf_explained_var: 0.9941670298576355
          vf_loss: 0.02040562964975834
    num_agent_steps_sampled: 466830
    num_agent_steps_trained: 466830
    num_steps_sampled: 466830
    num_steps_trained: 466830
  iterations_since_restore: 247
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.95352727272728
    ram_util_percent: 6.743709090909091
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09988078956881344
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2800.01807923748
    mean_inference_ms: 2.3730282227343795
    mean_raw_obs_processing_ms: 216.9489048528363
  time_since_restore: 298284.88313674927
  time_this_iter_s: 990.7884232997894
  time_total_s: 298284.88313674927
  timers:
    learn_throughput: 720.384
    learn_time_ms: 2623.602
    load_throughput: 172464.284
    load_time_ms: 10.959
    sample_throughput: 2.297
    sample_time_ms: 822653.205
    update_time_ms: 29.919
  timestamp: 1632307591
  timesteps_since_restore: 0
  timesteps_total: 466830
  training_iteration: 247
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    247 |           298285 | 466830 |  5.00976 |              7.85841 |               1.9916 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 468720
  custom_metrics: {}
  date: 2021-09-22_04-10-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.797148076527045
  episode_reward_mean: 4.872633196204209
  episode_reward_min: 2.205080442331966
  episodes_this_iter: 270
  episodes_total: 66960
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.3070001740707085e-05
          entropy: 11.126832008361816
          entropy_coeff: 0.00025304939481429756
          kl: 0.013414240442216396
          model: {}
          policy_loss: -0.1998765915632248
          total_loss: -0.1514604240655899
          vf_explained_var: 0.993980348110199
          vf_loss: 0.02067248895764351
    num_agent_steps_sampled: 468720
    num_agent_steps_trained: 468720
    num_steps_sampled: 468720
    num_steps_trained: 468720
  iterations_since_restore: 248
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.77372448979592
    ram_util_percent: 6.9755612244897955
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09988451470334185
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2799.665215199213
    mean_inference_ms: 2.3730909363786235
    mean_raw_obs_processing_ms: 216.91221464955487
  time_since_restore: 299697.1313633919
  time_this_iter_s: 1412.2482266426086
  time_total_s: 299697.1313633919
  timers:
    learn_throughput: 718.911
    learn_time_ms: 2628.977
    load_throughput: 173250.923
    load_time_ms: 10.909
    sample_throughput: 2.139
    sample_time_ms: 883507.363
    update_time_ms: 29.689
  timestamp: 1632309004
  timesteps_since_restore: 0
  timesteps_total: 468720
  training_iteration: 248
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    248 |           299697 | 468720 |  4.87263 |              7.79715 |              2.20508 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 470610
  custom_metrics: {}
  date: 2021-09-22_04-18-54
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.008740925611175
  episode_reward_mean: 4.882036707632194
  episode_reward_min: 2.199518181142717
  episodes_this_iter: 270
  episodes_total: 67230
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.2880001021549106e-05
          entropy: 11.074941635131836
          entropy_coeff: 0.0002520495909266174
          kl: 0.01412151474505663
          model: {}
          policy_loss: -0.1864365190267563
          total_loss: -0.1371387392282486
          vf_explained_var: 0.9942358136177063
          vf_loss: 0.01991863362491131
    num_agent_steps_sampled: 470610
    num_agent_steps_trained: 470610
    num_steps_sampled: 470610
    num_steps_trained: 470610
  iterations_since_restore: 249
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.418614130434786
    ram_util_percent: 7.200000000000001
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987951179681621
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2794.6942841033374
    mean_inference_ms: 2.372840648411623
    mean_raw_obs_processing_ms: 216.86397674661114
  time_since_restore: 300227.4582285881
  time_this_iter_s: 530.326865196228
  time_total_s: 300227.4582285881
  timers:
    learn_throughput: 719.476
    learn_time_ms: 2626.912
    load_throughput: 172554.381
    load_time_ms: 10.953
    sample_throughput: 2.441
    sample_time_ms: 774334.356
    update_time_ms: 29.296
  timestamp: 1632309534
  timesteps_since_restore: 0
  timesteps_total: 470610
  training_iteration: 249
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    249 |           300227 | 470610 |  4.88204 |              8.00874 |              2.19952 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 472500
  custom_metrics: {}
  date: 2021-09-22_05-16-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.839105196338633
  episode_reward_mean: 4.87519487065459
  episode_reward_min: 2.193810115692923
  episodes_this_iter: 270
  episodes_total: 67500
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.269000030239113e-05
          entropy: 10.9540433883667
          entropy_coeff: 0.0002510497870389372
          kl: 0.015682358294725418
          model: {}
          policy_loss: -0.17593595385551453
          total_loss: -0.12108053267002106
          vf_explained_var: 0.9936316609382629
          vf_loss: 0.02187902294099331
    num_agent_steps_sampled: 472500
    num_agent_steps_trained: 472500
    num_steps_sampled: 472500
    num_steps_trained: 472500
  iterations_since_restore: 250
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.419646936656285
    ram_util_percent: 7.327435098650052
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987726735868463
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2797.673674053596
    mean_inference_ms: 2.3726658997308756
    mean_raw_obs_processing_ms: 216.82302526691336
  time_since_restore: 303695.4576408863
  time_this_iter_s: 3467.9994122982025
  time_total_s: 303695.4576408863
  timers:
    learn_throughput: 720.226
    learn_time_ms: 2624.175
    load_throughput: 171675.46
    load_time_ms: 11.009
    sample_throughput: 1.867
    sample_time_ms: 1012252.372
    update_time_ms: 29.326
  timestamp: 1632313002
  timesteps_since_restore: 0
  timesteps_total: 472500
  training_iteration: 250
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    250 |           303695 | 472500 |  4.87519 |              7.83911 |              2.19381 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 474390
  custom_metrics: {}
  date: 2021-09-22_05-37-05
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.830261973283963
  episode_reward_mean: 4.886765177137336
  episode_reward_min: 2.0239176488186192
  episodes_this_iter: 270
  episodes_total: 67770
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.249999958323315e-05
          entropy: 11.026497840881348
          entropy_coeff: 0.0002500500122550875
          kl: 0.015723364427685738
          model: {}
          policy_loss: -0.19527551531791687
          total_loss: -0.14046137034893036
          vf_explained_var: 0.9937347769737244
          vf_loss: 0.021751493215560913
    num_agent_steps_sampled: 474390
    num_agent_steps_trained: 474390
    num_steps_sampled: 474390
    num_steps_trained: 474390
  iterations_since_restore: 251
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.706482027106656
    ram_util_percent: 7.448850913376547
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987764593158255
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2795.739522828147
    mean_inference_ms: 2.37253483012142
    mean_raw_obs_processing_ms: 216.77837317311835
  time_since_restore: 304918.1502497196
  time_this_iter_s: 1222.692608833313
  time_total_s: 304918.1502497196
  timers:
    learn_throughput: 720.517
    learn_time_ms: 2623.118
    load_throughput: 171240.089
    load_time_ms: 11.037
    sample_throughput: 1.742
    sample_time_ms: 1085149.007
    update_time_ms: 28.908
  timestamp: 1632314225
  timesteps_since_restore: 0
  timesteps_total: 474390
  training_iteration: 251
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    251 |           304918 | 474390 |  4.88677 |              7.83026 |              2.02392 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 476280
  custom_metrics: {}
  date: 2021-09-22_05-48-07
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.927790590705641
  episode_reward_mean: 4.9702418507340145
  episode_reward_min: 1.5093737432667416
  episodes_this_iter: 270
  episodes_total: 68040
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.230999886407517e-05
          entropy: 10.999211311340332
          entropy_coeff: 0.0002490502083674073
          kl: 0.013812626712024212
          model: {}
          policy_loss: -0.1885947436094284
          total_loss: -0.1371757537126541
          vf_explained_var: 0.9935961365699768
          vf_loss: 0.022691478952765465
    num_agent_steps_sampled: 476280
    num_agent_steps_trained: 476280
    num_steps_sampled: 476280
    num_steps_trained: 476280
  iterations_since_restore: 252
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.63115468409586
    ram_util_percent: 7.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09988986798883538
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2794.5298877449554
    mean_inference_ms: 2.3725566358766357
    mean_raw_obs_processing_ms: 216.73097332327384
  time_since_restore: 305579.50906062126
  time_this_iter_s: 661.3588109016418
  time_total_s: 305579.50906062126
  timers:
    learn_throughput: 719.384
    learn_time_ms: 2627.248
    load_throughput: 171938.717
    load_time_ms: 10.992
    sample_throughput: 1.732
    sample_time_ms: 1091402.478
    update_time_ms: 28.772
  timestamp: 1632314887
  timesteps_since_restore: 0
  timesteps_total: 476280
  training_iteration: 252
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    252 |           305580 | 476280 |  4.97024 |              7.92779 |              1.50937 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 478170
  custom_metrics: {}
  date: 2021-09-22_06-03-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.16009237750179
  episode_reward_mean: 4.894124043733503
  episode_reward_min: 2.02339291132093
  episodes_this_iter: 270
  episodes_total: 68310
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.2120001782896e-05
          entropy: 10.988465309143066
          entropy_coeff: 0.00024805040447972715
          kl: 0.014202757738530636
          model: {}
          policy_loss: -0.16663148999214172
          total_loss: -0.11158831417560577
          vf_explained_var: 0.9927287101745605
          vf_loss: 0.02541322633624077
    num_agent_steps_sampled: 478170
    num_agent_steps_trained: 478170
    num_steps_sampled: 478170
    num_steps_trained: 478170
  iterations_since_restore: 253
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.80739893211289
    ram_util_percent: 7.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09988595133855298
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2793.1725457486236
    mean_inference_ms: 2.372655574335888
    mean_raw_obs_processing_ms: 216.68048293572028
  time_since_restore: 306524.4985332489
  time_this_iter_s: 944.9894726276398
  time_total_s: 306524.4985332489
  timers:
    learn_throughput: 718.606
    learn_time_ms: 2630.094
    load_throughput: 171316.323
    load_time_ms: 11.032
    sample_throughput: 1.661
    sample_time_ms: 1137974.954
    update_time_ms: 28.661
  timestamp: 1632315832
  timesteps_since_restore: 0
  timesteps_total: 478170
  training_iteration: 253
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    253 |           306524 | 478170 |  4.89412 |              8.16009 |              2.02339 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 480060
  custom_metrics: {}
  date: 2021-09-22_06-48-21
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.534826319544801
  episode_reward_mean: 4.953511657538877
  episode_reward_min: 1.6423590320299348
  episodes_this_iter: 270
  episodes_total: 68580
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.193000106373802e-05
          entropy: 10.95244026184082
          entropy_coeff: 0.000247050600592047
          kl: 0.015666602179408073
          model: {}
          policy_loss: -0.17635689675807953
          total_loss: -0.12201615422964096
          vf_explained_var: 0.993958592414856
          vf_loss: 0.02135605178773403
    num_agent_steps_sampled: 480060
    num_agent_steps_trained: 480060
    num_steps_sampled: 480060
    num_steps_trained: 480060
  iterations_since_restore: 254
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.33982730706962
    ram_util_percent: 7.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987835762996994
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2794.863928015142
    mean_inference_ms: 2.3728774661608902
    mean_raw_obs_processing_ms: 216.63090057755554
  time_since_restore: 309193.53381705284
  time_this_iter_s: 2669.03528380394
  time_total_s: 309193.53381705284
  timers:
    learn_throughput: 718.303
    learn_time_ms: 2631.2
    load_throughput: 171686.987
    load_time_ms: 11.008
    sample_throughput: 1.39
    sample_time_ms: 1359471.935
    update_time_ms: 28.648
  timestamp: 1632318501
  timesteps_since_restore: 0
  timesteps_total: 480060
  training_iteration: 254
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    254 |           309194 | 480060 |  4.95351 |              7.53483 |              1.64236 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 481950
  custom_metrics: {}
  date: 2021-09-22_06-52-29
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.626572223602921
  episode_reward_mean: 4.880056408570903
  episode_reward_min: 1.8856813026544574
  episodes_this_iter: 270
  episodes_total: 68850
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.174000034458004e-05
          entropy: 10.916592597961426
          entropy_coeff: 0.0002460507967043668
          kl: 0.01645110920071602
          model: {}
          policy_loss: -0.20049262046813965
          total_loss: -0.1438291072845459
          vf_explained_var: 0.9936938285827637
          vf_loss: 0.02187185361981392
    num_agent_steps_sampled: 481950
    num_agent_steps_trained: 481950
    num_steps_sampled: 481950
    num_steps_trained: 481950
  iterations_since_restore: 255
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.19273255813954
    ram_util_percent: 7.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986337048709915
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2788.3765574013055
    mean_inference_ms: 2.3726051697963015
    mean_raw_obs_processing_ms: 216.57929373834455
  time_since_restore: 309441.73666882515
  time_this_iter_s: 248.20285177230835
  time_total_s: 309441.73666882515
  timers:
    learn_throughput: 716.958
    learn_time_ms: 2636.138
    load_throughput: 172129.495
    load_time_ms: 10.98
    sample_throughput: 1.443
    sample_time_ms: 1309636.153
    update_time_ms: 28.683
  timestamp: 1632318749
  timesteps_since_restore: 0
  timesteps_total: 481950
  training_iteration: 255
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    255 |           309442 | 481950 |  4.88006 |              7.62657 |              1.88568 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 483840
  custom_metrics: {}
  date: 2021-09-22_06-59-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.810891178585055
  episode_reward_mean: 4.91186913189852
  episode_reward_min: 2.0142582999956113
  episodes_this_iter: 270
  episodes_total: 69120
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.154999962542206e-05
          entropy: 10.965240478515625
          entropy_coeff: 0.00024505099281668663
          kl: 0.01623946987092495
          model: {}
          policy_loss: -0.18731482326984406
          total_loss: -0.1274460405111313
          vf_explained_var: 0.992486834526062
          vf_loss: 0.0255602840334177
    num_agent_steps_sampled: 483840
    num_agent_steps_trained: 483840
    num_steps_sampled: 483840
    num_steps_trained: 483840
  iterations_since_restore: 256
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.62357019064125
    ram_util_percent: 7.5
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985754993485421
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2783.399770295203
    mean_inference_ms: 2.372683824060057
    mean_raw_obs_processing_ms: 216.5358347587087
  time_since_restore: 309856.92430996895
  time_this_iter_s: 415.1876411437988
  time_total_s: 309856.92430996895
  timers:
    learn_throughput: 716.022
    learn_time_ms: 2639.584
    load_throughput: 172638.182
    load_time_ms: 10.948
    sample_throughput: 1.508
    sample_time_ms: 1253472.159
    update_time_ms: 28.355
  timestamp: 1632319164
  timesteps_since_restore: 0
  timesteps_total: 483840
  training_iteration: 256
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    256 |           309857 | 483840 |  4.91187 |              7.81089 |              2.01426 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 485730
  custom_metrics: {}
  date: 2021-09-22_07-38-37
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.665042539549947
  episode_reward_mean: 4.847498475301578
  episode_reward_min: 1.840163001119144
  episodes_this_iter: 270
  episodes_total: 69390
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.135999890626408e-05
          entropy: 11.010719299316406
          entropy_coeff: 0.00024405120348092169
          kl: 0.014375356025993824
          model: {}
          policy_loss: -0.19185926020145416
          total_loss: -0.1385210156440735
          vf_explained_var: 0.993251383304596
          vf_loss: 0.023276563733816147
    num_agent_steps_sampled: 485730
    num_agent_steps_trained: 485730
    num_steps_sampled: 485730
    num_steps_trained: 485730
  iterations_since_restore: 257
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.2973361910594
    ram_util_percent: 7.52354562155542
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998590700445277
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2787.59660897677
    mean_inference_ms: 2.3725496045993997
    mean_raw_obs_processing_ms: 216.48393258307198
  time_since_restore: 312209.47222042084
  time_this_iter_s: 2352.547910451889
  time_total_s: 312209.47222042084
  timers:
    learn_throughput: 715.684
    learn_time_ms: 2640.83
    load_throughput: 172787.571
    load_time_ms: 10.938
    sample_throughput: 1.36
    sample_time_ms: 1389646.339
    update_time_ms: 28.397
  timestamp: 1632321517
  timesteps_since_restore: 0
  timesteps_total: 485730
  training_iteration: 257
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    257 |           312209 | 485730 |   4.8475 |              7.66504 |              1.84016 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 487620
  custom_metrics: {}
  date: 2021-09-22_08-12-38
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.745874902222804
  episode_reward_mean: 4.9086876844917375
  episode_reward_min: 1.9190835322497597
  episodes_this_iter: 270
  episodes_total: 69660
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.11699981871061e-05
          entropy: 10.934419631958008
          entropy_coeff: 0.0002430513995932415
          kl: 0.014842064119875431
          model: {}
          policy_loss: -0.19015182554721832
          total_loss: -0.13579261302947998
          vf_explained_var: 0.9932474493980408
          vf_loss: 0.02320476807653904
    num_agent_steps_sampled: 487620
    num_agent_steps_trained: 487620
    num_steps_sampled: 487620
    num_steps_trained: 487620
  iterations_since_restore: 258
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.37020825979527
    ram_util_percent: 7.699188139781149
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986401322795907
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2789.9866184598445
    mean_inference_ms: 2.3723879649507613
    mean_raw_obs_processing_ms: 216.4388305094777
  time_since_restore: 314250.81277537346
  time_this_iter_s: 2041.3405549526215
  time_total_s: 314250.81277537346
  timers:
    learn_throughput: 715.871
    learn_time_ms: 2640.139
    load_throughput: 157167.277
    load_time_ms: 12.025
    sample_throughput: 1.301
    sample_time_ms: 1452554.684
    update_time_ms: 28.357
  timestamp: 1632323558
  timesteps_since_restore: 0
  timesteps_total: 487620
  training_iteration: 258
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    258 |           314251 | 487620 |  4.90869 |              7.74587 |              1.91908 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 489510
  custom_metrics: {}
  date: 2021-09-22_08-34-35
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.9129739179778555
  episode_reward_mean: 4.949925055272525
  episode_reward_min: 1.754853923042067
  episodes_this_iter: 270
  episodes_total: 69930
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.098000110592693e-05
          entropy: 10.871417045593262
          entropy_coeff: 0.00024205159570556134
          kl: 0.01489864569157362
          model: {}
          policy_loss: -0.1992918699979782
          total_loss: -0.14534462988376617
          vf_explained_var: 0.9935499429702759
          vf_loss: 0.022637683898210526
    num_agent_steps_sampled: 489510
    num_agent_steps_trained: 489510
    num_steps_sampled: 489510
    num_steps_trained: 489510
  iterations_since_restore: 259
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.57217058501914
    ram_util_percent: 7.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986222795197552
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2785.7014152312727
    mean_inference_ms: 2.3720724672825595
    mean_raw_obs_processing_ms: 216.3986878415093
  time_since_restore: 315568.06712150574
  time_this_iter_s: 1317.2543461322784
  time_total_s: 315568.06712150574
  timers:
    learn_throughput: 716.016
    learn_time_ms: 2639.606
    load_throughput: 157778.519
    load_time_ms: 11.979
    sample_throughput: 1.234
    sample_time_ms: 1531247.454
    update_time_ms: 28.686
  timestamp: 1632324875
  timesteps_since_restore: 0
  timesteps_total: 489510
  training_iteration: 259
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    259 |           315568 | 489510 |  4.94993 |              7.91297 |              1.75485 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 491400
  custom_metrics: {}
  date: 2021-09-22_08-41-58
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.5377728344845565
  episode_reward_mean: 4.931828955791478
  episode_reward_min: 2.059881320400166
  episodes_this_iter: 270
  episodes_total: 70200
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.079000038676895e-05
          entropy: 10.874582290649414
          entropy_coeff: 0.0002410518063697964
          kl: 0.014463409781455994
          model: {}
          policy_loss: -0.18337425589561462
          total_loss: -0.131735697388649
          vf_explained_var: 0.9937519431114197
          vf_loss: 0.02131045237183571
    num_agent_steps_sampled: 491400
    num_agent_steps_trained: 491400
    num_steps_sampled: 491400
    num_steps_trained: 491400
  iterations_since_restore: 260
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.561074918566774
    ram_util_percent: 7.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09988174763265399
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2779.465706849208
    mean_inference_ms: 2.3726581276946743
    mean_raw_obs_processing_ms: 216.3549596263868
  time_since_restore: 316010.28738188744
  time_this_iter_s: 442.2202603816986
  time_total_s: 316010.28738188744
  timers:
    learn_throughput: 715.979
    learn_time_ms: 2639.741
    load_throughput: 157244.593
    load_time_ms: 12.019
    sample_throughput: 1.538
    sample_time_ms: 1228669.58
    update_time_ms: 28.772
  timestamp: 1632325318
  timesteps_since_restore: 0
  timesteps_total: 491400
  training_iteration: 260
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    260 |           316010 | 491400 |  4.93183 |              7.53777 |              2.05988 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 493290
  custom_metrics: {}
  date: 2021-09-22_08-45-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.687860149570071
  episode_reward_mean: 5.016526715200008
  episode_reward_min: 2.0695802196226896
  episodes_this_iter: 270
  episodes_total: 70470
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.059999966761097e-05
          entropy: 10.849088668823242
          entropy_coeff: 0.00024005200248211622
          kl: 0.014210988767445087
          model: {}
          policy_loss: -0.1902584284543991
          total_loss: -0.1378868967294693
          vf_explained_var: 0.9936754107475281
          vf_loss: 0.022601479664444923
    num_agent_steps_sampled: 493290
    num_agent_steps_trained: 493290
    num_steps_sampled: 493290
    num_steps_trained: 493290
  iterations_since_restore: 261
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.436305732484076
    ram_util_percent: 7.8
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987842663990458
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2772.697208285386
    mean_inference_ms: 2.3724838070710716
    mean_raw_obs_processing_ms: 216.31763519085715
  time_since_restore: 316237.070335865
  time_this_iter_s: 226.78295397758484
  time_total_s: 316237.070335865
  timers:
    learn_throughput: 716.432
    learn_time_ms: 2638.075
    load_throughput: 159645.286
    load_time_ms: 11.839
    sample_throughput: 1.674
    sample_time_ms: 1129080.07
    update_time_ms: 29.167
  timestamp: 1632325545
  timesteps_since_restore: 0
  timesteps_total: 493290
  training_iteration: 261
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    261 |           316237 | 493290 |  5.01653 |              7.68786 |              2.06958 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 495180
  custom_metrics: {}
  date: 2021-09-22_09-01-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.97089846652441
  episode_reward_mean: 4.954610952739881
  episode_reward_min: 1.7869500459760255
  episodes_this_iter: 270
  episodes_total: 70740
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.0409998948452994e-05
          entropy: 10.866453170776367
          entropy_coeff: 0.00023905219859443605
          kl: 0.013736091554164886
          model: {}
          policy_loss: -0.18185076117515564
          total_loss: -0.1292799711227417
          vf_explained_var: 0.9932466149330139
          vf_loss: 0.023875918239355087
    num_agent_steps_sampled: 495180
    num_agent_steps_trained: 495180
    num_steps_sampled: 495180
    num_steps_trained: 495180
  iterations_since_restore: 262
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.479182156133827
    ram_util_percent: 7.799999999999997
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09987890956829333
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2773.132664242541
    mean_inference_ms: 2.3723262940203256
    mean_raw_obs_processing_ms: 216.27379046560526
  time_since_restore: 317204.8510608673
  time_this_iter_s: 967.7807250022888
  time_total_s: 317204.8510608673
  timers:
    learn_throughput: 717.266
    learn_time_ms: 2635.008
    load_throughput: 159160.004
    load_time_ms: 11.875
    sample_throughput: 1.63
    sample_time_ms: 1159725.331
    update_time_ms: 29.671
  timestamp: 1632326512
  timesteps_since_restore: 0
  timesteps_total: 495180
  training_iteration: 262
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    262 |           317205 | 495180 |  4.95461 |               7.9709 |              1.78695 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 497070
  custom_metrics: {}
  date: 2021-09-22_09-12-21
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.946041765882171
  episode_reward_mean: 4.914390106894652
  episode_reward_min: 1.9264978561647021
  episodes_this_iter: 270
  episodes_total: 71010
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.0219998229295015e-05
          entropy: 10.863774299621582
          entropy_coeff: 0.00023805239470675588
          kl: 0.014388920739293098
          model: {}
          policy_loss: -0.19616089761257172
          total_loss: -0.1458965390920639
          vf_explained_var: 0.9941207766532898
          vf_loss: 0.020070740953087807
    num_agent_steps_sampled: 497070
    num_agent_steps_trained: 497070
    num_steps_sampled: 497070
    num_steps_trained: 497070
  iterations_since_restore: 263
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.270183486238533
    ram_util_percent: 7.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998738096372234
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2769.8736958997574
    mean_inference_ms: 2.3724798552428275
    mean_raw_obs_processing_ms: 216.23388961576555
  time_since_restore: 317833.10579800606
  time_this_iter_s: 628.2547371387482
  time_total_s: 317833.10579800606
  timers:
    learn_throughput: 718.627
    learn_time_ms: 2630.016
    load_throughput: 159267.767
    load_time_ms: 11.867
    sample_throughput: 1.675
    sample_time_ms: 1128056.819
    update_time_ms: 29.779
  timestamp: 1632327141
  timesteps_since_restore: 0
  timesteps_total: 497070
  training_iteration: 263
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    263 |           317833 | 497070 |  4.91439 |              7.94604 |               1.9265 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 498960
  custom_metrics: {}
  date: 2021-09-22_09-26-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.377663288807362
  episode_reward_mean: 4.945860891795659
  episode_reward_min: 2.064735705910528
  episodes_this_iter: 270
  episodes_total: 71280
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.0030001148115844e-05
          entropy: 10.809536933898926
          entropy_coeff: 0.00023705260537099093
          kl: 0.016322435811161995
          model: {}
          policy_loss: -0.16945692896842957
          total_loss: -0.11119683086872101
          vf_explained_var: 0.9932818412780762
          vf_loss: 0.023637978360056877
    num_agent_steps_sampled: 498960
    num_agent_steps_trained: 498960
    num_steps_sampled: 498960
    num_steps_trained: 498960
  iterations_since_restore: 264
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.08008510638298
    ram_util_percent: 7.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09988029390631124
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2766.9559102832336
    mean_inference_ms: 2.3721029225839754
    mean_raw_obs_processing_ms: 216.18434107594183
  time_since_restore: 318679.62244319916
  time_this_iter_s: 846.5166451931
  time_total_s: 318679.62244319916
  timers:
    learn_throughput: 719.149
    learn_time_ms: 2628.105
    load_throughput: 159450.69
    load_time_ms: 11.853
    sample_throughput: 1.998
    sample_time_ms: 945806.964
    update_time_ms: 29.606
  timestamp: 1632327987
  timesteps_since_restore: 0
  timesteps_total: 498960
  training_iteration: 264
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    264 |           318680 | 498960 |  4.94586 |              8.37766 |              2.06474 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 500850
  custom_metrics: {}
  date: 2021-09-22_09-34-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.743482732368074
  episode_reward_mean: 4.886946298297327
  episode_reward_min: 1.7317818296921155
  episodes_this_iter: 270
  episodes_total: 71550
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.9840000428957865e-05
          entropy: 10.874510765075684
          entropy_coeff: 0.00023605280148331076
          kl: 0.013364902697503567
          model: {}
          policy_loss: -0.19724313914775848
          total_loss: -0.1464676558971405
          vf_explained_var: 0.993466317653656
          vf_loss: 0.022895539179444313
    num_agent_steps_sampled: 500850
    num_agent_steps_trained: 500850
    num_steps_sampled: 500850
    num_steps_trained: 500850
  iterations_since_restore: 265
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.589306358381503
    ram_util_percent: 7.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998758249623486
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2760.7182892693986
    mean_inference_ms: 2.3720291323143883
    mean_raw_obs_processing_ms: 216.1467004410301
  time_since_restore: 319177.6629076004
  time_this_iter_s: 498.0404644012451
  time_total_s: 319177.6629076004
  timers:
    learn_throughput: 712.92
    learn_time_ms: 2651.07
    load_throughput: 159941.944
    load_time_ms: 11.817
    sample_throughput: 1.947
    sample_time_ms: 970767.983
    update_time_ms: 29.75
  timestamp: 1632328485
  timesteps_since_restore: 0
  timesteps_total: 500850
  training_iteration: 265
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    265 |           319178 | 500850 |  4.88695 |              7.74348 |              1.73178 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 502740
  custom_metrics: {}
  date: 2021-09-22_09-42-38
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.766881564318267
  episode_reward_mean: 4.90901553458933
  episode_reward_min: 2.085487673301702
  episodes_this_iter: 270
  episodes_total: 71820
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.9649999709799886e-05
          entropy: 10.874176979064941
          entropy_coeff: 0.00023505299759563059
          kl: 0.014252620749175549
          model: {}
          policy_loss: -0.16441838443279266
          total_loss: -0.11202401667833328
          vf_explained_var: 0.993428647518158
          vf_loss: 0.022481124848127365
    num_agent_steps_sampled: 502740
    num_agent_steps_trained: 502740
    num_steps_sampled: 502740
    num_steps_trained: 502740
  iterations_since_restore: 266
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.736280487804876
    ram_util_percent: 7.799999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998656822619911
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2755.8600459476916
    mean_inference_ms: 2.3720996893670114
    mean_raw_obs_processing_ms: 216.12061757422666
  time_since_restore: 319650.2253551483
  time_this_iter_s: 472.5624475479126
  time_total_s: 319650.2253551483
  timers:
    learn_throughput: 712.662
    learn_time_ms: 2652.029
    load_throughput: 160303.542
    load_time_ms: 11.79
    sample_throughput: 1.935
    sample_time_ms: 976504.557
    update_time_ms: 29.748
  timestamp: 1632328958
  timesteps_since_restore: 0
  timesteps_total: 502740
  training_iteration: 266
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    266 |           319650 | 502740 |  4.90902 |              7.76688 |              2.08549 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 504630
  custom_metrics: {}
  date: 2021-09-22_09-55-30
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.18292199212579
  episode_reward_mean: 4.9606574939486965
  episode_reward_min: 2.3688943659048927
  episodes_this_iter: 270
  episodes_total: 72090
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.945999899064191e-05
          entropy: 10.865742683410645
          entropy_coeff: 0.0002340531937079504
          kl: 0.014836149290204048
          model: {}
          policy_loss: -0.18121221661567688
          total_loss: -0.12693466246128082
          vf_explained_var: 0.9933057427406311
          vf_loss: 0.02302214317023754
    num_agent_steps_sampled: 504630
    num_agent_steps_trained: 504630
    num_steps_sampled: 504630
    num_steps_trained: 504630
  iterations_since_restore: 267
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.40839552238806
    ram_util_percent: 7.799999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986704179188811
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2755.6768186126415
    mean_inference_ms: 2.372255896654764
    mean_raw_obs_processing_ms: 216.08491587418987
  time_since_restore: 320422.7158987522
  time_this_iter_s: 772.4905436038971
  time_total_s: 320422.7158987522
  timers:
    learn_throughput: 711.634
    learn_time_ms: 2655.859
    load_throughput: 160633.57
    load_time_ms: 11.766
    sample_throughput: 2.309
    sample_time_ms: 818495.601
    update_time_ms: 29.684
  timestamp: 1632329730
  timesteps_since_restore: 0
  timesteps_total: 504630
  training_iteration: 267
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    267 |           320423 | 504630 |  4.96066 |              8.18292 |              2.36889 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 506520
  custom_metrics: {}
  date: 2021-09-22_10-05-25
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.803111574605946
  episode_reward_mean: 4.971363296439961
  episode_reward_min: 2.1650158729164226
  episodes_this_iter: 270
  episodes_total: 72360
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.926999827148393e-05
          entropy: 10.843690872192383
          entropy_coeff: 0.00023305340437218547
          kl: 0.014965353533625603
          model: {}
          policy_loss: -0.19021572172641754
          total_loss: -0.13601328432559967
          vf_explained_var: 0.9936341047286987
          vf_loss: 0.02263665571808815
    num_agent_steps_sampled: 506520
    num_agent_steps_trained: 506520
    num_steps_sampled: 506520
    num_steps_trained: 506520
  iterations_since_restore: 268
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.211636363636362
    ram_util_percent: 7.799999999999998
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986249441819639
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2753.28261183132
    mean_inference_ms: 2.372025340926984
    mean_raw_obs_processing_ms: 216.046372578609
  time_since_restore: 321017.0696208477
  time_this_iter_s: 594.3537220954895
  time_total_s: 321017.0696208477
  timers:
    learn_throughput: 711.215
    learn_time_ms: 2657.424
    load_throughput: 177650.665
    load_time_ms: 10.639
    sample_throughput: 2.805
    sample_time_ms: 673796.429
    update_time_ms: 29.935
  timestamp: 1632330325
  timesteps_since_restore: 0
  timesteps_total: 506520
  training_iteration: 268
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 17.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    268 |           321017 | 506520 |  4.97136 |              7.80311 |              2.16502 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 508410
  custom_metrics: {}
  date: 2021-09-22_10-14-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.863380362047312
  episode_reward_mean: 4.951358731713667
  episode_reward_min: 1.9813989935944372
  episodes_this_iter: 270
  episodes_total: 72630
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.9080001190304756e-05
          entropy: 10.766423225402832
          entropy_coeff: 0.0002320536004845053
          kl: 0.01569068245589733
          model: {}
          policy_loss: -0.18475636839866638
          total_loss: -0.1273629516363144
          vf_explained_var: 0.9931530356407166
          vf_loss: 0.024146435782313347
    num_agent_steps_sampled: 508410
    num_agent_steps_trained: 508410
    num_steps_sampled: 508410
    num_steps_trained: 508410
  iterations_since_restore: 269
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.980774365821098
    ram_util_percent: 7.799999999999999
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985967752245713
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2747.934221738719
    mean_inference_ms: 2.372418196883066
    mean_raw_obs_processing_ms: 216.0017722305284
  time_since_restore: 321556.2543628216
  time_this_iter_s: 539.184741973877
  time_total_s: 321556.2543628216
  timers:
    learn_throughput: 711.468
    learn_time_ms: 2656.479
    load_throughput: 177299.424
    load_time_ms: 10.66
    sample_throughput: 3.171
    sample_time_ms: 595990.571
    update_time_ms: 29.609
  timestamp: 1632330864
  timesteps_since_restore: 0
  timesteps_total: 508410
  training_iteration: 269
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    269 |           321556 | 508410 |  4.95136 |              7.86338 |               1.9814 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 510300
  custom_metrics: {}
  date: 2021-09-22_10-24-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.7026676759807415
  episode_reward_mean: 5.016870099209249
  episode_reward_min: 1.8032837639781512
  episodes_this_iter: 270
  episodes_total: 72900
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.889000047114678e-05
          entropy: 10.813544273376465
          entropy_coeff: 0.00023105379659682512
          kl: 0.01565747894346714
          model: {}
          policy_loss: -0.18790927529335022
          total_loss: -0.13188418745994568
          vf_explained_var: 0.9934625625610352
          vf_loss: 0.022853905335068703
    num_agent_steps_sampled: 510300
    num_agent_steps_trained: 510300
    num_steps_sampled: 510300
    num_steps_trained: 510300
  iterations_since_restore: 270
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.57776412776413
    ram_util_percent: 7.799999999999997
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985793337154539
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2744.19867332381
    mean_inference_ms: 2.3723436188727947
    mean_raw_obs_processing_ms: 215.955478829041
  time_since_restore: 322142.5987422466
  time_this_iter_s: 586.3443794250488
  time_total_s: 322142.5987422466
  timers:
    learn_throughput: 710.738
    learn_time_ms: 2659.206
    load_throughput: 177275.238
    load_time_ms: 10.661
    sample_throughput: 3.096
    sample_time_ms: 610400.783
    update_time_ms: 29.199
  timestamp: 1632331450
  timesteps_since_restore: 0
  timesteps_total: 510300
  training_iteration: 270
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    270 |           322143 | 510300 |  5.01687 |              7.70267 |              1.80328 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 512190
  custom_metrics: {}
  date: 2021-09-22_10-55-22
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.503151929438737
  episode_reward_mean: 4.939525855235502
  episode_reward_min: 1.8710383962835733
  episodes_this_iter: 270
  episodes_total: 73170
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.86999997519888e-05
          entropy: 10.800613403320312
          entropy_coeff: 0.00023005400726106018
          kl: 0.014809811487793922
          model: {}
          policy_loss: -0.18750396370887756
          total_loss: -0.13504134118556976
          vf_explained_var: 0.9938990473747253
          vf_loss: 0.021208733320236206
    num_agent_steps_sampled: 512190
    num_agent_steps_trained: 512190
    num_steps_sampled: 512190
    num_steps_trained: 512190
  iterations_since_restore: 271
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.82326923076923
    ram_util_percent: 8.084615384615383
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985520481010404
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2742.0854002461565
    mean_inference_ms: 2.372143200990809
    mean_raw_obs_processing_ms: 215.9213156772964
  time_since_restore: 324013.6985707283
  time_this_iter_s: 1871.0998284816742
  time_total_s: 324013.6985707283
  timers:
    learn_throughput: 708.965
    learn_time_ms: 2665.857
    load_throughput: 176179.169
    load_time_ms: 10.728
    sample_throughput: 2.439
    sample_time_ms: 774825.436
    update_time_ms: 28.728
  timestamp: 1632333322
  timesteps_since_restore: 0
  timesteps_total: 512190
  training_iteration: 271
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 15.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    271 |           324014 | 512190 |  4.93953 |              7.50315 |              1.87104 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 514080
  custom_metrics: {}
  date: 2021-09-22_11-07-58
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.635524406781962
  episode_reward_mean: 4.92160452444881
  episode_reward_min: 2.2623305054578506
  episodes_this_iter: 270
  episodes_total: 73440
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.850999903283082e-05
          entropy: 10.779939651489258
          entropy_coeff: 0.00022905420337338
          kl: 0.014572241343557835
          model: {}
          policy_loss: -0.17462147772312164
          total_loss: -0.12215595692396164
          vf_explained_var: 0.9937743544578552
          vf_loss: 0.02173730544745922
    num_agent_steps_sampled: 514080
    num_agent_steps_trained: 514080
    num_steps_sampled: 514080
    num_steps_trained: 514080
  iterations_since_restore: 272
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.07478591817318
    ram_util_percent: 7.947002854424357
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985099035349691
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2740.1785374401775
    mean_inference_ms: 2.372183005084604
    mean_raw_obs_processing_ms: 215.8868651276341
  time_since_restore: 324770.02667832375
  time_this_iter_s: 756.3281075954437
  time_total_s: 324770.02667832375
  timers:
    learn_throughput: 708.681
    learn_time_ms: 2666.925
    load_throughput: 176569.231
    load_time_ms: 10.704
    sample_throughput: 2.508
    sample_time_ms: 753679.237
    update_time_ms: 28.256
  timestamp: 1632334078
  timesteps_since_restore: 0
  timesteps_total: 514080
  training_iteration: 272
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    272 |           324770 | 514080 |   4.9216 |              7.63552 |              2.26233 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 515970
  custom_metrics: {}
  date: 2021-09-22_11-30-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.800198305311534
  episode_reward_mean: 4.987880706492983
  episode_reward_min: 2.0722934494646212
  episodes_this_iter: 270
  episodes_total: 73710
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.831999831367284e-05
          entropy: 10.698845863342285
          entropy_coeff: 0.00022805439948569983
          kl: 0.014668130315840244
          model: {}
          policy_loss: -0.20554065704345703
          total_loss: -0.15332327783107758
          vf_explained_var: 0.9937807321548462
          vf_loss: 0.021241476759314537
    num_agent_steps_sampled: 515970
    num_agent_steps_trained: 515970
    num_steps_sampled: 515970
    num_steps_trained: 515970
  iterations_since_restore: 273
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.6133870967742
    ram_util_percent: 8.17284946236559
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09984758054277768
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2739.0750203867287
    mean_inference_ms: 2.3721772074714416
    mean_raw_obs_processing_ms: 215.85667988899698
  time_since_restore: 326108.11361193657
  time_this_iter_s: 1338.0869336128235
  time_total_s: 326108.11361193657
  timers:
    learn_throughput: 707.1
    learn_time_ms: 2672.888
    load_throughput: 177679.732
    load_time_ms: 10.637
    sample_throughput: 2.292
    sample_time_ms: 824655.539
    update_time_ms: 28.163
  timestamp: 1632335416
  timesteps_since_restore: 0
  timesteps_total: 515970
  training_iteration: 273
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    273 |           326108 | 515970 |  4.98788 |               7.8002 |              2.07229 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 517860
  custom_metrics: {}
  date: 2021-09-22_11-34-59
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.662846367985075
  episode_reward_mean: 4.896351023215829
  episode_reward_min: 1.4962647741272164
  episodes_this_iter: 270
  episodes_total: 73980
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.813000123249367e-05
          entropy: 10.73002815246582
          entropy_coeff: 0.00022705459559801966
          kl: 0.014116188511252403
          model: {}
          policy_loss: -0.1841815561056137
          total_loss: -0.13117435574531555
          vf_explained_var: 0.993101179599762
          vf_loss: 0.02328506112098694
    num_agent_steps_sampled: 517860
    num_agent_steps_trained: 517860
    num_steps_sampled: 517860
    num_steps_trained: 517860
  iterations_since_restore: 274
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.54681933842239
    ram_util_percent: 8.700000000000001
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985214313080722
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2733.25095676322
    mean_inference_ms: 2.3721114727443675
    mean_raw_obs_processing_ms: 215.81357354209865
  time_since_restore: 326390.88495469093
  time_this_iter_s: 282.771342754364
  time_total_s: 326390.88495469093
  timers:
    learn_throughput: 706.875
    learn_time_ms: 2673.739
    load_throughput: 178076.075
    load_time_ms: 10.613
    sample_throughput: 2.46
    sample_time_ms: 768280.346
    update_time_ms: 28.217
  timestamp: 1632335699
  timesteps_since_restore: 0
  timesteps_total: 517860
  training_iteration: 274
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    274 |           326391 | 517860 |  4.89635 |              7.66285 |              1.49626 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 519750
  custom_metrics: {}
  date: 2021-09-22_12-16-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.613291231732071
  episode_reward_mean: 5.047991193241578
  episode_reward_min: 2.1091819622971064
  episodes_this_iter: 270
  episodes_total: 74250
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.794000051333569e-05
          entropy: 10.819414138793945
          entropy_coeff: 0.00022605480626225471
          kl: 0.014779074117541313
          model: {}
          policy_loss: -0.1842259168624878
          total_loss: -0.13161884248256683
          vf_explained_var: 0.9940574765205383
          vf_loss: 0.021384265273809433
    num_agent_steps_sampled: 519750
    num_agent_steps_trained: 519750
    num_steps_sampled: 519750
    num_steps_trained: 519750
  iterations_since_restore: 275
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.13723776223776
    ram_util_percent: 8.700000000000001
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09984441878804177
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2735.7807537130298
    mean_inference_ms: 2.371920348967885
    mean_raw_obs_processing_ms: 215.77548774985522
  time_since_restore: 328860.2944717407
  time_this_iter_s: 2469.4095170497894
  time_total_s: 328860.2944717407
  timers:
    learn_throughput: 712.638
    learn_time_ms: 2652.118
    load_throughput: 177755.033
    load_time_ms: 10.633
    sample_throughput: 1.958
    sample_time_ms: 965438.345
    update_time_ms: 28.375
  timestamp: 1632338168
  timesteps_since_restore: 0
  timesteps_total: 519750
  training_iteration: 275
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    275 |           328860 | 519750 |  5.04799 |              7.61329 |              2.10918 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 521640
  custom_metrics: {}
  date: 2021-09-22_12-27-39
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.81149083243513
  episode_reward_mean: 4.891832201028949
  episode_reward_min: 1.8319171794730393
  episodes_this_iter: 270
  episodes_total: 74520
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.774999979417771e-05
          entropy: 10.759343147277832
          entropy_coeff: 0.00022505500237457454
          kl: 0.014314679428935051
          model: {}
          policy_loss: -0.17262816429138184
          total_loss: -0.12077231705188751
          vf_explained_var: 0.9935857057571411
          vf_loss: 0.02166666090488434
    num_agent_steps_sampled: 521640
    num_agent_steps_trained: 521640
    num_steps_sampled: 521640
    num_steps_trained: 521640
  iterations_since_restore: 276
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.406979166666666
    ram_util_percent: 8.700000000000001
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998421532336333
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2732.0758979837206
    mean_inference_ms: 2.3717392056967643
    mean_raw_obs_processing_ms: 215.73671881088237
  time_since_restore: 329551.1089680195
  time_this_iter_s: 690.8144962787628
  time_total_s: 329551.1089680195
  timers:
    learn_throughput: 711.968
    learn_time_ms: 2654.613
    load_throughput: 175265.744
    load_time_ms: 10.784
    sample_throughput: 1.914
    sample_time_ms: 987260.939
    update_time_ms: 28.496
  timestamp: 1632338859
  timesteps_since_restore: 0
  timesteps_total: 521640
  training_iteration: 276
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    276 |           329551 | 521640 |  4.89183 |              7.81149 |              1.83192 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 523530
  custom_metrics: {}
  date: 2021-09-22_12-45-03
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.693867412671819
  episode_reward_mean: 4.907142740440647
  episode_reward_min: 2.2719154237072594
  episodes_this_iter: 270
  episodes_total: 74790
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.755999907501973e-05
          entropy: 10.774043083190918
          entropy_coeff: 0.00022405519848689437
          kl: 0.014221063815057278
          model: {}
          policy_loss: -0.1787864863872528
          total_loss: -0.12648959457874298
          vf_explained_var: 0.9933848977088928
          vf_loss: 0.02231350541114807
    num_agent_steps_sampled: 523530
    num_agent_steps_trained: 523530
    num_steps_sampled: 523530
    num_steps_trained: 523530
  iterations_since_restore: 277
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.96250861474845
    ram_util_percent: 8.671399035148173
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09984200928617852
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2732.02051571596
    mean_inference_ms: 2.3715460905865253
    mean_raw_obs_processing_ms: 215.69401456144797
  time_since_restore: 330594.79285788536
  time_this_iter_s: 1043.6838898658752
  time_total_s: 330594.79285788536
  timers:
    learn_throughput: 712.359
    learn_time_ms: 2653.155
    load_throughput: 175091.156
    load_time_ms: 10.794
    sample_throughput: 1.863
    sample_time_ms: 1014381.557
    update_time_ms: 28.712
  timestamp: 1632339903
  timesteps_since_restore: 0
  timesteps_total: 523530
  training_iteration: 277
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    277 |           330595 | 523530 |  4.90714 |              7.69387 |              2.27192 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 525420
  custom_metrics: {}
  date: 2021-09-22_12-52-07
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.947523649814933
  episode_reward_mean: 4.891257926494004
  episode_reward_min: 2.0415360689788886
  episodes_this_iter: 270
  episodes_total: 75060
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.736999835586175e-05
          entropy: 10.837011337280273
          entropy_coeff: 0.0002230553945992142
          kl: 0.014269704930484295
          model: {}
          policy_loss: -0.1735990345478058
          total_loss: -0.12201344966888428
          vf_explained_var: 0.9937569499015808
          vf_loss: 0.02149467170238495
    num_agent_steps_sampled: 525420
    num_agent_steps_trained: 525420
    num_steps_sampled: 525420
    num_steps_trained: 525420
  iterations_since_restore: 278
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.80543293718166
    ram_util_percent: 8.608828522920204
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09982788938511143
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2729.0984231498815
    mean_inference_ms: 2.3713152524575745
    mean_raw_obs_processing_ms: 215.65315843523308
  time_since_restore: 331019.2692837715
  time_this_iter_s: 424.4764258861542
  time_total_s: 331019.2692837715
  timers:
    learn_throughput: 709.772
    learn_time_ms: 2662.828
    load_throughput: 174278.176
    load_time_ms: 10.845
    sample_throughput: 1.895
    sample_time_ms: 997384.521
    update_time_ms: 28.421
  timestamp: 1632340327
  timesteps_since_restore: 0
  timesteps_total: 525420
  training_iteration: 278
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    278 |           331019 | 525420 |  4.89126 |              7.94752 |              2.04154 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 527310
  custom_metrics: {}
  date: 2021-09-22_13-01-18
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.892968214200574
  episode_reward_mean: 4.948999687395899
  episode_reward_min: 2.2431951180001435
  episodes_this_iter: 270
  episodes_total: 75330
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.718000127468258e-05
          entropy: 10.749321937561035
          entropy_coeff: 0.00022205560526344925
          kl: 0.016392061486840248
          model: {}
          policy_loss: -0.18550586700439453
          total_loss: -0.12790384888648987
          vf_explained_var: 0.9936128258705139
          vf_loss: 0.022645803168416023
    num_agent_steps_sampled: 527310
    num_agent_steps_trained: 527310
    num_steps_sampled: 527310
    num_steps_trained: 527310
  iterations_since_restore: 279
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.575065274151434
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09981746201850192
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2726.19082421277
    mean_inference_ms: 2.3713543046465397
    mean_raw_obs_processing_ms: 215.61005891614178
  time_since_restore: 331569.9509937763
  time_this_iter_s: 550.6817100048065
  time_total_s: 331569.9509937763
  timers:
    learn_throughput: 707.102
    learn_time_ms: 2672.881
    load_throughput: 174339.885
    load_time_ms: 10.841
    sample_throughput: 1.893
    sample_time_ms: 998524.512
    update_time_ms: 28.461
  timestamp: 1632340878
  timesteps_since_restore: 0
  timesteps_total: 527310
  training_iteration: 279
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    279 |           331570 | 527310 |    4.949 |              7.89297 |               2.2432 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 529200
  custom_metrics: {}
  date: 2021-09-22_13-12-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.772872790401346
  episode_reward_mean: 4.961761664389821
  episode_reward_min: 2.1878177122943683
  episodes_this_iter: 270
  episodes_total: 75600
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.69900005555246e-05
          entropy: 10.790238380432129
          entropy_coeff: 0.00022105580137576908
          kl: 0.013926736079156399
          model: {}
          policy_loss: -0.17104680836200714
          total_loss: -0.11847764253616333
          vf_explained_var: 0.9935278296470642
          vf_loss: 0.02322755753993988
    num_agent_steps_sampled: 529200
    num_agent_steps_trained: 529200
    num_steps_sampled: 529200
    num_steps_trained: 529200
  iterations_since_restore: 280
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.23307692307692
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09982245598562535
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2724.505876374413
    mean_inference_ms: 2.371493346772183
    mean_raw_obs_processing_ms: 215.57727719251392
  time_since_restore: 332224.2808403969
  time_this_iter_s: 654.3298466205597
  time_total_s: 332224.2808403969
  timers:
    learn_throughput: 706.267
    learn_time_ms: 2676.042
    load_throughput: 175498.553
    load_time_ms: 10.769
    sample_throughput: 1.88
    sample_time_ms: 1005319.901
    update_time_ms: 28.641
  timestamp: 1632341533
  timesteps_since_restore: 0
  timesteps_total: 529200
  training_iteration: 280
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    280 |           332224 | 529200 |  4.96176 |              7.77287 |              2.18782 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 531090
  custom_metrics: {}
  date: 2021-09-22_13-19-49
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.777353656549384
  episode_reward_mean: 4.9836261658624945
  episode_reward_min: 1.6317086222130692
  episodes_this_iter: 270
  episodes_total: 75870
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.6799999836366624e-05
          entropy: 10.66813850402832
          entropy_coeff: 0.0002200559974880889
          kl: 0.01405051164329052
          model: {}
          policy_loss: -0.1889953911304474
          total_loss: -0.1388138085603714
          vf_explained_var: 0.994171142578125
          vf_loss: 0.020520372316241264
    num_agent_steps_sampled: 531090
    num_agent_steps_trained: 531090
    num_steps_sampled: 531090
    num_steps_trained: 531090
  iterations_since_restore: 281
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.70740157480315
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998241969055667
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2720.7141048114177
    mean_inference_ms: 2.371327678334478
    mean_raw_obs_processing_ms: 215.53891344504433
  time_since_restore: 332680.5953886509
  time_this_iter_s: 456.31454825401306
  time_total_s: 332680.5953886509
  timers:
    learn_throughput: 706.864
    learn_time_ms: 2673.781
    load_throughput: 176370.453
    load_time_ms: 10.716
    sample_throughput: 2.188
    sample_time_ms: 863843.75
    update_time_ms: 28.669
  timestamp: 1632341989
  timesteps_since_restore: 0
  timesteps_total: 531090
  training_iteration: 281
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    281 |           332681 | 531090 |  4.98363 |              7.77735 |              1.63171 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 532980
  custom_metrics: {}
  date: 2021-09-22_13-29-34
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.611563357806142
  episode_reward_mean: 4.909962230128435
  episode_reward_min: 2.0050732984020563
  episodes_this_iter: 270
  episodes_total: 76140
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.6609999117208645e-05
          entropy: 10.727845191955566
          entropy_coeff: 0.00021905619360040873
          kl: 0.015053213573992252
          model: {}
          policy_loss: -0.19049474596977234
          total_loss: -0.13665787875652313
          vf_explained_var: 0.9937804341316223
          vf_loss: 0.02189374901354313
    num_agent_steps_sampled: 532980
    num_agent_steps_trained: 532980
    num_steps_sampled: 532980
    num_steps_trained: 532980
  iterations_since_restore: 282
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.77967980295566
    ram_util_percent: 8.758374384236454
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998222851278513
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2716.2466433148807
    mean_inference_ms: 2.3711530148172977
    mean_raw_obs_processing_ms: 215.49600431727532
  time_since_restore: 333265.5430636406
  time_this_iter_s: 584.9476749897003
  time_total_s: 333265.5430636406
  timers:
    learn_throughput: 707.47
    learn_time_ms: 2671.491
    load_throughput: 175808.76
    load_time_ms: 10.75
    sample_throughput: 2.232
    sample_time_ms: 846708.045
    update_time_ms: 28.712
  timestamp: 1632342574
  timesteps_since_restore: 0
  timesteps_total: 532980
  training_iteration: 282
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    282 |           333266 | 532980 |  4.90996 |              7.61156 |              2.00507 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 534870
  custom_metrics: {}
  date: 2021-09-22_13-43-23
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.130404576462658
  episode_reward_mean: 5.00562247737013
  episode_reward_min: 1.728339832511994
  episodes_this_iter: 270
  episodes_total: 76410
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.6419998398050666e-05
          entropy: 10.729121208190918
          entropy_coeff: 0.0002180564042646438
          kl: 0.014459507539868355
          model: {}
          policy_loss: -0.184752956032753
          total_loss: -0.13090160489082336
          vf_explained_var: 0.9934689402580261
          vf_loss: 0.023250339552760124
    num_agent_steps_sampled: 534870
    num_agent_steps_trained: 534870
    num_steps_sampled: 534870
    num_steps_trained: 534870
  iterations_since_restore: 283
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.03993055555556
    ram_util_percent: 8.970833333333333
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09981787568717301
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2712.7488911732885
    mean_inference_ms: 2.3707021288884644
    mean_raw_obs_processing_ms: 215.45762728800736
  time_since_restore: 334094.38286709785
  time_this_iter_s: 828.8398034572601
  time_total_s: 334094.38286709785
  timers:
    learn_throughput: 707.56
    learn_time_ms: 2671.15
    load_throughput: 175158.472
    load_time_ms: 10.79
    sample_throughput: 2.375
    sample_time_ms: 795784.852
    update_time_ms: 28.837
  timestamp: 1632343403
  timesteps_since_restore: 0
  timesteps_total: 534870
  training_iteration: 283
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    283 |           334094 | 534870 |  5.00562 |               8.1304 |              1.72834 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 536760
  custom_metrics: {}
  date: 2021-09-22_13-53-39
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.952258410512914
  episode_reward_mean: 4.945176584594908
  episode_reward_min: 1.412183901330329
  episodes_this_iter: 270
  episodes_total: 76680
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.6230001316871494e-05
          entropy: 10.864800453186035
          entropy_coeff: 0.00021705660037696362
          kl: 0.014333699829876423
          model: {}
          policy_loss: -0.19643162190914154
          total_loss: -0.1411893218755722
          vf_explained_var: 0.9928425550460815
          vf_loss: 0.024946603924036026
    num_agent_steps_sampled: 536760
    num_agent_steps_trained: 536760
    num_steps_sampled: 536760
    num_steps_trained: 536760
  iterations_since_restore: 284
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.146674445740956
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09982785671550558
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2710.7113371665496
    mean_inference_ms: 2.370610988175965
    mean_raw_obs_processing_ms: 215.41620282102187
  time_since_restore: 334710.9080648422
  time_this_iter_s: 616.5251977443695
  time_total_s: 334710.9080648422
  timers:
    learn_throughput: 706.948
    learn_time_ms: 2673.463
    load_throughput: 174383.605
    load_time_ms: 10.838
    sample_throughput: 2.279
    sample_time_ms: 829157.766
    update_time_ms: 28.905
  timestamp: 1632344019
  timesteps_since_restore: 0
  timesteps_total: 536760
  training_iteration: 284
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    284 |           334711 | 536760 |  4.94518 |              7.95226 |              1.41218 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 538650
  custom_metrics: {}
  date: 2021-09-22_14-09-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.951893434785535
  episode_reward_mean: 4.908259576512394
  episode_reward_min: 1.8880035536484197
  episodes_this_iter: 270
  episodes_total: 76950
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.6040000597713515e-05
          entropy: 10.685494422912598
          entropy_coeff: 0.00021605679648928344
          kl: 0.014067347161471844
          model: {}
          policy_loss: -0.17935697734355927
          total_loss: -0.12748855352401733
          vf_explained_var: 0.9937080144882202
          vf_loss: 0.02212993986904621
    num_agent_steps_sampled: 538650
    num_agent_steps_trained: 538650
    num_steps_sampled: 538650
    num_steps_trained: 538650
  iterations_since_restore: 285
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.423252279635264
    ram_util_percent: 8.86724924012158
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09983073532571925
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2709.728680893182
    mean_inference_ms: 2.370611018995219
    mean_raw_obs_processing_ms: 215.37704901833
  time_since_restore: 335657.65316295624
  time_this_iter_s: 946.7450981140137
  time_total_s: 335657.65316295624
  timers:
    learn_throughput: 707.013
    learn_time_ms: 2673.219
    load_throughput: 173408.96
    load_time_ms: 10.899
    sample_throughput: 2.792
    sample_time_ms: 676891.693
    update_time_ms: 28.761
  timestamp: 1632344966
  timesteps_since_restore: 0
  timesteps_total: 538650
  training_iteration: 285
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    285 |           335658 | 538650 |  4.90826 |              7.95189 |                1.888 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 540540
  custom_metrics: {}
  date: 2021-09-22_14-18-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.960138326669907
  episode_reward_mean: 4.990027808930301
  episode_reward_min: 2.3863482524028403
  episodes_this_iter: 270
  episodes_total: 77220
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.5849999878555536e-05
          entropy: 10.689140319824219
          entropy_coeff: 0.0002150570071535185
          kl: 0.01719549670815468
          model: {}
          policy_loss: -0.1931113749742508
          total_loss: -0.13451477885246277
          vf_explained_var: 0.9938965439796448
          vf_loss: 0.021721871569752693
    num_agent_steps_sampled: 540540
    num_agent_steps_trained: 540540
    num_steps_sampled: 540540
    num_steps_trained: 540540
  iterations_since_restore: 286
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.40428176795581
    ram_util_percent: 8.408425414364643
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09982286028950853
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2706.4139314186627
    mean_inference_ms: 2.370362459728565
    mean_raw_obs_processing_ms: 215.33263441427914
  time_since_restore: 336179.4283359051
  time_this_iter_s: 521.7751729488373
  time_total_s: 336179.4283359051
  timers:
    learn_throughput: 706.409
    learn_time_ms: 2675.506
    load_throughput: 175946.114
    load_time_ms: 10.742
    sample_throughput: 2.864
    sample_time_ms: 659984.878
    update_time_ms: 28.85
  timestamp: 1632345488
  timesteps_since_restore: 0
  timesteps_total: 540540
  training_iteration: 286
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    286 |           336179 | 540540 |  4.99003 |              7.96014 |              2.38635 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 542430
  custom_metrics: {}
  date: 2021-09-22_14-30-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.876259487427465
  episode_reward_mean: 4.971918593949305
  episode_reward_min: 1.8006032614591694
  episodes_this_iter: 270
  episodes_total: 77490
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.565999915939756e-05
          entropy: 10.631231307983398
          entropy_coeff: 0.00021405720326583833
          kl: 0.014696554280817509
          model: {}
          policy_loss: -0.203804150223732
          total_loss: -0.14930881559848785
          vf_explained_var: 0.9933492541313171
          vf_loss: 0.023290419951081276
    num_agent_steps_sampled: 542430
    num_agent_steps_trained: 542430
    num_steps_sampled: 542430
    num_steps_trained: 542430
  iterations_since_restore: 287
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.20524809160305
    ram_util_percent: 8.995419847328245
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.099821006240065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2704.1350925105935
    mean_inference_ms: 2.370157347547811
    mean_raw_obs_processing_ms: 215.2978268162541
  time_since_restore: 336933.3200662136
  time_this_iter_s: 753.8917303085327
  time_total_s: 336933.3200662136
  timers:
    learn_throughput: 706.629
    learn_time_ms: 2674.669
    load_throughput: 176584.571
    load_time_ms: 10.703
    sample_throughput: 2.995
    sample_time_ms: 631007.089
    update_time_ms: 28.643
  timestamp: 1632346242
  timesteps_since_restore: 0
  timesteps_total: 542430
  training_iteration: 287
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    287 |           336933 | 542430 |  4.97192 |              7.87626 |               1.8006 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 544320
  custom_metrics: {}
  date: 2021-09-22_14-37-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.5816480683331005
  episode_reward_mean: 5.04913890580776
  episode_reward_min: 2.022698617740234
  episodes_this_iter: 270
  episodes_total: 77760
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.546999844023958e-05
          entropy: 10.58485221862793
          entropy_coeff: 0.00021305739937815815
          kl: 0.013773294165730476
          model: {}
          policy_loss: -0.1883179098367691
          total_loss: -0.13992561399936676
          vf_explained_var: 0.9946369528770447
          vf_loss: 0.019270222634077072
    num_agent_steps_sampled: 544320
    num_agent_steps_trained: 544320
    num_steps_sampled: 544320
    num_steps_trained: 544320
  iterations_since_restore: 288
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.923793103448276
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09983414704447295
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2699.5722340613065
    mean_inference_ms: 2.3698930395901803
    mean_raw_obs_processing_ms: 215.2599988576318
  time_since_restore: 337350.82142424583
  time_this_iter_s: 417.50135803222656
  time_total_s: 337350.82142424583
  timers:
    learn_throughput: 709.191
    learn_time_ms: 2665.007
    load_throughput: 177203.908
    load_time_ms: 10.666
    sample_throughput: 2.998
    sample_time_ms: 630318.59
    update_time_ms: 28.827
  timestamp: 1632346660
  timesteps_since_restore: 0
  timesteps_total: 544320
  training_iteration: 288
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    288 |           337351 | 544320 |  5.04914 |              7.58165 |               2.0227 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 546210
  custom_metrics: {}
  date: 2021-09-22_14-52-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.827493891484237
  episode_reward_mean: 5.0034562605285915
  episode_reward_min: 2.15631667786693
  episodes_this_iter: 270
  episodes_total: 78030
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.528000135906041e-05
          entropy: 10.568376541137695
          entropy_coeff: 0.00021205759549047798
          kl: 0.013307160697877407
          model: {}
          policy_loss: -0.19940710067749023
          total_loss: -0.14686386287212372
          vf_explained_var: 0.9930437207221985
          vf_loss: 0.02446896955370903
    num_agent_steps_sampled: 546210
    num_agent_steps_trained: 546210
    num_steps_sampled: 546210
    num_steps_trained: 546210
  iterations_since_restore: 289
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.48368852459016
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09984655762263361
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2700.4014717032364
    mean_inference_ms: 2.369641404987644
    mean_raw_obs_processing_ms: 215.2272872772767
  time_since_restore: 338227.93177604675
  time_this_iter_s: 877.1103518009186
  time_total_s: 338227.93177604675
  timers:
    learn_throughput: 709.887
    learn_time_ms: 2662.394
    load_throughput: 177563.918
    load_time_ms: 10.644
    sample_throughput: 2.851
    sample_time_ms: 662963.5
    update_time_ms: 28.862
  timestamp: 1632347537
  timesteps_since_restore: 0
  timesteps_total: 546210
  training_iteration: 289
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    289 |           338228 | 546210 |  5.00346 |              7.82749 |              2.15632 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 548100
  custom_metrics: {}
  date: 2021-09-22_14-56-58
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.722982187657547
  episode_reward_mean: 4.867091868915627
  episode_reward_min: 2.0254673130919216
  episodes_this_iter: 270
  episodes_total: 78300
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.509000063990243e-05
          entropy: 10.704839706420898
          entropy_coeff: 0.00021105780615471303
          kl: 0.015714772045612335
          model: {}
          policy_loss: -0.1880955547094345
          total_loss: -0.13123895227909088
          vf_explained_var: 0.9930915832519531
          vf_loss: 0.023315710946917534
    num_agent_steps_sampled: 548100
    num_agent_steps_trained: 548100
    num_steps_sampled: 548100
    num_steps_trained: 548100
  iterations_since_restore: 290
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.01662404092072
    ram_util_percent: 8.615089514066495
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09984497765361355
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2695.411661882451
    mean_inference_ms: 2.369584514674942
    mean_raw_obs_processing_ms: 215.19684079659984
  time_since_restore: 338509.5456690788
  time_this_iter_s: 281.613893032074
  time_total_s: 338509.5456690788
  timers:
    learn_throughput: 711.118
    learn_time_ms: 2657.788
    load_throughput: 178086.076
    load_time_ms: 10.613
    sample_throughput: 3.021
    sample_time_ms: 625696.588
    update_time_ms: 28.909
  timestamp: 1632347818
  timesteps_since_restore: 0
  timesteps_total: 548100
  training_iteration: 290
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    290 |           338510 | 548100 |  4.86709 |              7.72298 |              2.02547 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 549990
  custom_metrics: {}
  date: 2021-09-22_15-18-23
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.891367093605674
  episode_reward_mean: 5.001797120670537
  episode_reward_min: 1.8877167075048584
  episodes_this_iter: 270
  episodes_total: 78570
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.489999992074445e-05
          entropy: 10.582369804382324
          entropy_coeff: 0.00021005800226703286
          kl: 0.014617581851780415
          model: {}
          policy_loss: -0.1828831136226654
          total_loss: -0.12855009734630585
          vf_explained_var: 0.9934253692626953
          vf_loss: 0.02325521968305111
    num_agent_steps_sampled: 549990
    num_agent_steps_trained: 549990
    num_steps_sampled: 549990
    num_steps_trained: 549990
  iterations_since_restore: 291
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.47369601794728
    ram_util_percent: 8.74481211441391
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998385498074612
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2694.9141137691495
    mean_inference_ms: 2.3695209159062305
    mean_raw_obs_processing_ms: 215.158786553953
  time_since_restore: 339793.6398656368
  time_this_iter_s: 1284.0941965579987
  time_total_s: 339793.6398656368
  timers:
    learn_throughput: 711.094
    learn_time_ms: 2657.876
    load_throughput: 176024.642
    load_time_ms: 10.737
    sample_throughput: 2.668
    sample_time_ms: 708474.396
    update_time_ms: 29.004
  timestamp: 1632349103
  timesteps_since_restore: 0
  timesteps_total: 549990
  training_iteration: 291
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    291 |           339794 | 549990 |   5.0018 |              7.89137 |              1.88772 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 551880
  custom_metrics: {}
  date: 2021-09-22_15-44-35
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.679794608585862
  episode_reward_mean: 4.92585825924979
  episode_reward_min: 1.7802433426173132
  episodes_this_iter: 270
  episodes_total: 78840
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.470999920158647e-05
          entropy: 10.639692306518555
          entropy_coeff: 0.0002090581983793527
          kl: 0.014103059656918049
          model: {}
          policy_loss: -0.1827496439218521
          total_loss: -0.1299765557050705
          vf_explained_var: 0.9934813380241394
          vf_loss: 0.022868884727358818
    num_agent_steps_sampled: 551880
    num_agent_steps_trained: 551880
    num_steps_sampled: 551880
    num_steps_trained: 551880
  iterations_since_restore: 292
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.55811614083219
    ram_util_percent: 8.96872427983539
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985344123351414
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2697.273021414512
    mean_inference_ms: 2.3694415242037303
    mean_raw_obs_processing_ms: 215.12441455286188
  time_since_restore: 341366.4176132679
  time_this_iter_s: 1572.777747631073
  time_total_s: 341366.4176132679
  timers:
    learn_throughput: 710.595
    learn_time_ms: 2659.742
    load_throughput: 175802.522
    load_time_ms: 10.751
    sample_throughput: 2.341
    sample_time_ms: 807255.351
    update_time_ms: 28.865
  timestamp: 1632350675
  timesteps_since_restore: 0
  timesteps_total: 551880
  training_iteration: 292
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    292 |           341366 | 551880 |  4.92586 |              7.67979 |              1.78024 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 553770
  custom_metrics: {}
  date: 2021-09-22_15-53-01
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.915057778624183
  episode_reward_mean: 4.981644716118524
  episode_reward_min: 1.9476921516760166
  episodes_this_iter: 270
  episodes_total: 79110
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.451999848242849e-05
          entropy: 10.584436416625977
          entropy_coeff: 0.00020805839449167252
          kl: 0.013718843460083008
          model: {}
          policy_loss: -0.18142211437225342
          total_loss: -0.1275157630443573
          vf_explained_var: 0.9928820729255676
          vf_loss: 0.024855295196175575
    num_agent_steps_sampled: 553770
    num_agent_steps_trained: 553770
    num_steps_sampled: 553770
    num_steps_trained: 553770
  iterations_since_restore: 293
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.184022824536378
    ram_util_percent: 8.4
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986213310260225
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2693.91008212168
    mean_inference_ms: 2.369441022432248
    mean_raw_obs_processing_ms: 215.1136529567138
  time_since_restore: 341871.8403570652
  time_this_iter_s: 505.42274379730225
  time_total_s: 341871.8403570652
  timers:
    learn_throughput: 711.652
    learn_time_ms: 2655.791
    load_throughput: 175432.916
    load_time_ms: 10.773
    sample_throughput: 2.439
    sample_time_ms: 774917.631
    update_time_ms: 28.757
  timestamp: 1632351181
  timesteps_since_restore: 0
  timesteps_total: 553770
  training_iteration: 293
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    293 |           341872 | 553770 |  4.98164 |              7.91506 |              1.94769 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 555660
  custom_metrics: {}
  date: 2021-09-22_15-58-00
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.65071725463456
  episode_reward_mean: 4.863842645030199
  episode_reward_min: 2.343547163941052
  episodes_this_iter: 270
  episodes_total: 79380
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.433000140124932e-05
          entropy: 10.701668739318848
          entropy_coeff: 0.00020705860515590757
          kl: 0.013320289552211761
          model: {}
          policy_loss: -0.18623152375221252
          total_loss: -0.1353919804096222
          vf_explained_var: 0.9932361841201782
          vf_loss: 0.022710135206580162
    num_agent_steps_sampled: 555660
    num_agent_steps_trained: 555660
    num_steps_sampled: 555660
    num_steps_trained: 555660
  iterations_since_restore: 294
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.85397590361446
    ram_util_percent: 8.872048192771084
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998633798770779
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2689.245809878702
    mean_inference_ms: 2.3693710470389617
    mean_raw_obs_processing_ms: 215.07678408083913
  time_since_restore: 342170.83113861084
  time_this_iter_s: 298.99078154563904
  time_total_s: 342170.83113861084
  timers:
    learn_throughput: 712.217
    learn_time_ms: 2653.687
    load_throughput: 162673.341
    load_time_ms: 11.618
    sample_throughput: 2.543
    sample_time_ms: 743165.744
    update_time_ms: 28.625
  timestamp: 1632351480
  timesteps_since_restore: 0
  timesteps_total: 555660
  training_iteration: 294
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    294 |           342171 | 555660 |  4.86384 |              7.65072 |              2.34355 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 557550
  custom_metrics: {}
  date: 2021-09-22_16-04-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.123852684872832
  episode_reward_mean: 4.8735703903787995
  episode_reward_min: 2.0686406757155997
  episodes_this_iter: 270
  episodes_total: 79650
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.414000068209134e-05
          entropy: 10.629548072814941
          entropy_coeff: 0.0002060588012682274
          kl: 0.014526993967592716
          model: {}
          policy_loss: -0.18020670115947723
          total_loss: -0.12790314853191376
          vf_explained_var: 0.993708610534668
          vf_loss: 0.021399537101387978
    num_agent_steps_sampled: 557550
    num_agent_steps_trained: 557550
    num_steps_sampled: 557550
    num_steps_trained: 557550
  iterations_since_restore: 295
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.93222003929273
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986232201901751
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2685.190774769905
    mean_inference_ms: 2.3692598722272122
    mean_raw_obs_processing_ms: 215.03901109049286
  time_since_restore: 342536.68702697754
  time_this_iter_s: 365.8558883666992
  time_total_s: 342536.68702697754
  timers:
    learn_throughput: 711.956
    learn_time_ms: 2654.659
    load_throughput: 162973.999
    load_time_ms: 11.597
    sample_throughput: 2.759
    sample_time_ms: 685075.876
    update_time_ms: 28.679
  timestamp: 1632351846
  timesteps_since_restore: 0
  timesteps_total: 557550
  training_iteration: 295
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    295 |           342537 | 557550 |  4.87357 |              8.12385 |              2.06864 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 559440
  custom_metrics: {}
  date: 2021-09-22_16-12-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.894370913763262
  episode_reward_mean: 4.997887245920625
  episode_reward_min: 2.4939673888400278
  episodes_this_iter: 270
  episodes_total: 79920
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.394999996293336e-05
          entropy: 10.55107307434082
          entropy_coeff: 0.00020505899738054723
          kl: 0.0142104122787714
          model: {}
          policy_loss: -0.1902892142534256
          total_loss: -0.13941247761249542
          vf_explained_var: 0.9941288828849792
          vf_loss: 0.02066723443567753
    num_agent_steps_sampled: 559440
    num_agent_steps_trained: 559440
    num_steps_sampled: 559440
    num_steps_trained: 559440
  iterations_since_restore: 296
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.08870967741935
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09986174850624069
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2681.4968121640304
    mean_inference_ms: 2.369550698053708
    mean_raw_obs_processing_ms: 215.0050556447196
  time_since_restore: 343026.9523844719
  time_this_iter_s: 490.26535749435425
  time_total_s: 343026.9523844719
  timers:
    learn_throughput: 712.793
    learn_time_ms: 2651.543
    load_throughput: 162610.94
    load_time_ms: 11.623
    sample_throughput: 2.772
    sample_time_ms: 681928.66
    update_time_ms: 28.458
  timestamp: 1632352336
  timesteps_since_restore: 0
  timesteps_total: 559440
  training_iteration: 296
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    296 |           343027 | 559440 |  4.99789 |              7.89437 |              2.49397 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 561330
  custom_metrics: {}
  date: 2021-09-22_16-32-18
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.319088021453558
  episode_reward_mean: 4.842530956734629
  episode_reward_min: 1.3584782617043396
  episodes_this_iter: 270
  episodes_total: 80190
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.375999924377538e-05
          entropy: 10.56518268585205
          entropy_coeff: 0.00020405919349286705
          kl: 0.014486841857433319
          model: {}
          policy_loss: -0.17559553682804108
          total_loss: -0.12254232168197632
          vf_explained_var: 0.9935539364814758
          vf_loss: 0.022206300869584084
    num_agent_steps_sampled: 561330
    num_agent_steps_trained: 561330
    num_steps_sampled: 561330
    num_steps_trained: 561330
  iterations_since_restore: 297
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.2934171154997
    ram_util_percent: 8.988809096349492
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985773123015641
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2680.329221898359
    mean_inference_ms: 2.369629798853083
    mean_raw_obs_processing_ms: 214.97343775053898
  time_since_restore: 344229.0658171177
  time_this_iter_s: 1202.1134326457977
  time_total_s: 344229.0658171177
  timers:
    learn_throughput: 714.037
    learn_time_ms: 2646.922
    load_throughput: 162845.105
    load_time_ms: 11.606
    sample_throughput: 2.601
    sample_time_ms: 726755.018
    update_time_ms: 28.493
  timestamp: 1632353538
  timesteps_since_restore: 0
  timesteps_total: 561330
  training_iteration: 297
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    297 |           344229 | 561330 |  4.84253 |              8.31909 |              1.35848 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 563220
  custom_metrics: {}
  date: 2021-09-22_16-35-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.973979601461018
  episode_reward_mean: 4.9378591925642565
  episode_reward_min: 2.5105711758912306
  episodes_this_iter: 270
  episodes_total: 80460
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.3569998524617404e-05
          entropy: 10.466019630432129
          entropy_coeff: 0.0002030594041571021
          kl: 0.014980314299464226
          model: {}
          policy_loss: -0.20753447711467743
          total_loss: -0.15454590320587158
          vf_explained_var: 0.9939489364624023
          vf_loss: 0.020986750721931458
    num_agent_steps_sampled: 563220
    num_agent_steps_trained: 563220
    num_steps_sampled: 563220
    num_steps_trained: 563220
  iterations_since_restore: 298
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.482456140350877
    ram_util_percent: 8.4
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09985013725927498
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2674.853646744109
    mean_inference_ms: 2.369693399894465
    mean_raw_obs_processing_ms: 214.94440703404683
  time_since_restore: 344435.1104063988
  time_this_iter_s: 206.04458928108215
  time_total_s: 344435.1104063988
  timers:
    learn_throughput: 715.176
    learn_time_ms: 2642.708
    load_throughput: 163299.65
    load_time_ms: 11.574
    sample_throughput: 2.679
    sample_time_ms: 705613.792
    update_time_ms: 28.295
  timestamp: 1632353744
  timesteps_since_restore: 0
  timesteps_total: 563220
  training_iteration: 298
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 16.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    298 |           344435 | 563220 |  4.93786 |              7.97398 |              2.51057 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 565110
  custom_metrics: {}
  date: 2021-09-22_16-38-28
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.006427739646927
  episode_reward_mean: 4.966804229270677
  episode_reward_min: 1.8698426945482112
  episodes_this_iter: 270
  episodes_total: 80730
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.338000144343823e-05
          entropy: 10.537779808044434
          entropy_coeff: 0.00020205960026942194
          kl: 0.016065295785665512
          model: {}
          policy_loss: -0.1788615733385086
          total_loss: -0.1235806867480278
          vf_explained_var: 0.994232714176178
          vf_loss: 0.020811384543776512
    num_agent_steps_sampled: 565110
    num_agent_steps_trained: 565110
    num_steps_sampled: 565110
    num_steps_trained: 565110
  iterations_since_restore: 299
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.08070175438597
    ram_util_percent: 8.688157894736843
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09984954781546587
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2668.683635196757
    mean_inference_ms: 2.369520145805629
    mean_raw_obs_processing_ms: 214.90494343468674
  time_since_restore: 344598.94593811035
  time_this_iter_s: 163.83553171157837
  time_total_s: 344598.94593811035
  timers:
    learn_throughput: 715.497
    learn_time_ms: 2641.522
    load_throughput: 163699.253
    load_time_ms: 11.546
    sample_throughput: 2.98
    sample_time_ms: 634288.169
    update_time_ms: 28.146
  timestamp: 1632353908
  timesteps_since_restore: 0
  timesteps_total: 565110
  training_iteration: 299
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    299 |           344599 | 565110 |   4.9668 |              8.00643 |              1.86984 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 567000
  custom_metrics: {}
  date: 2021-09-22_16-45-11
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.628388257104969
  episode_reward_mean: 4.869584335278751
  episode_reward_min: 2.1956668858317356
  episodes_this_iter: 270
  episodes_total: 81000
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.319000072428025e-05
          entropy: 10.651822090148926
          entropy_coeff: 0.00020105979638174176
          kl: 0.014051773585379124
          model: {}
          policy_loss: -0.18598869442939758
          total_loss: -0.13260915875434875
          vf_explained_var: 0.9930920004844666
          vf_loss: 0.02350950799882412
    num_agent_steps_sampled: 567000
    num_agent_steps_trained: 567000
    num_steps_sampled: 567000
    num_steps_trained: 567000
  iterations_since_restore: 300
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.370535714285715
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998485595655503
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2664.6189063278525
    mean_inference_ms: 2.3695582408219455
    mean_raw_obs_processing_ms: 214.87371063583868
  time_since_restore: 345001.8926408291
  time_this_iter_s: 402.94670271873474
  time_total_s: 345001.8926408291
  timers:
    learn_throughput: 714.18
    learn_time_ms: 2646.39
    load_throughput: 162653.648
    load_time_ms: 11.62
    sample_throughput: 2.924
    sample_time_ms: 646415.554
    update_time_ms: 28.085
  timestamp: 1632354311
  timesteps_since_restore: 0
  timesteps_total: 567000
  training_iteration: 300
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | RUNNING  | 10.19.5.30:21290 |    300 |           345002 | 567000 |  4.86958 |              7.62839 |              2.19567 |                  7 |
+------------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_a1429_00000:
  agent_timesteps_total: 567000
  custom_metrics: {}
  date: 2021-09-22_16-45-11
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.628388257104969
  episode_reward_mean: 4.869584335278751
  episode_reward_min: 2.1956668858317356
  episodes_this_iter: 270
  episodes_total: 81000
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  experiment_tag: '0'
  hostname: sh03-05n30.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 4.319000072428025e-05
          entropy: 10.651822090148926
          entropy_coeff: 0.00020105979638174176
          kl: 0.014051773585379124
          model: {}
          policy_loss: -0.18598869442939758
          total_loss: -0.13260915875434875
          vf_explained_var: 0.9930920004844666
          vf_loss: 0.02350950799882412
    num_agent_steps_sampled: 567000
    num_agent_steps_trained: 567000
    num_steps_sampled: 567000
    num_steps_trained: 567000
  iterations_since_restore: 300
  node_ip: 10.19.5.30
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.370535714285715
    ram_util_percent: 9.0
  pid: 21290
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0998485595655503
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2664.6189063278525
    mean_inference_ms: 2.3695582408219455
    mean_raw_obs_processing_ms: 214.87371063583868
  time_since_restore: 345001.8926408291
  time_this_iter_s: 402.94670271873474
  time_total_s: 345001.8926408291
  timers:
    learn_throughput: 714.18
    learn_time_ms: 2646.39
    load_throughput: 162653.648
    load_time_ms: 11.62
    sample_throughput: 2.924
    sample_time_ms: 646415.554
    update_time_ms: 28.085
  timestamp: 1632354311
  timesteps_since_restore: 0
  timesteps_total: 567000
  training_iteration: 300
  trial_id: a1429_00000
  
== Status ==
Memory usage on this node: 18.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/140 CPUs, 0/0 GPUs, 0.0/1206.19 GiB heap, 0.0/520.93 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 ERROR)
+------------------------------+----------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc   |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_a1429_00000 | ERROR    |       |    300 |           345002 | 567000 |  4.86958 |              7.62839 |              2.19567 |                  7 |
+------------------------------+----------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
Number of errored trials: 1
+------------------------------+--------------+---------------------------------------------------------------------------------------------+
| Trial name                   |   # failures | error file                                                                                  |
|------------------------------+--------------+---------------------------------------------------------------------------------------------|
| PPO_ReservoirEnv_a1429_00000 |            1 | /scratch/users/nyusuf/logs/PPO/PPO_ReservoirEnv_a1429_00000_0_2021-09-18_16-53-32/error.txt |
+------------------------------+--------------+---------------------------------------------------------------------------------------------+

