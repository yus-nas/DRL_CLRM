IP Head: 10.19.5.33:8479
Starting HEAD at sh03-05n33
Starting WORKER 1 at sh03-05n37
Starting WORKER 2 at sh03-05n54
Starting WORKER 3 at sh03-05n56
Starting WORKER 4 at sh03-05n57
Starting WORKER 5 at sh03-05n62
Starting WORKER 6 at sh03-05n68
== Status ==
Memory usage on this node: 10.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 PENDING)
+------------------------------+----------+-------+
| Trial name                   | status   | loc   |
|------------------------------+----------+-------|
| PPO_ReservoirEnv_20cd0_00000 | PENDING  |       |
+------------------------------+----------+-------+


== Status ==
Memory usage on this node: 10.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 PENDING)
+------------------------------+----------+-------+
| Trial name                   | status   | loc   |
|------------------------------+----------+-------|
| PPO_ReservoirEnv_20cd0_00000 | PENDING  |       |
+------------------------------+----------+-------+


== Status ==
Memory usage on this node: 12.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-------+
| Trial name                   | status   | loc   |
|------------------------------+----------+-------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  |       |
+------------------------------+----------+-------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 568890
  custom_metrics: {}
  date: 2021-09-30_21-04-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6524871449589495
  episode_reward_mean: 4.9925199066903385
  episode_reward_min: 1.6000691251966168
  episodes_this_iter: 270
  episodes_total: 81270
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 0.20000000298023224
          cur_lr: 9.999999747378752e-05
          entropy: 10.501004219055176
          entropy_coeff: 0.0005000000237487257
          kl: 0.05508774518966675
          model: {}
          policy_loss: -0.19681403040885925
          total_loss: -0.17795655131340027
          vf_explained_var: 0.996365487575531
          vf_loss: 0.013090446591377258
    num_agent_steps_sampled: 568890
    num_agent_steps_trained: 568890
    num_steps_sampled: 568890
    num_steps_trained: 568890
  iterations_since_restore: 1
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.788735919899875
    ram_util_percent: 5.790362953692113
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11586813279140142
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1406.388440544223
    mean_inference_ms: 22.416167318085098
    mean_raw_obs_processing_ms: 190.7525533511315
  time_since_restore: 570.9269268512726
  time_this_iter_s: 570.9269268512726
  time_total_s: 345572.81956768036
  timers:
    learn_throughput: 526.96
    learn_time_ms: 3586.612
    load_throughput: 11036.013
    load_time_ms: 171.257
    sample_throughput: 3.335
    sample_time_ms: 566686.776
    update_time_ms: 27.274
  timestamp: 1633061067
  timesteps_since_restore: 0
  timesteps_total: 568890
  training_iteration: 301
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    301 |           345573 | 568890 |  4.99252 |              7.65249 |              1.60007 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 570780
  custom_metrics: {}
  date: 2021-09-30_21-12-05
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.936337948156727
  episode_reward_mean: 5.005730769059796
  episode_reward_min: 2.183593736938475
  episodes_this_iter: 270
  episodes_total: 81540
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 0.30000001192092896
          cur_lr: 4.2809999285964295e-05
          entropy: 10.543068885803223
          entropy_coeff: 0.00019906020315829664
          kl: 0.037014272063970566
          model: {}
          policy_loss: -0.18625524640083313
          total_loss: -0.15716008841991425
          vf_explained_var: 0.994432270526886
          vf_loss: 0.020089562982320786
    num_agent_steps_sampled: 570780
    num_agent_steps_trained: 570780
    num_steps_sampled: 570780
    num_steps_trained: 570780
  iterations_since_restore: 2
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.82112676056338
    ram_util_percent: 5.799999999999998
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11326325807534873
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1960.7068401034614
    mean_inference_ms: 12.735317980497125
    mean_raw_obs_processing_ms: 197.43898441660605
  time_since_restore: 1029.5585939884186
  time_this_iter_s: 458.631667137146
  time_total_s: 346031.4512348175
  timers:
    learn_throughput: 614.465
    learn_time_ms: 3075.847
    load_throughput: 20764.779
    load_time_ms: 91.02
    sample_throughput: 3.697
    sample_time_ms: 511292.755
    update_time_ms: 27.414
  timestamp: 1633061525
  timesteps_since_restore: 0
  timesteps_total: 570780
  training_iteration: 302
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    302 |           346031 | 570780 |  5.00573 |              7.93634 |              2.18359 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 572670
  custom_metrics: {}
  date: 2021-09-30_21-18-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.612631354687059
  episode_reward_mean: 5.001144878449217
  episode_reward_min: 2.098185259050908
  episodes_this_iter: 270
  episodes_total: 81810
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 0.44999998807907104
          cur_lr: 4.2619998566806316e-05
          entropy: 10.411267280578613
          entropy_coeff: 0.00019806039927061647
          kl: 0.03299278765916824
          model: {}
          policy_loss: -0.20993134379386902
          total_loss: -0.17612521350383759
          vf_explained_var: 0.9941068291664124
          vf_loss: 0.02102142758667469
    num_agent_steps_sampled: 572670
    num_agent_steps_trained: 572670
    num_steps_sampled: 572670
    num_steps_trained: 572670
  iterations_since_restore: 3
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 11.648679245283018
    ram_util_percent: 5.897547169811321
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1100301126485031
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1923.2171993025613
    mean_inference_ms: 9.373988949153878
    mean_raw_obs_processing_ms: 199.85437487652342
  time_since_restore: 1409.981556892395
  time_this_iter_s: 380.42296290397644
  time_total_s: 346411.8741977215
  timers:
    learn_throughput: 649.276
    learn_time_ms: 2910.936
    load_throughput: 29408.416
    load_time_ms: 64.267
    sample_throughput: 4.049
    sample_time_ms: 466752.34
    update_time_ms: 27.282
  timestamp: 1633061906
  timesteps_since_restore: 0
  timesteps_total: 572670
  training_iteration: 303
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    303 |           346412 | 572670 |  5.00114 |              7.61263 |              2.09819 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 574560
  custom_metrics: {}
  date: 2021-09-30_21-28-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.837251193321898
  episode_reward_mean: 4.9340008823206345
  episode_reward_min: 2.3115168810092355
  episodes_this_iter: 270
  episodes_total: 82080
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 0.675000011920929
          cur_lr: 4.2430001485627145e-05
          entropy: 10.479978561401367
          entropy_coeff: 0.0001970605953829363
          kl: 0.025458112359046936
          model: {}
          policy_loss: -0.19197306036949158
          total_loss: -0.15844972431659698
          vf_explained_var: 0.994657576084137
          vf_loss: 0.018404293805360794
    num_agent_steps_sampled: 574560
    num_agent_steps_trained: 574560
    num_steps_sampled: 574560
    num_steps_trained: 574560
  iterations_since_restore: 4
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 9.674069627851141
    ram_util_percent: 6.165306122448979
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10787494156250636
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1902.4608041962842
    mean_inference_ms: 7.6401864808437665
    mean_raw_obs_processing_ms: 201.09280291279083
  time_since_restore: 2007.9751057624817
  time_this_iter_s: 597.9935488700867
  time_total_s: 347009.86774659157
  timers:
    learn_throughput: 670.436
    learn_time_ms: 2819.06
    load_throughput: 37158.273
    load_time_ms: 50.864
    sample_throughput: 3.788
    sample_time_ms: 498884.944
    update_time_ms: 27.196
  timestamp: 1633062504
  timesteps_since_restore: 0
  timesteps_total: 574560
  training_iteration: 304
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    304 |           347010 | 574560 |    4.934 |              7.83725 |              2.31152 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 576450
  custom_metrics: {}
  date: 2021-09-30_22-29-34
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.574548851343419
  episode_reward_mean: 4.9615603712732534
  episode_reward_min: 2.122200298466755
  episodes_this_iter: 270
  episodes_total: 82350
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.0125000476837158
          cur_lr: 4.2240000766469166e-05
          entropy: 10.433709144592285
          entropy_coeff: 0.00019606080604717135
          kl: 0.021897995844483376
          model: {}
          policy_loss: -0.17852891981601715
          total_loss: -0.13610287010669708
          vf_explained_var: 0.9934543967247009
          vf_loss: 0.022299977019429207
    num_agent_steps_sampled: 576450
    num_agent_steps_trained: 576450
    num_steps_sampled: 576450
    num_steps_trained: 576450
  iterations_since_restore: 5
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.351934349355215
    ram_util_percent: 6.650801094177412
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10773196591022553
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2238.491222842261
    mean_inference_ms: 6.58468312127676
    mean_raw_obs_processing_ms: 201.60689378817005
  time_since_restore: 5678.376842260361
  time_this_iter_s: 3670.401736497879
  time_total_s: 350680.26948308945
  timers:
    learn_throughput: 682.593
    learn_time_ms: 2768.853
    load_throughput: 44360.028
    load_time_ms: 42.606
    sample_throughput: 1.669
    sample_time_ms: 1132640.064
    update_time_ms: 27.502
  timestamp: 1633066174
  timesteps_since_restore: 0
  timesteps_total: 576450
  training_iteration: 305
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    305 |           350680 | 576450 |  4.96156 |              7.57455 |               2.1222 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 578340
  custom_metrics: {}
  date: 2021-09-30_22-33-41
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.839603511362284
  episode_reward_mean: 4.987998804427507
  episode_reward_min: 2.162999347493912
  episodes_this_iter: 270
  episodes_total: 82620
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.205000004731119e-05
          entropy: 10.391512870788574
          entropy_coeff: 0.00019506100215949118
          kl: 0.019670793786644936
          model: {}
          policy_loss: -0.17977407574653625
          total_loss: -0.13019675016403198
          vf_explained_var: 0.9940246343612671
          vf_loss: 0.021729281172156334
    num_agent_steps_sampled: 578340
    num_agent_steps_trained: 578340
    num_steps_sampled: 578340
    num_steps_trained: 578340
  iterations_since_restore: 6
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.21627906976744
    ram_util_percent: 6.877906976744186
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10717873999236197
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2091.937306119466
    mean_inference_ms: 5.872952122574019
    mean_raw_obs_processing_ms: 202.1830292412918
  time_since_restore: 5925.273168325424
  time_this_iter_s: 246.89632606506348
  time_total_s: 350927.1658091545
  timers:
    learn_throughput: 691.431
    learn_time_ms: 2733.461
    load_throughput: 50380.482
    load_time_ms: 37.515
    sample_throughput: 1.92
    sample_time_ms: 984560.955
    update_time_ms: 27.733
  timestamp: 1633066421
  timesteps_since_restore: 0
  timesteps_total: 578340
  training_iteration: 306
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    306 |           350927 | 578340 |    4.988 |               7.8396 |                2.163 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 580230
  custom_metrics: {}
  date: 2021-09-30_22-47-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.798815892320675
  episode_reward_mean: 4.892904612976163
  episode_reward_min: 1.4144613191150948
  episodes_this_iter: 270
  episodes_total: 82890
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.185999932815321e-05
          entropy: 10.464760780334473
          entropy_coeff: 0.000194061198271811
          kl: 0.018019666895270348
          model: {}
          policy_loss: -0.18426108360290527
          total_loss: -0.13998891413211823
          vf_explained_var: 0.9945185780525208
          vf_loss: 0.018935607746243477
    num_agent_steps_sampled: 580230
    num_agent_steps_trained: 580230
    num_steps_sampled: 580230
    num_steps_trained: 580230
  iterations_since_restore: 7
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 15.52260334212841
    ram_util_percent: 6.659542656112577
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10651043636095107
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2119.1569653870324
    mean_inference_ms: 5.377317411463402
    mean_raw_obs_processing_ms: 202.5940527061465
  time_since_restore: 6741.221943616867
  time_this_iter_s: 815.9487752914429
  time_total_s: 351743.11458444595
  timers:
    learn_throughput: 698.234
    learn_time_ms: 2706.829
    load_throughput: 56097.675
    load_time_ms: 33.691
    sample_throughput: 1.969
    sample_time_ms: 960085.201
    update_time_ms: 27.567
  timestamp: 1633067237
  timesteps_since_restore: 0
  timesteps_total: 580230
  training_iteration: 307
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    307 |           351743 | 580230 |   4.8929 |              7.79882 |              1.41446 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 582120
  custom_metrics: {}
  date: 2021-09-30_22-54-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.653738690904394
  episode_reward_mean: 4.933623560501547
  episode_reward_min: 1.967051899053428
  episodes_this_iter: 270
  episodes_total: 83160
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.166999860899523e-05
          entropy: 10.405831336975098
          entropy_coeff: 0.00019306139438413084
          kl: 0.018060261383652687
          model: {}
          policy_loss: -0.18116667866706848
          total_loss: -0.13342823088169098
          vf_explained_var: 0.993560791015625
          vf_loss: 0.0223184023052454
    num_agent_steps_sampled: 582120
    num_agent_steps_trained: 582120
    num_steps_sampled: 582120
    num_steps_trained: 582120
  iterations_since_restore: 8
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 16.28729096989967
    ram_util_percent: 6.611036789297658
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10616720719088964
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2034.903206476732
    mean_inference_ms: 5.009346874140786
    mean_raw_obs_processing_ms: 202.8490605099558
  time_since_restore: 7170.514376878738
  time_this_iter_s: 429.29243326187134
  time_total_s: 352172.4070177078
  timers:
    learn_throughput: 702.892
    learn_time_ms: 2688.889
    load_throughput: 61377.801
    load_time_ms: 30.793
    sample_throughput: 2.116
    sample_time_ms: 893394.894
    update_time_ms: 27.582
  timestamp: 1633067666
  timesteps_since_restore: 0
  timesteps_total: 582120
  training_iteration: 308
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    308 |           352172 | 582120 |  4.93362 |              7.65374 |              1.96705 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 584010
  custom_metrics: {}
  date: 2021-09-30_22-56-54
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.881589814885626
  episode_reward_mean: 5.074428160715504
  episode_reward_min: 2.4055180849272078
  episodes_this_iter: 270
  episodes_total: 83430
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.148000152781606e-05
          entropy: 10.287359237670898
          entropy_coeff: 0.0001920616050483659
          kl: 0.017897775396704674
          model: {}
          policy_loss: -0.20092861354351044
          total_loss: -0.15442100167274475
          vf_explained_var: 0.9942449927330017
          vf_loss: 0.021301209926605225
    num_agent_steps_sampled: 584010
    num_agent_steps_trained: 584010
    num_steps_sampled: 584010
    num_steps_trained: 584010
  iterations_since_restore: 9
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.211274509803922
    ram_util_percent: 6.534803921568627
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10614728885894269
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1933.083066929183
    mean_inference_ms: 4.714028304782266
    mean_raw_obs_processing_ms: 203.00168176196556
  time_since_restore: 7317.675635576248
  time_this_iter_s: 147.16125869750977
  time_total_s: 352319.56827640533
  timers:
    learn_throughput: 707.062
    learn_time_ms: 2673.034
    load_throughput: 66349.708
    load_time_ms: 28.485
    sample_throughput: 2.333
    sample_time_ms: 810178.696
    update_time_ms: 27.423
  timestamp: 1633067814
  timesteps_since_restore: 0
  timesteps_total: 584010
  training_iteration: 309
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 23.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    309 |           352320 | 584010 |  5.07443 |              7.88159 |              2.40552 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 585900
  custom_metrics: {}
  date: 2021-09-30_23-19-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.940202293018801
  episode_reward_mean: 4.9173120751401775
  episode_reward_min: 2.2861495388587985
  episodes_this_iter: 270
  episodes_total: 83700
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.129000080865808e-05
          entropy: 10.390798568725586
          entropy_coeff: 0.00019106180116068572
          kl: 0.01648229919373989
          model: {}
          policy_loss: -0.1724812239408493
          total_loss: -0.12901028990745544
          vf_explained_var: 0.9939869046211243
          vf_loss: 0.020423734560608864
    num_agent_steps_sampled: 585900
    num_agent_steps_trained: 585900
    num_steps_sampled: 585900
    num_steps_trained: 585900
  iterations_since_restore: 10
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 15.642465753424654
    ram_util_percent: 8.269757639620652
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10586920954406433
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 2010.3750475253962
    mean_inference_ms: 4.4750114422881575
    mean_raw_obs_processing_ms: 203.09660556844315
  time_since_restore: 8679.655596971512
  time_this_iter_s: 1361.9799613952637
  time_total_s: 353681.5482378006
  timers:
    learn_throughput: 710.957
    learn_time_ms: 2658.387
    load_throughput: 70743.319
    load_time_ms: 26.716
    sample_throughput: 2.185
    sample_time_ms: 865088.904
    update_time_ms: 27.485
  timestamp: 1633069176
  timesteps_since_restore: 0
  timesteps_total: 585900
  training_iteration: 310
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    310 |           353682 | 585900 |  4.91731 |               7.9402 |              2.28615 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 587790
  custom_metrics: {}
  date: 2021-09-30_23-22-01
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.552407049137606
  episode_reward_mean: 4.9179623612482155
  episode_reward_min: 2.0598561628776286
  episodes_this_iter: 270
  episodes_total: 83970
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.11000000895001e-05
          entropy: 10.35802173614502
          entropy_coeff: 0.00019006199727300555
          kl: 0.019549259915947914
          model: {}
          policy_loss: -0.18930940330028534
          total_loss: -0.14138755202293396
          vf_explained_var: 0.9941414594650269
          vf_loss: 0.020200124010443687
    num_agent_steps_sampled: 587790
    num_agent_steps_trained: 587790
    num_steps_sampled: 587790
    num_steps_trained: 587790
  iterations_since_restore: 11
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.262376237623762
    ram_util_percent: 10.112376237623762
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10558371048247087
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1911.4088632727185
    mean_inference_ms: 4.276320219324766
    mean_raw_obs_processing_ms: 203.04943226001882
  time_since_restore: 8824.484816551208
  time_this_iter_s: 144.82921957969666
  time_total_s: 353826.3774573803
  timers:
    learn_throughput: 739.893
    learn_time_ms: 2554.422
    load_throughput: 178641.503
    load_time_ms: 10.58
    sample_throughput: 2.298
    sample_time_ms: 822631.221
    update_time_ms: 27.339
  timestamp: 1633069321
  timesteps_since_restore: 0
  timesteps_total: 587790
  training_iteration: 311
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 27.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    311 |           353826 | 587790 |  4.91796 |              7.55241 |              2.05986 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 589680
  custom_metrics: {}
  date: 2021-09-30_23-26-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.186452632198387
  episode_reward_mean: 4.904291854407205
  episode_reward_min: 1.9135008389491752
  episodes_this_iter: 270
  episodes_total: 84240
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.090999937034212e-05
          entropy: 10.363248825073242
          entropy_coeff: 0.00018906219338532537
          kl: 0.016260391101241112
          model: {}
          policy_loss: -0.1732853800058365
          total_loss: -0.128780335187912
          vf_explained_var: 0.9937628507614136
          vf_loss: 0.021768851205706596
    num_agent_steps_sampled: 589680
    num_agent_steps_trained: 589680
    num_steps_sampled: 589680
    num_steps_trained: 589680
  iterations_since_restore: 12
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 18.079539641943732
    ram_util_percent: 10.364450127877237
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1052747555477118
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1853.4441928499864
    mean_inference_ms: 4.119072926036625
    mean_raw_obs_processing_ms: 203.22402850227766
  time_since_restore: 9105.787333011627
  time_this_iter_s: 281.3025164604187
  time_total_s: 354107.6799738407
  timers:
    learn_throughput: 740.037
    learn_time_ms: 2553.926
    load_throughput: 179196.354
    load_time_ms: 10.547
    sample_throughput: 2.348
    sample_time_ms: 804898.515
    update_time_ms: 27.281
  timestamp: 1633069602
  timesteps_since_restore: 0
  timesteps_total: 589680
  training_iteration: 312
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    312 |           354108 | 589680 |  4.90429 |              8.18645 |               1.9135 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 591570
  custom_metrics: {}
  date: 2021-09-30_23-33-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.832344716541246
  episode_reward_mean: 4.977871339635425
  episode_reward_min: 2.2977727861575916
  episodes_this_iter: 270
  episodes_total: 84510
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.071999865118414e-05
          entropy: 10.293413162231445
          entropy_coeff: 0.00018806240404956043
          kl: 0.018792716786265373
          model: {}
          policy_loss: -0.18568886816501617
          total_loss: -0.13667869567871094
          vf_explained_var: 0.9934857487678528
          vf_loss: 0.022404523566365242
    num_agent_steps_sampled: 591570
    num_agent_steps_trained: 591570
    num_steps_sampled: 591570
    num_steps_trained: 591570
  iterations_since_restore: 13
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 19.454428044280444
    ram_util_percent: 10.394095940959408
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10508399481619932
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1832.8518049699096
    mean_inference_ms: 3.98028105817037
    mean_raw_obs_processing_ms: 203.15519204339475
  time_since_restore: 9495.315972089767
  time_this_iter_s: 389.52863907814026
  time_total_s: 354497.20861291885
  timers:
    learn_throughput: 742.316
    learn_time_ms: 2546.086
    load_throughput: 179529.947
    load_time_ms: 10.527
    sample_throughput: 2.345
    sample_time_ms: 805817.393
    update_time_ms: 27.211
  timestamp: 1633069992
  timesteps_since_restore: 0
  timesteps_total: 591570
  training_iteration: 313
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    313 |           354497 | 591570 |  4.97787 |              7.83234 |              2.29777 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 593460
  custom_metrics: {}
  date: 2021-09-30_23-41-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.278605072301334
  episode_reward_mean: 4.996655690630787
  episode_reward_min: 1.9171096852466922
  episodes_this_iter: 270
  episodes_total: 84780
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.053000157000497e-05
          entropy: 10.228567123413086
          entropy_coeff: 0.00018706260016188025
          kl: 0.01880461350083351
          model: {}
          policy_loss: -0.17982426285743713
          total_loss: -0.13471780717372894
          vf_explained_var: 0.9947258234024048
          vf_loss: 0.018460318446159363
    num_agent_steps_sampled: 593460
    num_agent_steps_trained: 593460
    num_steps_sampled: 593460
    num_steps_trained: 593460
  iterations_since_restore: 14
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 18.16782608695652
    ram_util_percent: 10.444927536231884
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1049623204299574
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1792.0479879004547
    mean_inference_ms: 3.8678431649879537
    mean_raw_obs_processing_ms: 203.45467487477927
  time_since_restore: 9990.712193489075
  time_this_iter_s: 495.39622139930725
  time_total_s: 354992.60483431816
  timers:
    learn_throughput: 743.48
    learn_time_ms: 2542.098
    load_throughput: 179595.024
    load_time_ms: 10.524
    sample_throughput: 2.376
    sample_time_ms: 795561.623
    update_time_ms: 27.349
  timestamp: 1633070487
  timesteps_since_restore: 0
  timesteps_total: 593460
  training_iteration: 314
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    314 |           354993 | 593460 |  4.99666 |              8.27861 |              1.91711 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 595350
  custom_metrics: {}
  date: 2021-10-01_00-02-55
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.732833398688033
  episode_reward_mean: 4.9571751084006035
  episode_reward_min: 2.658548093645115
  episodes_this_iter: 270
  episodes_total: 85050
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.034000085084699e-05
          entropy: 10.327558517456055
          entropy_coeff: 0.00018606279627420008
          kl: 0.017503486946225166
          model: {}
          policy_loss: -0.1797693818807602
          total_loss: -0.1326761096715927
          vf_explained_var: 0.9934733510017395
          vf_loss: 0.022431407123804092
    num_agent_steps_sampled: 595350
    num_agent_steps_trained: 595350
    num_steps_sampled: 595350
    num_steps_trained: 595350
  iterations_since_restore: 15
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 15.1008356545961
    ram_util_percent: 10.186350974930361
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1051523025483233
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1827.1917735523816
    mean_inference_ms: 3.7666426011296936
    mean_raw_obs_processing_ms: 203.57504653495425
  time_since_restore: 11278.927785158157
  time_this_iter_s: 1288.2155916690826
  time_total_s: 356280.82042598724
  timers:
    learn_throughput: 745.892
    learn_time_ms: 2533.879
    load_throughput: 178926.171
    load_time_ms: 10.563
    sample_throughput: 3.391
    sample_time_ms: 557351.724
    update_time_ms: 27.196
  timestamp: 1633071775
  timesteps_since_restore: 0
  timesteps_total: 595350
  training_iteration: 315
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    315 |           356281 | 595350 |  4.95718 |              7.73283 |              2.65855 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 597240
  custom_metrics: {}
  date: 2021-10-01_00-09-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.7579149696944985
  episode_reward_mean: 4.895409124849608
  episode_reward_min: 1.8560505542235277
  episodes_this_iter: 270
  episodes_total: 85320
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 1.5187499523162842
          cur_lr: 4.015000013168901e-05
          entropy: 10.31484317779541
          entropy_coeff: 0.00018506300693843514
          kl: 0.020025894045829773
          model: {}
          policy_loss: -0.18102386593818665
          total_loss: -0.13121916353702545
          vf_explained_var: 0.9938492178916931
          vf_loss: 0.021299269050359726
    num_agent_steps_sampled: 597240
    num_agent_steps_trained: 597240
    num_steps_sampled: 597240
    num_steps_trained: 597240
  iterations_since_restore: 16
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.329173989455185
    ram_util_percent: 9.704217926186292
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1050407032907745
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1833.6054743072132
    mean_inference_ms: 3.6763494907583234
    mean_raw_obs_processing_ms: 203.66662076471275
  time_since_restore: 11688.188831806183
  time_this_iter_s: 409.2610466480255
  time_total_s: 356690.08147263527
  timers:
    learn_throughput: 747.952
    learn_time_ms: 2526.899
    load_throughput: 182104.664
    load_time_ms: 10.379
    sample_throughput: 3.295
    sample_time_ms: 573595.589
    update_time_ms: 27.184
  timestamp: 1633072185
  timesteps_since_restore: 0
  timesteps_total: 597240
  training_iteration: 316
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    316 |           356690 | 597240 |  4.89541 |              7.75791 |              1.85605 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 599130
  custom_metrics: {}
  date: 2021-10-01_00-25-41
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.93094128309734
  episode_reward_mean: 4.980064151513304
  episode_reward_min: 2.048014692344129
  episodes_this_iter: 270
  episodes_total: 85590
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.995999941253103e-05
          entropy: 10.215553283691406
          entropy_coeff: 0.00018406320305075496
          kl: 0.014557604677975178
          model: {}
          policy_loss: -0.1949416697025299
          total_loss: -0.14237813651561737
          vf_explained_var: 0.9937885403633118
          vf_loss: 0.02127980813384056
    num_agent_steps_sampled: 599130
    num_agent_steps_trained: 599130
    num_steps_sampled: 599130
    num_steps_trained: 599130
  iterations_since_restore: 17
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.644703230653644
    ram_util_percent: 13.499023290758828
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10483004447195707
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1824.7156228609792
    mean_inference_ms: 3.6017279061128713
    mean_raw_obs_processing_ms: 203.7186854055237
  time_since_restore: 12644.148805618286
  time_this_iter_s: 955.9599738121033
  time_total_s: 357646.0414464474
  timers:
    learn_throughput: 748.449
    learn_time_ms: 2525.221
    load_throughput: 182532.785
    load_time_ms: 10.354
    sample_throughput: 3.216
    sample_time_ms: 587598.552
    update_time_ms: 27.158
  timestamp: 1633073141
  timesteps_since_restore: 0
  timesteps_total: 599130
  training_iteration: 317
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 24.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    317 |           357646 | 599130 |  4.98006 |              7.93094 |              2.04801 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 601020
  custom_metrics: {}
  date: 2021-10-01_00-31-02
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.462994303922667
  episode_reward_mean: 5.01730128836719
  episode_reward_min: 2.198375126403423
  episodes_this_iter: 270
  episodes_total: 85860
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.9769998693373054e-05
          entropy: 10.196724891662598
          entropy_coeff: 0.0001830633991630748
          kl: 0.018539680168032646
          model: {}
          policy_loss: -0.1857464611530304
          total_loss: -0.1209791898727417
          vf_explained_var: 0.9931277632713318
          vf_loss: 0.024398228153586388
    num_agent_steps_sampled: 601020
    num_agent_steps_trained: 601020
    num_steps_sampled: 601020
    num_steps_trained: 601020
  iterations_since_restore: 18
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 9.65324384787472
    ram_util_percent: 13.461744966442952
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10474862160261754
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1803.32880583467
    mean_inference_ms: 3.528905393019408
    mean_raw_obs_processing_ms: 203.7946986202753
  time_since_restore: 12965.627030611038
  time_this_iter_s: 321.4782249927521
  time_total_s: 357967.5196714401
  timers:
    learn_throughput: 749.733
    learn_time_ms: 2520.896
    load_throughput: 183450.266
    load_time_ms: 10.303
    sample_throughput: 3.277
    sample_time_ms: 576821.62
    update_time_ms: 27.224
  timestamp: 1633073462
  timesteps_since_restore: 0
  timesteps_total: 601020
  training_iteration: 318
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    318 |           357968 | 601020 |   5.0173 |              8.46299 |              2.19838 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 602910
  custom_metrics: {}
  date: 2021-10-01_00-42-33
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.014524141373263
  episode_reward_mean: 4.983569468589884
  episode_reward_min: 2.370900832062304
  episodes_this_iter: 270
  episodes_total: 86130
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.958000161219388e-05
          entropy: 10.070648193359375
          entropy_coeff: 0.00018206359527539462
          kl: 0.01423664204776287
          model: {}
          policy_loss: -0.18221881985664368
          total_loss: -0.12828503549098969
          vf_explained_var: 0.9934074282646179
          vf_loss: 0.023334432393312454
    num_agent_steps_sampled: 602910
    num_agent_steps_trained: 602910
    num_steps_sampled: 602910
    num_steps_trained: 602910
  iterations_since_restore: 19
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.439125910509886
    ram_util_percent: 13.484495317377732
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10460315687941817
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1808.704221356112
    mean_inference_ms: 3.4671270958112927
    mean_raw_obs_processing_ms: 203.8582181116962
  time_since_restore: 13656.370012044907
  time_this_iter_s: 690.7429814338684
  time_total_s: 358658.262652874
  timers:
    learn_throughput: 748.647
    learn_time_ms: 2524.553
    load_throughput: 182389.155
    load_time_ms: 10.362
    sample_throughput: 2.994
    sample_time_ms: 631175.584
    update_time_ms: 27.401
  timestamp: 1633074153
  timesteps_since_restore: 0
  timesteps_total: 602910
  training_iteration: 319
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 27.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    319 |           358658 | 602910 |  4.98357 |              8.01452 |               2.3709 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 604800
  custom_metrics: {}
  date: 2021-10-01_01-06-01
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.700629991192631
  episode_reward_mean: 4.921924396132989
  episode_reward_min: 1.875222949318878
  episodes_this_iter: 270
  episodes_total: 86400
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.9390000893035904e-05
          entropy: 10.118141174316406
          entropy_coeff: 0.00018106380593962967
          kl: 0.014197592623531818
          model: {}
          policy_loss: -0.20670060813426971
          total_loss: -0.1555974930524826
          vf_explained_var: 0.993999183177948
          vf_loss: 0.020591264590620995
    num_agent_steps_sampled: 604800
    num_agent_steps_trained: 604800
    num_steps_sampled: 604800
    num_steps_trained: 604800
  iterations_since_restore: 20
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.150562372188139
    ram_util_percent: 13.6420245398773
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10455963528956037
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1808.402986389478
    mean_inference_ms: 3.407156406159739
    mean_raw_obs_processing_ms: 203.87784052353942
  time_since_restore: 15064.735567569733
  time_this_iter_s: 1408.365555524826
  time_total_s: 360066.6282083988
  timers:
    learn_throughput: 747.548
    learn_time_ms: 2528.265
    load_throughput: 183246.715
    load_time_ms: 10.314
    sample_throughput: 2.973
    sample_time_ms: 635810.565
    update_time_ms: 27.641
  timestamp: 1633075561
  timesteps_since_restore: 0
  timesteps_total: 604800
  training_iteration: 320
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    320 |           360067 | 604800 |  4.92192 |              7.70063 |              1.87522 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 606690
  custom_metrics: {}
  date: 2021-10-01_01-20-55
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.484841190379167
  episode_reward_mean: 5.040285038694989
  episode_reward_min: 2.176549498004774
  episodes_this_iter: 270
  episodes_total: 86670
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.9200000173877925e-05
          entropy: 10.085006713867188
          entropy_coeff: 0.0001800640020519495
          kl: 0.014705916866660118
          model: {}
          policy_loss: -0.1993534415960312
          total_loss: -0.14745940268039703
          vf_explained_var: 0.9943919777870178
          vf_loss: 0.02020806260406971
    num_agent_steps_sampled: 606690
    num_agent_steps_trained: 606690
    num_steps_sampled: 606690
    num_steps_trained: 606690
  iterations_since_restore: 21
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 8.884003215434085
    ram_util_percent: 13.699196141479103
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1044617549323916
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1809.2387828838873
    mean_inference_ms: 3.355359007409737
    mean_raw_obs_processing_ms: 203.84578321687994
  time_since_restore: 15958.687438488007
  time_this_iter_s: 893.9518709182739
  time_total_s: 360960.5800793171
  timers:
    learn_throughput: 747.458
    learn_time_ms: 2528.572
    load_throughput: 181393.362
    load_time_ms: 10.419
    sample_throughput: 2.659
    sample_time_ms: 710722.853
    update_time_ms: 27.761
  timestamp: 1633076455
  timesteps_since_restore: 0
  timesteps_total: 606690
  training_iteration: 321
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    321 |           360961 | 606690 |  5.04029 |              7.48484 |              2.17655 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 608580
  custom_metrics: {}
  date: 2021-10-01_01-32-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.034225664901154
  episode_reward_mean: 4.97017432730837
  episode_reward_min: 2.1514895132841927
  episodes_this_iter: 270
  episodes_total: 86940
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.9009999454719946e-05
          entropy: 10.124489784240723
          entropy_coeff: 0.00017906419816426933
          kl: 0.01348977442830801
          model: {}
          policy_loss: -0.1709684431552887
          total_loss: -0.11896264553070068
          vf_explained_var: 0.9933732151985168
          vf_loss: 0.02308734692633152
    num_agent_steps_sampled: 608580
    num_agent_steps_trained: 608580
    num_steps_sampled: 608580
    num_steps_trained: 608580
  iterations_since_restore: 22
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 10.054744525547445
    ram_util_percent: 13.729301355578729
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10442251955078301
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1809.2335811296298
    mean_inference_ms: 3.3078951357252007
    mean_raw_obs_processing_ms: 203.88064040392405
  time_since_restore: 16649.502324581146
  time_this_iter_s: 690.8148860931396
  time_total_s: 361651.39496541023
  timers:
    learn_throughput: 748.531
    learn_time_ms: 2524.945
    load_throughput: 182164.924
    load_time_ms: 10.375
    sample_throughput: 2.514
    sample_time_ms: 751677.154
    update_time_ms: 27.807
  timestamp: 1633077146
  timesteps_since_restore: 0
  timesteps_total: 608580
  training_iteration: 322
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    322 |           361651 | 608580 |  4.97017 |              8.03423 |              2.15149 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 610470
  custom_metrics: {}
  date: 2021-10-01_01-44-37
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.8659551614234875
  episode_reward_mean: 4.9408288580780395
  episode_reward_min: 1.8237396038856202
  episodes_this_iter: 270
  episodes_total: 87210
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.881999873556197e-05
          entropy: 10.140254020690918
          entropy_coeff: 0.00017806439427658916
          kl: 0.01494160108268261
          model: {}
          policy_loss: -0.19675853848457336
          total_loss: -0.14166367053985596
          vf_explained_var: 0.993503987789154
          vf_loss: 0.022861648350954056
    num_agent_steps_sampled: 610470
    num_agent_steps_trained: 610470
    num_steps_sampled: 610470
    num_steps_trained: 610470
  iterations_since_restore: 23
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 9.611406096361847
    ram_util_percent: 13.677876106194688
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10447683663505465
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1810.4862675591903
    mean_inference_ms: 3.266116646284522
    mean_raw_obs_processing_ms: 203.8964293197459
  time_since_restore: 17380.462476491928
  time_this_iter_s: 730.9601519107819
  time_total_s: 362382.355117321
  timers:
    learn_throughput: 746.946
    learn_time_ms: 2530.302
    load_throughput: 182834.217
    load_time_ms: 10.337
    sample_throughput: 2.405
    sample_time_ms: 785814.443
    update_time_ms: 28.078
  timestamp: 1633077877
  timesteps_since_restore: 0
  timesteps_total: 610470
  training_iteration: 323
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    323 |           362382 | 610470 |  4.94083 |              7.86596 |              1.82374 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 612360
  custom_metrics: {}
  date: 2021-10-01_02-42-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.140995978513466
  episode_reward_mean: 4.9934399256971655
  episode_reward_min: 2.319604376649819
  episodes_this_iter: 270
  episodes_total: 87480
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.8630001654382795e-05
          entropy: 10.146098136901855
          entropy_coeff: 0.0001770646049408242
          kl: 0.015802646055817604
          model: {}
          policy_loss: -0.2015247642993927
          total_loss: -0.1451955884695053
          vf_explained_var: 0.9938316345214844
          vf_loss: 0.022125260904431343
    num_agent_steps_sampled: 612360
    num_agent_steps_trained: 612360
    num_steps_sampled: 612360
    num_steps_trained: 612360
  iterations_since_restore: 24
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.302588527645476
    ram_util_percent: 13.547649616897909
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10440275425150547
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1881.4242897459221
    mean_inference_ms: 3.227558134938219
    mean_raw_obs_processing_ms: 203.83471516098655
  time_since_restore: 20849.779908180237
  time_this_iter_s: 3469.3174316883087
  time_total_s: 365851.6725490093
  timers:
    learn_throughput: 746.238
    learn_time_ms: 2532.705
    load_throughput: 182880.193
    load_time_ms: 10.335
    sample_throughput: 1.745
    sample_time_ms: 1083203.615
    update_time_ms: 28.146
  timestamp: 1633081347
  timesteps_since_restore: 0
  timesteps_total: 612360
  training_iteration: 324
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    324 |           365852 | 612360 |  4.99344 |                8.141 |               2.3196 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 614250
  custom_metrics: {}
  date: 2021-10-01_02-49-03
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.406530507650729
  episode_reward_mean: 4.9833069492814985
  episode_reward_min: 1.5778041026604621
  episodes_this_iter: 270
  episodes_total: 87750
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.8440000935224816e-05
          entropy: 10.066336631774902
          entropy_coeff: 0.00017606480105314404
          kl: 0.013992255553603172
          model: {}
          policy_loss: -0.19502350687980652
          total_loss: -0.14246542751789093
          vf_explained_var: 0.9937571287155151
          vf_loss: 0.02245430089533329
    num_agent_steps_sampled: 614250
    num_agent_steps_trained: 614250
    num_steps_sampled: 614250
    num_steps_trained: 614250
  iterations_since_restore: 25
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 8.598185117967333
    ram_util_percent: 13.574954627949182
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10491864545644429
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1859.4937415292547
    mean_inference_ms: 3.190423195229007
    mean_raw_obs_processing_ms: 203.90844097038834
  time_since_restore: 21245.957630872726
  time_this_iter_s: 396.1777226924896
  time_total_s: 366247.8502717018
  timers:
    learn_throughput: 743.851
    learn_time_ms: 2540.832
    load_throughput: 181863.189
    load_time_ms: 10.392
    sample_throughput: 1.901
    sample_time_ms: 993991.099
    update_time_ms: 28.204
  timestamp: 1633081743
  timesteps_since_restore: 0
  timesteps_total: 614250
  training_iteration: 325
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 27.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    325 |           366248 | 614250 |  4.98331 |              8.40653 |               1.5778 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 616140
  custom_metrics: {}
  date: 2021-10-01_03-23-53
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.661466439718029
  episode_reward_mean: 4.989924031800008
  episode_reward_min: 2.222260480153452
  episodes_this_iter: 270
  episodes_total: 88020
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.825000021606684e-05
          entropy: 10.094740867614746
          entropy_coeff: 0.00017506499716546386
          kl: 0.014720715582370758
          model: {}
          policy_loss: -0.18797509372234344
          total_loss: -0.1341477483510971
          vf_explained_var: 0.9937325716018677
          vf_loss: 0.022058947011828423
    num_agent_steps_sampled: 616140
    num_agent_steps_trained: 616140
    num_steps_sampled: 616140
    num_steps_trained: 616140
  iterations_since_restore: 26
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.141850068775791
    ram_util_percent: 13.54566712517194
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10488038375134519
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1885.8649631637657
    mean_inference_ms: 3.157861497667101
    mean_raw_obs_processing_ms: 203.9289382313303
  time_since_restore: 23335.694882631302
  time_this_iter_s: 2089.7372517585754
  time_total_s: 368337.5875234604
  timers:
    learn_throughput: 742.843
    learn_time_ms: 2544.28
    load_throughput: 181278.875
    load_time_ms: 10.426
    sample_throughput: 1.626
    sample_time_ms: 1162034.73
    update_time_ms: 28.085
  timestamp: 1633083833
  timesteps_since_restore: 0
  timesteps_total: 616140
  training_iteration: 326
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    326 |           368338 | 616140 |  4.98992 |              7.66147 |              2.22226 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 618030
  custom_metrics: {}
  date: 2021-10-01_03-59-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.866137251979493
  episode_reward_mean: 5.083275828572535
  episode_reward_min: 1.7753046093810947
  episodes_this_iter: 270
  episodes_total: 88290
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.805999949690886e-05
          entropy: 9.908951759338379
          entropy_coeff: 0.0001740651932777837
          kl: 0.014547661878168583
          model: {}
          policy_loss: -0.19427046179771423
          total_loss: -0.14110563695430756
          vf_explained_var: 0.9939907789230347
          vf_loss: 0.021748235449194908
    num_agent_steps_sampled: 618030
    num_agent_steps_trained: 618030
    num_steps_sampled: 618030
    num_steps_trained: 618030
  iterations_since_restore: 27
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 8.500935828877006
    ram_util_percent: 13.683589572192515
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10486398404450473
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1916.9674276664882
    mean_inference_ms: 3.1279452115247808
    mean_raw_obs_processing_ms: 203.92514579890002
  time_since_restore: 25485.330035209656
  time_this_iter_s: 2149.635152578354
  time_total_s: 370487.22267603874
  timers:
    learn_throughput: 742.595
    learn_time_ms: 2545.13
    load_throughput: 181731.026
    load_time_ms: 10.4
    sample_throughput: 1.475
    sample_time_ms: 1281400.872
    update_time_ms: 28.489
  timestamp: 1633085982
  timesteps_since_restore: 0
  timesteps_total: 618030
  training_iteration: 327
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    327 |           370487 | 618030 |  5.08328 |              7.86614 |               1.7753 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 619920
  custom_metrics: {}
  date: 2021-10-01_04-11-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.263857224931229
  episode_reward_mean: 4.989056048311535
  episode_reward_min: 2.5638362849239935
  episodes_this_iter: 270
  episodes_total: 88560
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.786999877775088e-05
          entropy: 10.074397087097168
          entropy_coeff: 0.00017306540394201875
          kl: 0.01572251319885254
          model: {}
          policy_loss: -0.20725445449352264
          total_loss: -0.15181532502174377
          vf_explained_var: 0.9938693046569824
          vf_loss: 0.02136479690670967
    num_agent_steps_sampled: 619920
    num_agent_steps_trained: 619920
    num_steps_sampled: 619920
    num_steps_trained: 619920
  iterations_since_restore: 28
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.345962732919254
    ram_util_percent: 13.63695652173913
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10509774974980819
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1920.0370983425405
    mean_inference_ms: 3.1007627101255046
    mean_raw_obs_processing_ms: 203.95848133844635
  time_since_restore: 26179.87354159355
  time_this_iter_s: 694.5435063838959
  time_total_s: 371181.76618242264
  timers:
    learn_throughput: 743.049
    learn_time_ms: 2543.575
    load_throughput: 181202.631
    load_time_ms: 10.43
    sample_throughput: 1.433
    sample_time_ms: 1318708.291
    update_time_ms: 28.466
  timestamp: 1633086677
  timesteps_since_restore: 0
  timesteps_total: 619920
  training_iteration: 328
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    328 |           371182 | 619920 |  4.98906 |              8.26386 |              2.56384 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 621810
  custom_metrics: {}
  date: 2021-10-01_04-41-53
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.782137175404132
  episode_reward_mean: 4.98683306029968
  episode_reward_min: 2.218311735409506
  episodes_this_iter: 270
  episodes_total: 88830
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.768000169657171e-05
          entropy: 10.01372241973877
          entropy_coeff: 0.00017206560005433857
          kl: 0.014413298107683659
          model: {}
          policy_loss: -0.18127094209194183
          total_loss: -0.12784163653850555
          vf_explained_var: 0.9934352040290833
          vf_loss: 0.02231704257428646
    num_agent_steps_sampled: 621810
    num_agent_steps_trained: 621810
    num_steps_sampled: 621810
    num_steps_trained: 621810
  iterations_since_restore: 29
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 5.8131898238747555
    ram_util_percent: 13.542152641878667
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10495917444960717
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1926.5988763876371
    mean_inference_ms: 3.0754378868392545
    mean_raw_obs_processing_ms: 204.01244527140028
  time_since_restore: 28015.646403074265
  time_this_iter_s: 1835.772861480713
  time_total_s: 373017.53904390335
  timers:
    learn_throughput: 744.95
    learn_time_ms: 2537.083
    load_throughput: 181828.566
    load_time_ms: 10.394
    sample_throughput: 1.319
    sample_time_ms: 1433217.637
    update_time_ms: 28.577
  timestamp: 1633088513
  timesteps_since_restore: 0
  timesteps_total: 621810
  training_iteration: 329
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 24.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    329 |           373018 | 621810 |  4.98683 |              7.78214 |              2.21831 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 623700
  custom_metrics: {}
  date: 2021-10-01_04-44-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.795223886858066
  episode_reward_mean: 4.974021221545839
  episode_reward_min: 2.3053708003831477
  episodes_this_iter: 270
  episodes_total: 89100
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.749000097741373e-05
          entropy: 10.054780006408691
          entropy_coeff: 0.0001710657961666584
          kl: 0.0149694187566638
          model: {}
          policy_loss: -0.20297381281852722
          total_loss: -0.14970184862613678
          vf_explained_var: 0.9941097497940063
          vf_loss: 0.020889772102236748
    num_agent_steps_sampled: 623700
    num_agent_steps_trained: 623700
    num_steps_sampled: 623700
    num_steps_trained: 623700
  iterations_since_restore: 30
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 12.605376344086022
    ram_util_percent: 13.462903225806453
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10500590174307427
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1893.297328589889
    mean_inference_ms: 3.0526217489745098
    mean_raw_obs_processing_ms: 204.09391091850887
  time_since_restore: 28149.4085187912
  time_this_iter_s: 133.7621157169342
  time_total_s: 373151.3011596203
  timers:
    learn_throughput: 745.159
    learn_time_ms: 2536.371
    load_throughput: 182039.846
    load_time_ms: 10.382
    sample_throughput: 1.447
    sample_time_ms: 1305758.157
    update_time_ms: 28.208
  timestamp: 1633088646
  timesteps_since_restore: 0
  timesteps_total: 623700
  training_iteration: 330
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    330 |           373151 | 623700 |  4.97402 |              7.79522 |              2.30537 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 625590
  custom_metrics: {}
  date: 2021-10-01_04-59-54
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.268478369499102
  episode_reward_mean: 4.990524698023583
  episode_reward_min: 2.260123413751954
  episodes_this_iter: 270
  episodes_total: 89370
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.730000025825575e-05
          entropy: 9.988297462463379
          entropy_coeff: 0.00017006600683089346
          kl: 0.013632519170641899
          model: {}
          policy_loss: -0.1874924898147583
          total_loss: -0.13752514123916626
          vf_explained_var: 0.994227945804596
          vf_loss: 0.020609457045793533
    num_agent_steps_sampled: 625590
    num_agent_steps_trained: 625590
    num_steps_sampled: 625590
    num_steps_trained: 625590
  iterations_since_restore: 31
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.298101746393319
    ram_util_percent: 13.60250569476082
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10499359445908872
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1922.1276985142474
    mean_inference_ms: 3.032566312527646
    mean_raw_obs_processing_ms: 204.10176849202838
  time_since_restore: 29096.70580625534
  time_this_iter_s: 947.2972874641418
  time_total_s: 374098.5984470844
  timers:
    learn_throughput: 744.983
    learn_time_ms: 2536.97
    load_throughput: 183903.088
    load_time_ms: 10.277
    sample_throughput: 1.442
    sample_time_ms: 1311091.851
    update_time_ms: 28.457
  timestamp: 1633089594
  timesteps_since_restore: 0
  timesteps_total: 625590
  training_iteration: 331
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    331 |           374099 | 625590 |  4.99052 |              8.26848 |              2.26012 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 627480
  custom_metrics: {}
  date: 2021-10-01_05-17-14
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.806268219562116
  episode_reward_mean: 4.96825654264836
  episode_reward_min: 1.9425121801297847
  episodes_this_iter: 270
  episodes_total: 89640
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.710999953909777e-05
          entropy: 9.89705753326416
          entropy_coeff: 0.00016906620294321328
          kl: 0.014446484856307507
          model: {}
          policy_loss: -0.20895694196224213
          total_loss: -0.15723584592342377
          vf_explained_var: 0.9941692352294922
          vf_loss: 0.020483437925577164
    num_agent_steps_sampled: 627480
    num_agent_steps_trained: 627480
    num_steps_sampled: 627480
    num_steps_trained: 627480
  iterations_since_restore: 32
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 8.832365145228216
    ram_util_percent: 13.551659751037345
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.105054077806719
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1937.1412943763166
    mean_inference_ms: 3.010784713366259
    mean_raw_obs_processing_ms: 204.1327247739202
  time_since_restore: 30136.68330526352
  time_this_iter_s: 1039.9774990081787
  time_total_s: 375138.5759460926
  timers:
    learn_throughput: 744.486
    learn_time_ms: 2538.664
    load_throughput: 182910.997
    load_time_ms: 10.333
    sample_throughput: 1.404
    sample_time_ms: 1346007.05
    update_time_ms: 28.468
  timestamp: 1633090634
  timesteps_since_restore: 0
  timesteps_total: 627480
  training_iteration: 332
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    332 |           375139 | 627480 |  4.96826 |              7.80627 |              1.94251 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 629370
  custom_metrics: {}
  date: 2021-10-01_05-25-25
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.685965210728158
  episode_reward_mean: 4.957447027363398
  episode_reward_min: 1.8358481648117198
  episodes_this_iter: 270
  episodes_total: 89910
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.691999881993979e-05
          entropy: 9.952195167541504
          entropy_coeff: 0.0001680663990555331
          kl: 0.014345578849315643
          model: {}
          policy_loss: -0.1707303673028946
          total_loss: -0.11728530377149582
          vf_explained_var: 0.9937266111373901
          vf_loss: 0.022436683997511864
    num_agent_steps_sampled: 629370
    num_agent_steps_trained: 629370
    num_steps_sampled: 629370
    num_steps_trained: 629370
  iterations_since_restore: 33
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.95095168374817
    ram_util_percent: 13.634260614934115
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10499776854399498
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1916.1245862614314
    mean_inference_ms: 2.9917747723980903
    mean_raw_obs_processing_ms: 204.16361767428842
  time_since_restore: 30627.51179790497
  time_this_iter_s: 490.828492641449
  time_total_s: 375629.40443873405
  timers:
    learn_throughput: 744.343
    learn_time_ms: 2539.151
    load_throughput: 183556.037
    load_time_ms: 10.297
    sample_throughput: 1.43
    sample_time_ms: 1321993.718
    update_time_ms: 28.425
  timestamp: 1633091125
  timesteps_since_restore: 0
  timesteps_total: 629370
  training_iteration: 333
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    333 |           375629 | 629370 |  4.95745 |              7.68597 |              1.83585 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 631260
  custom_metrics: {}
  date: 2021-10-01_05-34-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.859245160542079
  episode_reward_mean: 5.034872388683012
  episode_reward_min: 2.1440687342151286
  episodes_this_iter: 270
  episodes_total: 90180
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.673000173876062e-05
          entropy: 9.825337409973145
          entropy_coeff: 0.00016706659516785294
          kl: 0.013862038031220436
          model: {}
          policy_loss: -0.18495415151119232
          total_loss: -0.13327465951442719
          vf_explained_var: 0.9939578175544739
          vf_loss: 0.021741503849625587
    num_agent_steps_sampled: 631260
    num_agent_steps_trained: 631260
    num_steps_sampled: 631260
    num_steps_trained: 631260
  iterations_since_restore: 34
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.081855388813097
    ram_util_percent: 13.62387448840382
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10494152871484443
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1900.0518780130806
    mean_inference_ms: 2.972895503553214
    mean_raw_obs_processing_ms: 204.1187092776402
  time_since_restore: 31154.63116455078
  time_this_iter_s: 527.119366645813
  time_total_s: 376156.52380537987
  timers:
    learn_throughput: 743.714
    learn_time_ms: 2541.298
    load_throughput: 184292.147
    load_time_ms: 10.255
    sample_throughput: 1.839
    sample_time_ms: 1027772.147
    update_time_ms: 28.514
  timestamp: 1633091652
  timesteps_since_restore: 0
  timesteps_total: 631260
  training_iteration: 334
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    334 |           376157 | 631260 |  5.03487 |              7.85925 |              2.14407 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 633150
  custom_metrics: {}
  date: 2021-10-01_05-43-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.519312725905502
  episode_reward_mean: 4.951136817073908
  episode_reward_min: 2.4758641518533717
  episodes_this_iter: 270
  episodes_total: 90450
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.654000101960264e-05
          entropy: 9.984309196472168
          entropy_coeff: 0.000166066805832088
          kl: 0.01451941393315792
          model: {}
          policy_loss: -0.18184886872768402
          total_loss: -0.12912051379680634
          vf_explained_var: 0.9937124252319336
          vf_loss: 0.021309373900294304
    num_agent_steps_sampled: 633150
    num_agent_steps_trained: 633150
    num_steps_sampled: 633150
    num_steps_trained: 633150
  iterations_since_restore: 35
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 11.451478494623656
    ram_util_percent: 13.527822580645163
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10503613679955931
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1906.0915254311046
    mean_inference_ms: 2.9557693811206422
    mean_raw_obs_processing_ms: 204.13907849519796
  time_since_restore: 31689.222960948944
  time_this_iter_s: 534.5917963981628
  time_total_s: 376691.11560177803
  timers:
    learn_throughput: 743.41
    learn_time_ms: 2542.338
    load_throughput: 184862.88
    load_time_ms: 10.224
    sample_throughput: 1.814
    sample_time_ms: 1041613.014
    update_time_ms: 28.536
  timestamp: 1633092186
  timesteps_since_restore: 0
  timesteps_total: 633150
  training_iteration: 335
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 24.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    335 |           376691 | 633150 |  4.95114 |              7.51931 |              2.47586 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 635040
  custom_metrics: {}
  date: 2021-10-01_05-48-07
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.19104746351049
  episode_reward_mean: 4.963709649004127
  episode_reward_min: 1.5430539412739974
  episodes_this_iter: 270
  episodes_total: 90720
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.635000030044466e-05
          entropy: 9.961610794067383
          entropy_coeff: 0.00016506700194440782
          kl: 0.013394872657954693
          model: {}
          policy_loss: -0.17298771440982819
          total_loss: -0.11980413645505905
          vf_explained_var: 0.9930176734924316
          vf_loss: 0.024312693625688553
    num_agent_steps_sampled: 635040
    num_agent_steps_trained: 635040
    num_steps_sampled: 635040
    num_steps_trained: 635040
  iterations_since_restore: 36
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 10.659808612440191
    ram_util_percent: 13.630382775119617
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1051004771395922
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1897.909521068633
    mean_inference_ms: 2.9397519079809182
    mean_raw_obs_processing_ms: 204.1193198668113
  time_since_restore: 31990.171657562256
  time_this_iter_s: 300.94869661331177
  time_total_s: 376992.06429839134
  timers:
    learn_throughput: 743.134
    learn_time_ms: 2543.282
    load_throughput: 184861.156
    load_time_ms: 10.224
    sample_throughput: 2.191
    sample_time_ms: 862733.386
    update_time_ms: 28.845
  timestamp: 1633092487
  timesteps_since_restore: 0
  timesteps_total: 635040
  training_iteration: 336
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    336 |           376992 | 635040 |  4.96371 |              8.19105 |              1.54305 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 636930
  custom_metrics: {}
  date: 2021-10-01_06-01-15
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.613016192887773
  episode_reward_mean: 4.926168230132994
  episode_reward_min: 2.1688996911479834
  episodes_this_iter: 270
  episodes_total: 90990
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.6159999581286684e-05
          entropy: 9.94297981262207
          entropy_coeff: 0.00016406719805672765
          kl: 0.013216177932918072
          model: {}
          policy_loss: -0.1748773753643036
          total_loss: -0.12340681254863739
          vf_explained_var: 0.9933070540428162
          vf_loss: 0.022993778809905052
    num_agent_steps_sampled: 636930
    num_agent_steps_trained: 636930
    num_steps_sampled: 636930
    num_steps_trained: 636930
  iterations_since_restore: 37
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.579815668202763
    ram_util_percent: 13.680552995391706
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10516530720688909
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1890.8217860435357
    mean_inference_ms: 2.9238596604296254
    mean_raw_obs_processing_ms: 204.1208083459637
  time_since_restore: 32777.53862142563
  time_this_iter_s: 787.3669638633728
  time_total_s: 377779.4312622547
  timers:
    learn_throughput: 742.479
    learn_time_ms: 2545.526
    load_throughput: 184368.013
    load_time_ms: 10.251
    sample_throughput: 2.601
    sample_time_ms: 726504.819
    update_time_ms: 28.677
  timestamp: 1633093275
  timesteps_since_restore: 0
  timesteps_total: 636930
  training_iteration: 337
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    337 |           377779 | 636930 |  4.92617 |              7.61302 |               2.1689 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 638820
  custom_metrics: {}
  date: 2021-10-01_06-24-25
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.294583103855254
  episode_reward_mean: 5.064699960056547
  episode_reward_min: 2.205440393001652
  episodes_this_iter: 270
  episodes_total: 91260
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.5969998862128705e-05
          entropy: 9.87846565246582
          entropy_coeff: 0.00016306739416904747
          kl: 0.013697043992578983
          model: {}
          policy_loss: -0.18445217609405518
          total_loss: -0.1356247365474701
          vf_explained_var: 0.9946131110191345
          vf_loss: 0.01923469826579094
    num_agent_steps_sampled: 638820
    num_agent_steps_trained: 638820
    num_steps_sampled: 638820
    num_steps_trained: 638820
  iterations_since_restore: 38
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.972788411795138
    ram_util_percent: 13.671805483704086
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10512143044216275
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1910.9100589377117
    mean_inference_ms: 2.9097864323167326
    mean_raw_obs_processing_ms: 204.15581831813776
  time_since_restore: 34168.00284719467
  time_this_iter_s: 1390.464225769043
  time_total_s: 379169.89548802376
  timers:
    learn_throughput: 741.447
    learn_time_ms: 2549.068
    load_throughput: 183974.791
    load_time_ms: 10.273
    sample_throughput: 2.374
    sample_time_ms: 796059.754
    update_time_ms: 29.911
  timestamp: 1633094665
  timesteps_since_restore: 0
  timesteps_total: 638820
  training_iteration: 338
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    338 |           379170 | 638820 |   5.0647 |              8.29458 |              2.20544 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 640710
  custom_metrics: {}
  date: 2021-10-01_06-30-03
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.878942640378163
  episode_reward_mean: 4.994113307425335
  episode_reward_min: 1.8952725818911045
  episodes_this_iter: 270
  episodes_total: 91530
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.578000178094953e-05
          entropy: 9.832940101623535
          entropy_coeff: 0.00016206760483328253
          kl: 0.013893155381083488
          model: {}
          policy_loss: -0.18520508706569672
          total_loss: -0.13258923590183258
          vf_explained_var: 0.9935469627380371
          vf_loss: 0.022559110075235367
    num_agent_steps_sampled: 640710
    num_agent_steps_trained: 640710
    num_steps_sampled: 640710
    num_steps_trained: 640710
  iterations_since_restore: 39
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 8.139872068230277
    ram_util_percent: 13.640724946695096
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10510009162774202
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1889.9735925935183
    mean_inference_ms: 2.8953206491531684
    mean_raw_obs_processing_ms: 204.2177531381654
  time_since_restore: 34505.2602455616
  time_this_iter_s: 337.2573983669281
  time_total_s: 379507.1528863907
  timers:
    learn_throughput: 740.205
    learn_time_ms: 2553.347
    load_throughput: 184039.285
    load_time_ms: 10.27
    sample_throughput: 2.925
    sample_time_ms: 646204.361
    update_time_ms: 30.013
  timestamp: 1633095003
  timesteps_since_restore: 0
  timesteps_total: 640710
  training_iteration: 339
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 24.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    339 |           379507 | 640710 |  4.99411 |              7.87894 |              1.89527 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 642600
  custom_metrics: {}
  date: 2021-10-01_06-41-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.729775969042173
  episode_reward_mean: 4.964421539502653
  episode_reward_min: 2.3163605963425726
  episodes_this_iter: 270
  episodes_total: 91800
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.5590001061791554e-05
          entropy: 9.861021041870117
          entropy_coeff: 0.00016106780094560236
          kl: 0.013168700970709324
          model: {}
          policy_loss: -0.19454386830329895
          total_loss: -0.14560595154762268
          vf_explained_var: 0.9941597580909729
          vf_loss: 0.02052626945078373
    num_agent_steps_sampled: 642600
    num_agent_steps_trained: 642600
    num_steps_sampled: 642600
    num_steps_trained: 642600
  iterations_since_restore: 40
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.742002063983488
    ram_util_percent: 13.600206398348812
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10504045761590933
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1877.5865395423125
    mean_inference_ms: 2.8826896023764474
    mean_raw_obs_processing_ms: 204.21969620648048
  time_since_restore: 35202.170612335205
  time_this_iter_s: 696.9103667736053
  time_total_s: 380204.0632531643
  timers:
    learn_throughput: 740.144
    learn_time_ms: 2553.558
    load_throughput: 183685.761
    load_time_ms: 10.289
    sample_throughput: 2.69
    sample_time_ms: 702519.059
    update_time_ms: 30.413
  timestamp: 1633095700
  timesteps_since_restore: 0
  timesteps_total: 642600
  training_iteration: 340
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 27.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    340 |           380204 | 642600 |  4.96442 |              7.72978 |              2.31636 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 644490
  custom_metrics: {}
  date: 2021-10-01_06-48-15
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.668079641621289
  episode_reward_mean: 4.960118201843574
  episode_reward_min: 1.8633031003896063
  episodes_this_iter: 270
  episodes_total: 92070
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.5400000342633575e-05
          entropy: 9.86577033996582
          entropy_coeff: 0.00016006799705792218
          kl: 0.017532875761389732
          model: {}
          policy_loss: -0.18646804988384247
          total_loss: -0.1254883110523224
          vf_explained_var: 0.9935398101806641
          vf_loss: 0.022616853937506676
    num_agent_steps_sampled: 644490
    num_agent_steps_trained: 644490
    num_steps_sampled: 644490
    num_steps_trained: 644490
  iterations_since_restore: 41
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.814363636363636
    ram_util_percent: 13.662727272727272
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10498780328870395
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1874.3848951004938
    mean_inference_ms: 2.871682884228594
    mean_raw_obs_processing_ms: 204.2526807953962
  time_since_restore: 35597.37237405777
  time_this_iter_s: 395.2017617225647
  time_total_s: 380599.26501488686
  timers:
    learn_throughput: 740.783
    learn_time_ms: 2551.353
    load_throughput: 183030.936
    load_time_ms: 10.326
    sample_throughput: 2.92
    sample_time_ms: 647311.709
    update_time_ms: 30.395
  timestamp: 1633096095
  timesteps_since_restore: 0
  timesteps_total: 644490
  training_iteration: 341
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 24.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    341 |           380599 | 644490 |  4.96012 |              7.66808 |               1.8633 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 646380
  custom_metrics: {}
  date: 2021-10-01_07-07-11
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.667699124277609
  episode_reward_mean: 4.99131872404801
  episode_reward_min: 2.3336834139062725
  episodes_this_iter: 270
  episodes_total: 92340
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.5209999623475596e-05
          entropy: 9.904154777526855
          entropy_coeff: 0.000159068193170242
          kl: 0.015047775581479073
          model: {}
          policy_loss: -0.1897212266921997
          total_loss: -0.13455593585968018
          vf_explained_var: 0.9936279058456421
          vf_loss: 0.022460008040070534
    num_agent_steps_sampled: 646380
    num_agent_steps_trained: 646380
    num_steps_sampled: 646380
    num_steps_trained: 646380
  iterations_since_restore: 42
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 9.028752374920836
    ram_util_percent: 13.688853704876502
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10500976981317922
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1898.4606632007665
    mean_inference_ms: 2.860047605392551
    mean_raw_obs_processing_ms: 204.2598006695777
  time_since_restore: 36733.77828478813
  time_this_iter_s: 1136.405910730362
  time_total_s: 381735.6709256172
  timers:
    learn_throughput: 740.837
    learn_time_ms: 2551.167
    load_throughput: 183508.021
    load_time_ms: 10.299
    sample_throughput: 2.877
    sample_time_ms: 656954.695
    update_time_ms: 30.904
  timestamp: 1633097231
  timesteps_since_restore: 0
  timesteps_total: 646380
  training_iteration: 342
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 27.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    342 |           381736 | 646380 |  4.99132 |               7.6677 |              2.33368 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 648270
  custom_metrics: {}
  date: 2021-10-01_07-16-34
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.603145392005279
  episode_reward_mean: 4.938591135138946
  episode_reward_min: 1.8520510516361266
  episodes_this_iter: 270
  episodes_total: 92610
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.501999890431762e-05
          entropy: 9.911148071289062
          entropy_coeff: 0.00015806840383447707
          kl: 0.014822850935161114
          model: {}
          policy_loss: -0.17474699020385742
          total_loss: -0.11806877702474594
          vf_explained_var: 0.9928789138793945
          vf_loss: 0.02447652816772461
    num_agent_steps_sampled: 648270
    num_agent_steps_trained: 648270
    num_steps_sampled: 648270
    num_steps_trained: 648270
  iterations_since_restore: 43
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.530946291560102
    ram_util_percent: 13.61457800511509
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10494187425742622
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1893.8040797273006
    mean_inference_ms: 2.848517737322127
    mean_raw_obs_processing_ms: 204.2447386132976
  time_since_restore: 37295.99880361557
  time_this_iter_s: 562.2205188274384
  time_total_s: 382297.89144444466
  timers:
    learn_throughput: 740.457
    learn_time_ms: 2552.478
    load_throughput: 182946.878
    load_time_ms: 10.331
    sample_throughput: 2.846
    sample_time_ms: 664092.209
    update_time_ms: 30.969
  timestamp: 1633097794
  timesteps_since_restore: 0
  timesteps_total: 648270
  training_iteration: 343
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    343 |           382298 | 648270 |  4.93859 |              7.60315 |              1.85205 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 650160
  custom_metrics: {}
  date: 2021-10-01_07-22-19
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.38333895775963
  episode_reward_mean: 5.012696699775777
  episode_reward_min: 2.360404743841348
  episodes_this_iter: 270
  episodes_total: 92880
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.482999818515964e-05
          entropy: 9.843378067016602
          entropy_coeff: 0.0001570685999467969
          kl: 0.013873660936951637
          model: {}
          policy_loss: -0.19015686213970184
          total_loss: -0.13824895024299622
          vf_explained_var: 0.9938318133354187
          vf_loss: 0.021848082542419434
    num_agent_steps_sampled: 650160
    num_agent_steps_trained: 650160
    num_steps_sampled: 650160
    num_steps_trained: 650160
  iterations_since_restore: 44
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.383958333333333
    ram_util_percent: 13.623541666666664
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10487851392994538
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1877.4567984698867
    mean_inference_ms: 2.8383572867698215
    mean_raw_obs_processing_ms: 204.30779267891634
  time_since_restore: 37641.41288948059
  time_this_iter_s: 345.41408586502075
  time_total_s: 382643.3055303097
  timers:
    learn_throughput: 739.751
    learn_time_ms: 2554.914
    load_throughput: 181530.023
    load_time_ms: 10.412
    sample_throughput: 2.926
    sample_time_ms: 645919.013
    update_time_ms: 30.803
  timestamp: 1633098139
  timesteps_since_restore: 0
  timesteps_total: 650160
  training_iteration: 344
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 24.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    344 |           382643 | 650160 |   5.0127 |              8.38334 |               2.3604 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 652050
  custom_metrics: {}
  date: 2021-10-01_07-45-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.184275282524071
  episode_reward_mean: 4.93063627752953
  episode_reward_min: 1.5466961183094305
  episodes_this_iter: 270
  episodes_total: 93150
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.464000110398047e-05
          entropy: 9.888379096984863
          entropy_coeff: 0.00015606879605911672
          kl: 0.014436871744692326
          model: {}
          policy_loss: -0.1834459751844406
          total_loss: -0.12777043879032135
          vf_explained_var: 0.9930272698402405
          vf_loss: 0.024329831823706627
    num_agent_steps_sampled: 652050
    num_agent_steps_trained: 652050
    num_steps_sampled: 652050
    num_steps_trained: 652050
  iterations_since_restore: 45
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.380356207438449
    ram_util_percent: 13.632477737035098
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1049093918291806
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1886.1944204150088
    mean_inference_ms: 2.8289624025139193
    mean_raw_obs_processing_ms: 204.2808460812827
  time_since_restore: 39014.09739995003
  time_this_iter_s: 1372.6845104694366
  time_total_s: 384015.9900407791
  timers:
    learn_throughput: 740.391
    learn_time_ms: 2552.706
    load_throughput: 180555.9
    load_time_ms: 10.468
    sample_throughput: 2.59
    sample_time_ms: 729729.413
    update_time_ms: 30.71
  timestamp: 1633099512
  timesteps_since_restore: 0
  timesteps_total: 652050
  training_iteration: 345
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    345 |           384016 | 652050 |  4.93064 |              8.18428 |               1.5467 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 653940
  custom_metrics: {}
  date: 2021-10-01_08-05-50
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.8226562019508865
  episode_reward_mean: 4.908227203642729
  episode_reward_min: 1.694228822432406
  episodes_this_iter: 270
  episodes_total: 93420
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.445000038482249e-05
          entropy: 9.946497917175293
          entropy_coeff: 0.00015506900672335178
          kl: 0.01595677249133587
          model: {}
          policy_loss: -0.19951793551445007
          total_loss: -0.1402433216571808
          vf_explained_var: 0.9932149648666382
          vf_loss: 0.02446546033024788
    num_agent_steps_sampled: 653940
    num_agent_steps_trained: 653940
    num_steps_sampled: 653940
    num_steps_trained: 653940
  iterations_since_restore: 46
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.5861788617886186
    ram_util_percent: 13.698606271777004
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10498219921846962
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1901.7083564093496
    mean_inference_ms: 2.8195437486918538
    mean_raw_obs_processing_ms: 204.3009584702331
  time_since_restore: 40252.50156378746
  time_this_iter_s: 1238.4041638374329
  time_total_s: 385254.39420461655
  timers:
    learn_throughput: 739.679
    learn_time_ms: 2555.162
    load_throughput: 181341.493
    load_time_ms: 10.422
    sample_throughput: 2.295
    sample_time_ms: 823472.904
    update_time_ms: 30.34
  timestamp: 1633100750
  timesteps_since_restore: 0
  timesteps_total: 653940
  training_iteration: 346
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    346 |           385254 | 653940 |  4.90823 |              7.82266 |              1.69423 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 655830
  custom_metrics: {}
  date: 2021-10-01_08-16-07
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.911884556564259
  episode_reward_mean: 5.0191811659758905
  episode_reward_min: 2.25333841207057
  episodes_this_iter: 270
  episodes_total: 93690
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.425999966566451e-05
          entropy: 9.799958229064941
          entropy_coeff: 0.0001540692028356716
          kl: 0.0132735725492239
          model: {}
          policy_loss: -0.19961395859718323
          total_loss: -0.1496109515428543
          vf_explained_var: 0.9941970109939575
          vf_loss: 0.02127401903271675
    num_agent_steps_sampled: 655830
    num_agent_steps_trained: 655830
    num_steps_sampled: 655830
    num_steps_trained: 655830
  iterations_since_restore: 47
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.627421236872811
    ram_util_percent: 13.625087514585765
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.104990358169891
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1899.6067678989343
    mean_inference_ms: 2.810200919712641
    mean_raw_obs_processing_ms: 204.38460180290846
  time_since_restore: 40869.37689733505
  time_this_iter_s: 616.8753335475922
  time_total_s: 385871.26953816414
  timers:
    learn_throughput: 740.873
    learn_time_ms: 2551.045
    load_throughput: 181238.258
    load_time_ms: 10.428
    sample_throughput: 2.344
    sample_time_ms: 806427.636
    update_time_ms: 30.344
  timestamp: 1633101367
  timesteps_since_restore: 0
  timesteps_total: 655830
  training_iteration: 347
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    347 |           385871 | 655830 |  5.01918 |              7.91188 |              2.25334 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 657720
  custom_metrics: {}
  date: 2021-10-01_08-31-47
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.715277235311978
  episode_reward_mean: 4.982474876198548
  episode_reward_min: 2.2496702578180625
  episodes_this_iter: 270
  episodes_total: 93960
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.406999894650653e-05
          entropy: 9.827012062072754
          entropy_coeff: 0.00015306939894799143
          kl: 0.01400857511907816
          model: {}
          policy_loss: -0.1822761595249176
          total_loss: -0.13014884293079376
          vf_explained_var: 0.9939312934875488
          vf_loss: 0.021718276664614677
    num_agent_steps_sampled: 657720
    num_agent_steps_trained: 657720
    num_steps_sampled: 657720
    num_steps_trained: 657720
  iterations_since_restore: 48
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 8.417903596021423
    ram_util_percent: 13.638255547054326
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10497003349248359
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1898.6011306293915
    mean_inference_ms: 2.8020007826421094
    mean_raw_obs_processing_ms: 204.43359040891218
  time_since_restore: 41809.729039907455
  time_this_iter_s: 940.352142572403
  time_total_s: 386811.62168073654
  timers:
    learn_throughput: 741.372
    learn_time_ms: 2549.326
    load_throughput: 181715.196
    load_time_ms: 10.401
    sample_throughput: 2.482
    sample_time_ms: 761451.516
    update_time_ms: 29.149
  timestamp: 1633102307
  timesteps_since_restore: 0
  timesteps_total: 657720
  training_iteration: 348
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    348 |           386812 | 657720 |  4.98247 |              7.71528 |              2.24967 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 659610
  custom_metrics: {}
  date: 2021-10-01_08-43-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.773538209663561
  episode_reward_mean: 4.946244657634514
  episode_reward_min: 2.049849561220861
  episodes_this_iter: 270
  episodes_total: 94230
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.387999822734855e-05
          entropy: 9.796472549438477
          entropy_coeff: 0.00015206959506031126
          kl: 0.013727732002735138
          model: {}
          policy_loss: -0.19420266151428223
          total_loss: -0.14188747107982635
          vf_explained_var: 0.9935069680213928
          vf_loss: 0.022531447932124138
    num_agent_steps_sampled: 659610
    num_agent_steps_trained: 659610
    num_steps_sampled: 659610
    num_steps_trained: 659610
  iterations_since_restore: 49
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 13.505949895615865
    ram_util_percent: 10.731837160751567
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10498333404029346
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1897.188935579846
    mean_inference_ms: 2.7929382713935245
    mean_raw_obs_processing_ms: 204.459230995826
  time_since_restore: 42497.92010855675
  time_this_iter_s: 688.191068649292
  time_total_s: 387499.81274938583
  timers:
    learn_throughput: 741.739
    learn_time_ms: 2548.067
    load_throughput: 180511.496
    load_time_ms: 10.47
    sample_throughput: 2.373
    sample_time_ms: 796544.511
    update_time_ms: 30.044
  timestamp: 1633102996
  timesteps_since_restore: 0
  timesteps_total: 659610
  training_iteration: 349
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 26.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    349 |           387500 | 659610 |  4.94624 |              7.77354 |              2.04985 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 661500
  custom_metrics: {}
  date: 2021-10-01_08-57-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.994201370681173
  episode_reward_mean: 5.079915382308615
  episode_reward_min: 2.1051183271188973
  episodes_this_iter: 270
  episodes_total: 94500
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.369000114616938e-05
          entropy: 9.735330581665039
          entropy_coeff: 0.0001510698057245463
          kl: 0.014185400679707527
          model: {}
          policy_loss: -0.19075770676136017
          total_loss: -0.13748085498809814
          vf_explained_var: 0.9938241243362427
          vf_loss: 0.022431431338191032
    num_agent_steps_sampled: 661500
    num_agent_steps_trained: 661500
    num_steps_sampled: 661500
    num_steps_trained: 661500
  iterations_since_restore: 50
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 16.10543103448276
    ram_util_percent: 10.779655172413795
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1049912964075185
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1901.7608473781754
    mean_inference_ms: 2.784851124303405
    mean_raw_obs_processing_ms: 204.48317458488904
  time_since_restore: 43331.780064344406
  time_this_iter_s: 833.8599557876587
  time_total_s: 388333.6727051735
  timers:
    learn_throughput: 742.377
    learn_time_ms: 2545.877
    load_throughput: 180325.485
    load_time_ms: 10.481
    sample_throughput: 2.333
    sample_time_ms: 810241.528
    update_time_ms: 29.734
  timestamp: 1633103830
  timesteps_since_restore: 0
  timesteps_total: 661500
  training_iteration: 350
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 25.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    350 |           388334 | 661500 |  5.07992 |               7.9942 |              2.10512 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 663390
  custom_metrics: {}
  date: 2021-10-01_09-12-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.614597298254763
  episode_reward_mean: 4.966420683683964
  episode_reward_min: 1.9751330173960384
  episodes_this_iter: 270
  episodes_total: 94770
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.35000004270114e-05
          entropy: 9.788090705871582
          entropy_coeff: 0.00015007000183686614
          kl: 0.014859283342957497
          model: {}
          policy_loss: -0.19152678549289703
          total_loss: -0.1356177181005478
          vf_explained_var: 0.9932271242141724
          vf_loss: 0.023526672273874283
    num_agent_steps_sampled: 663390
    num_agent_steps_trained: 663390
    num_steps_sampled: 663390
    num_steps_trained: 663390
  iterations_since_restore: 51
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.42085004009623
    ram_util_percent: 10.76110665597434
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1049490501089622
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1900.7321730181204
    mean_inference_ms: 2.7771214863328355
    mean_raw_obs_processing_ms: 204.50840631379762
  time_since_restore: 44228.53628063202
  time_this_iter_s: 896.7562162876129
  time_total_s: 389230.4289214611
  timers:
    learn_throughput: 741.287
    learn_time_ms: 2549.618
    load_throughput: 179735.508
    load_time_ms: 10.515
    sample_throughput: 2.197
    sample_time_ms: 860393.359
    update_time_ms: 29.66
  timestamp: 1633104726
  timesteps_since_restore: 0
  timesteps_total: 663390
  training_iteration: 351
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 27.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    351 |           389230 | 663390 |  4.96642 |               7.6146 |              1.97513 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 665280
  custom_metrics: {}
  date: 2021-10-01_10-06-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.672052447399257
  episode_reward_mean: 4.9707097979987545
  episode_reward_min: 2.111946205601214
  episodes_this_iter: 270
  episodes_total: 95040
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.330999970785342e-05
          entropy: 9.7865629196167
          entropy_coeff: 0.00014907019794918597
          kl: 0.014622901566326618
          model: {}
          policy_loss: -0.19438436627388
          total_loss: -0.1411164551973343
          vf_explained_var: 0.9938632845878601
          vf_loss: 0.021414000540971756
    num_agent_steps_sampled: 665280
    num_agent_steps_trained: 665280
    num_steps_sampled: 665280
    num_steps_trained: 665280
  iterations_since_restore: 52
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 12.267389380530975
    ram_util_percent: 7.108053097345133
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10499468011075523
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1919.3176082580462
    mean_inference_ms: 2.770097506565334
    mean_raw_obs_processing_ms: 204.5229777907996
  time_since_restore: 47478.78874063492
  time_this_iter_s: 3250.252460002899
  time_total_s: 392480.681381464
  timers:
    learn_throughput: 741.017
    learn_time_ms: 2550.55
    load_throughput: 180079.703
    load_time_ms: 10.495
    sample_throughput: 1.763
    sample_time_ms: 1071776.401
    update_time_ms: 29.359
  timestamp: 1633107977
  timesteps_since_restore: 0
  timesteps_total: 665280
  training_iteration: 352
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    352 |           392481 | 665280 |  4.97071 |              7.67205 |              2.11195 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 667170
  custom_metrics: {}
  date: 2021-10-01_10-24-50
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.820303284381846
  episode_reward_mean: 5.076492124604635
  episode_reward_min: 2.2750186205195218
  episodes_this_iter: 270
  episodes_total: 95310
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.311999898869544e-05
          entropy: 9.755582809448242
          entropy_coeff: 0.0001480703940615058
          kl: 0.012845958583056927
          model: {}
          policy_loss: -0.19590725004673004
          total_loss: -0.1441584974527359
          vf_explained_var: 0.9932771325111389
          vf_loss: 0.02392856776714325
    num_agent_steps_sampled: 667170
    num_agent_steps_trained: 667170
    num_steps_sampled: 667170
    num_steps_trained: 667170
  iterations_since_restore: 53
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.648320413436696
    ram_util_percent: 7.054134366925065
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10495939340805831
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1910.4978390428348
    mean_inference_ms: 2.7622676823884476
    mean_raw_obs_processing_ms: 204.57047675484392
  time_since_restore: 48591.78901100159
  time_this_iter_s: 1113.0002703666687
  time_total_s: 393593.6816518307
  timers:
    learn_throughput: 740.783
    learn_time_ms: 2551.354
    load_throughput: 180950.735
    load_time_ms: 10.445
    sample_throughput: 1.677
    sample_time_ms: 1126853.685
    update_time_ms: 29.193
  timestamp: 1633109090
  timesteps_since_restore: 0
  timesteps_total: 667170
  training_iteration: 353
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    353 |           393594 | 667170 |  5.07649 |               7.8203 |              2.27502 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 669060
  custom_metrics: {}
  date: 2021-10-01_11-04-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.713079132147415
  episode_reward_mean: 4.950628653350093
  episode_reward_min: 2.1146896329774143
  episodes_this_iter: 270
  episodes_total: 95580
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.2929998269537464e-05
          entropy: 9.80351734161377
          entropy_coeff: 0.00014707060472574085
          kl: 0.014143622480332851
          model: {}
          policy_loss: -0.16997049748897552
          total_loss: -0.11859244108200073
          vf_explained_var: 0.9940280914306641
          vf_loss: 0.020598899573087692
    num_agent_steps_sampled: 669060
    num_agent_steps_trained: 669060
    num_steps_sampled: 669060
    num_steps_trained: 669060
  iterations_since_restore: 54
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 15.534855403348555
    ram_util_percent: 6.693546423135465
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10506510355449351
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1931.5735914240704
    mean_inference_ms: 2.754686279909048
    mean_raw_obs_processing_ms: 204.6126734645255
  time_since_restore: 50953.62971544266
  time_this_iter_s: 2361.8407044410706
  time_total_s: 395955.52235627174
  timers:
    learn_throughput: 742.087
    learn_time_ms: 2546.872
    load_throughput: 182283.467
    load_time_ms: 10.368
    sample_throughput: 1.423
    sample_time_ms: 1328500.99
    update_time_ms: 29.483
  timestamp: 1633111452
  timesteps_since_restore: 0
  timesteps_total: 669060
  training_iteration: 354
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    354 |           395956 | 669060 |  4.95063 |              7.71308 |              2.11469 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 670950
  custom_metrics: {}
  date: 2021-10-01_11-19-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.044077328006908
  episode_reward_mean: 5.001817092431059
  episode_reward_min: 2.149932484224134
  episodes_this_iter: 270
  episodes_total: 95850
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.274000118835829e-05
          entropy: 9.68089771270752
          entropy_coeff: 0.00014607080083806068
          kl: 0.013448653742671013
          model: {}
          policy_loss: -0.1934528350830078
          total_loss: -0.14015711843967438
          vf_explained_var: 0.9934045076370239
          vf_loss: 0.024072082713246346
    num_agent_steps_sampled: 670950
    num_agent_steps_trained: 670950
    num_steps_sampled: 670950
    num_steps_trained: 670950
  iterations_since_restore: 55
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 9.816224648985958
    ram_util_percent: 6.008658346333852
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10505239586839989
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1942.6184833590933
    mean_inference_ms: 2.7465460365384473
    mean_raw_obs_processing_ms: 204.6289606488804
  time_since_restore: 51881.90503549576
  time_this_iter_s: 928.2753200531006
  time_total_s: 396883.79767632484
  timers:
    learn_throughput: 742.198
    learn_time_ms: 2546.492
    load_throughput: 182797.537
    load_time_ms: 10.339
    sample_throughput: 1.472
    sample_time_ms: 1284061.457
    update_time_ms: 30.027
  timestamp: 1633112380
  timesteps_since_restore: 0
  timesteps_total: 670950
  training_iteration: 355
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    355 |           396884 | 670950 |  5.00182 |              8.04408 |              2.14993 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 672840
  custom_metrics: {}
  date: 2021-10-01_11-22-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6753074165362944
  episode_reward_mean: 5.007157241345649
  episode_reward_min: 2.0818736102036284
  episodes_this_iter: 270
  episodes_total: 96120
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.255000046920031e-05
          entropy: 9.721841812133789
          entropy_coeff: 0.0001450709969503805
          kl: 0.014642414636909962
          model: {}
          policy_loss: -0.17695806920528412
          total_loss: -0.12165362387895584
          vf_explained_var: 0.9936065673828125
          vf_loss: 0.023357536643743515
    num_agent_steps_sampled: 672840
    num_agent_steps_trained: 672840
    num_steps_sampled: 672840
    num_steps_trained: 672840
  iterations_since_restore: 56
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.293436293436294
    ram_util_percent: 7.2150579150579155
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10503104791013552
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1929.0327683044056
    mean_inference_ms: 2.7380505224589218
    mean_raw_obs_processing_ms: 204.62571134205047
  time_since_restore: 52068.08757019043
  time_this_iter_s: 186.18253469467163
  time_total_s: 397069.9802110195
  timers:
    learn_throughput: 742.062
    learn_time_ms: 2546.957
    load_throughput: 182281.791
    load_time_ms: 10.369
    sample_throughput: 1.603
    sample_time_ms: 1178839.002
    update_time_ms: 30.098
  timestamp: 1633112566
  timesteps_since_restore: 0
  timesteps_total: 672840
  training_iteration: 356
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    356 |           397070 | 672840 |  5.00716 |              7.67531 |              2.08187 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 674730
  custom_metrics: {}
  date: 2021-10-01_11-38-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.087686616373224
  episode_reward_mean: 5.024590723487843
  episode_reward_min: 1.9236827103383174
  episodes_this_iter: 270
  episodes_total: 96390
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.2359999750042334e-05
          entropy: 9.635482788085938
          entropy_coeff: 0.00014407119306270033
          kl: 0.014672316610813141
          model: {}
          policy_loss: -0.17964611947536469
          total_loss: -0.12558461725711823
          vf_explained_var: 0.9938139915466309
          vf_loss: 0.022024303674697876
    num_agent_steps_sampled: 674730
    num_agent_steps_trained: 674730
    num_steps_sampled: 674730
    num_steps_trained: 674730
  iterations_since_restore: 57
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.26486486486487
    ram_util_percent: 7.368843843843845
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10502514021124655
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1933.3545523614182
    mean_inference_ms: 2.7315619083249465
    mean_raw_obs_processing_ms: 204.65601055644092
  time_since_restore: 53025.00770401955
  time_this_iter_s: 956.9201338291168
  time_total_s: 398026.90034484863
  timers:
    learn_throughput: 740.154
    learn_time_ms: 2553.522
    load_throughput: 182038.174
    load_time_ms: 10.382
    sample_throughput: 1.558
    sample_time_ms: 1212837.073
    update_time_ms: 29.958
  timestamp: 1633113523
  timesteps_since_restore: 0
  timesteps_total: 674730
  training_iteration: 357
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    357 |           398027 | 674730 |  5.02459 |              8.08769 |              1.92368 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 676620
  custom_metrics: {}
  date: 2021-10-01_11-48-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.816290675864796
  episode_reward_mean: 5.023301461620128
  episode_reward_min: 1.8763771518847705
  episodes_this_iter: 270
  episodes_total: 96660
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.2169999030884355e-05
          entropy: 9.617002487182617
          entropy_coeff: 0.0001430714037269354
          kl: 0.014897255226969719
          model: {}
          policy_loss: -0.19184592366218567
          total_loss: -0.13756582140922546
          vf_explained_var: 0.9938508868217468
          vf_loss: 0.021718215197324753
    num_agent_steps_sampled: 676620
    num_agent_steps_trained: 676620
    num_steps_sampled: 676620
    num_steps_trained: 676620
  iterations_since_restore: 58
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.436711990111252
    ram_util_percent: 7.421755253399258
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1050259853994041
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1928.8852932186944
    mean_inference_ms: 2.7240581281320595
    mean_raw_obs_processing_ms: 204.63187431160623
  time_since_restore: 53606.396292209625
  time_this_iter_s: 581.3885881900787
  time_total_s: 398608.2889330387
  timers:
    learn_throughput: 740.366
    learn_time_ms: 2552.792
    load_throughput: 181708.948
    load_time_ms: 10.401
    sample_throughput: 1.606
    sample_time_ms: 1176941.551
    update_time_ms: 29.71
  timestamp: 1633114104
  timesteps_since_restore: 0
  timesteps_total: 676620
  training_iteration: 358
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    358 |           398608 | 676620 |   5.0233 |              7.81629 |              1.87638 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 678510
  custom_metrics: {}
  date: 2021-10-01_12-01-49
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.645887881963813
  episode_reward_mean: 5.092887232933614
  episode_reward_min: 1.8966572113664824
  episodes_this_iter: 270
  episodes_total: 96930
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.1979998311726376e-05
          entropy: 9.606805801391602
          entropy_coeff: 0.00014207159983925521
          kl: 0.014091430231928825
          model: {}
          policy_loss: -0.1879105120897293
          total_loss: -0.13481377065181732
          vf_explained_var: 0.9939295053482056
          vf_loss: 0.022359555587172508
    num_agent_steps_sampled: 678510
    num_agent_steps_trained: 678510
    num_steps_sampled: 678510
    num_steps_trained: 678510
  iterations_since_restore: 59
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.07287522603978
    ram_util_percent: 7.08625678119349
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10504823818204224
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1929.3146884309513
    mean_inference_ms: 2.7186566457526746
    mean_raw_obs_processing_ms: 204.61476712002855
  time_since_restore: 54410.866845846176
  time_this_iter_s: 804.4705536365509
  time_total_s: 399412.75948667526
  timers:
    learn_throughput: 740.547
    learn_time_ms: 2552.167
    load_throughput: 182769.721
    load_time_ms: 10.341
    sample_throughput: 1.59
    sample_time_ms: 1188571.552
    update_time_ms: 29.099
  timestamp: 1633114909
  timesteps_since_restore: 0
  timesteps_total: 678510
  training_iteration: 359
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    359 |           399413 | 678510 |  5.09289 |              7.64589 |              1.89666 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 680400
  custom_metrics: {}
  date: 2021-10-01_12-14-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.431527983817452
  episode_reward_mean: 5.037558664979207
  episode_reward_min: 2.263597455040458
  episodes_this_iter: 270
  episodes_total: 97200
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.1790001230547205e-05
          entropy: 9.62986946105957
          entropy_coeff: 0.00014107179595157504
          kl: 0.013198187574744225
          model: {}
          policy_loss: -0.19130904972553253
          total_loss: -0.1427185982465744
          vf_explained_var: 0.9944692850112915
          vf_loss: 0.019881825894117355
    num_agent_steps_sampled: 680400
    num_agent_steps_trained: 680400
    num_steps_sampled: 680400
    num_steps_trained: 680400
  iterations_since_restore: 60
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.581510164569217
    ram_util_percent: 7.061084220716361
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10502853003802734
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1920.8605011655961
    mean_inference_ms: 2.714035113810044
    mean_raw_obs_processing_ms: 204.62781506758174
  time_since_restore: 55154.850497722626
  time_this_iter_s: 743.9836518764496
  time_total_s: 400156.7431385517
  timers:
    learn_throughput: 738.739
    learn_time_ms: 2558.412
    load_throughput: 181995.963
    load_time_ms: 10.385
    sample_throughput: 1.602
    sample_time_ms: 1179577.456
    update_time_ms: 29.094
  timestamp: 1633115653
  timesteps_since_restore: 0
  timesteps_total: 680400
  training_iteration: 360
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    360 |           400157 | 680400 |  5.03756 |              7.43153 |               2.2636 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 682290
  custom_metrics: {}
  date: 2021-10-01_12-46-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.652256046308204
  episode_reward_mean: 5.030612547336571
  episode_reward_min: 1.9573801324102038
  episodes_this_iter: 270
  episodes_total: 97470
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.1600000511389226e-05
          entropy: 9.557317733764648
          entropy_coeff: 0.0001400720066158101
          kl: 0.013409665785729885
          model: {}
          policy_loss: -0.1877189427614212
          total_loss: -0.13857631385326385
          vf_explained_var: 0.994365394115448
          vf_loss: 0.019932450726628304
    num_agent_steps_sampled: 682290
    num_agent_steps_trained: 682290
    num_steps_sampled: 682290
    num_steps_trained: 682290
  iterations_since_restore: 61
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.9256660412758
    ram_util_percent: 7.206303939962478
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10503592727657829
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1933.475762421573
    mean_inference_ms: 2.7096347300707317
    mean_raw_obs_processing_ms: 204.67185093568003
  time_since_restore: 57070.06078958511
  time_this_iter_s: 1915.2102918624878
  time_total_s: 402071.9534304142
  timers:
    learn_throughput: 737.993
    learn_time_ms: 2561.001
    load_throughput: 181982.176
    load_time_ms: 10.386
    sample_throughput: 1.475
    sample_time_ms: 1281419.477
    update_time_ms: 29.153
  timestamp: 1633117568
  timesteps_since_restore: 0
  timesteps_total: 682290
  training_iteration: 361
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    361 |           402072 | 682290 |  5.03061 |              7.65226 |              1.95738 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 684180
  custom_metrics: {}
  date: 2021-10-01_13-18-33
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6815882936320365
  episode_reward_mean: 5.103204445374912
  episode_reward_min: 1.9782545965414249
  episodes_this_iter: 270
  episodes_total: 97740
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.140999979223125e-05
          entropy: 9.534423828125
          entropy_coeff: 0.00013907220272812992
          kl: 0.01436451356858015
          model: {}
          policy_loss: -0.18354804813861847
          total_loss: -0.12902823090553284
          vf_explained_var: 0.9936918616294861
          vf_loss: 0.023121628910303116
    num_agent_steps_sampled: 684180
    num_agent_steps_trained: 684180
    num_steps_sampled: 684180
    num_steps_trained: 684180
  iterations_since_restore: 62
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.074889053254438
    ram_util_percent: 8.060244082840237
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10502087725949999
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1937.8077339781564
    mean_inference_ms: 2.702846229571757
    mean_raw_obs_processing_ms: 204.67258585752774
  time_since_restore: 59014.611221551895
  time_this_iter_s: 1944.5504319667816
  time_total_s: 404016.503862381
  timers:
    learn_throughput: 737.864
    learn_time_ms: 2561.446
    load_throughput: 181448.168
    load_time_ms: 10.416
    sample_throughput: 1.642
    sample_time_ms: 1150849.515
    update_time_ms: 29.044
  timestamp: 1633119513
  timesteps_since_restore: 0
  timesteps_total: 684180
  training_iteration: 362
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    362 |           404017 | 684180 |   5.1032 |              7.68159 |              1.97825 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 686070
  custom_metrics: {}
  date: 2021-10-01_13-27-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.758456244492205
  episode_reward_mean: 4.953596014108471
  episode_reward_min: 2.243679364012405
  episodes_this_iter: 270
  episodes_total: 98010
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.121999907307327e-05
          entropy: 9.674559593200684
          entropy_coeff: 0.00013807239884044975
          kl: 0.013918501324951649
          model: {}
          policy_loss: -0.1688629537820816
          total_loss: -0.11636900901794434
          vf_explained_var: 0.9935699701309204
          vf_loss: 0.02212163247168064
    num_agent_steps_sampled: 686070
    num_agent_steps_trained: 686070
    num_steps_sampled: 686070
    num_steps_trained: 686070
  iterations_since_restore: 63
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.947753530166885
    ram_util_percent: 8.090243902439024
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10500355638077052
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1938.963841787946
    mean_inference_ms: 2.6965543470858306
    mean_raw_obs_processing_ms: 204.68615648565583
  time_since_restore: 59573.878651857376
  time_this_iter_s: 559.267430305481
  time_total_s: 404575.77129268646
  timers:
    learn_throughput: 737.866
    learn_time_ms: 2561.442
    load_throughput: 180735.381
    load_time_ms: 10.457
    sample_throughput: 1.725
    sample_time_ms: 1095476.258
    update_time_ms: 29.031
  timestamp: 1633120072
  timesteps_since_restore: 0
  timesteps_total: 686070
  training_iteration: 363
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    363 |           404576 | 686070 |   4.9536 |              7.75846 |              2.24368 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 687960
  custom_metrics: {}
  date: 2021-10-01_13-40-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.8046006150249525
  episode_reward_mean: 5.055282551364735
  episode_reward_min: 2.2291640740273593
  episodes_this_iter: 270
  episodes_total: 98280
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.102999835391529e-05
          entropy: 9.576689720153809
          entropy_coeff: 0.00013707259495276958
          kl: 0.01261658500880003
          model: {}
          policy_loss: -0.18813107907772064
          total_loss: -0.13795147836208344
          vf_explained_var: 0.9938310980796814
          vf_loss: 0.022750144824385643
    num_agent_steps_sampled: 687960
    num_agent_steps_trained: 687960
    num_steps_sampled: 687960
    num_steps_trained: 687960
  iterations_since_restore: 64
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.73345724907064
    ram_util_percent: 7.890241635687732
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10498333264747851
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1932.5125581124762
    mean_inference_ms: 2.6893979449055383
    mean_raw_obs_processing_ms: 204.72991174048704
  time_since_restore: 60347.638875961304
  time_this_iter_s: 773.7602241039276
  time_total_s: 405349.5315167904
  timers:
    learn_throughput: 736.453
    learn_time_ms: 2566.354
    load_throughput: 181039.997
    load_time_ms: 10.44
    sample_throughput: 2.018
    sample_time_ms: 936663.244
    update_time_ms: 28.875
  timestamp: 1633120846
  timesteps_since_restore: 0
  timesteps_total: 687960
  training_iteration: 364
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    364 |           405350 | 687960 |  5.05528 |               7.8046 |              2.22916 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 689850
  custom_metrics: {}
  date: 2021-10-01_13-46-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.843821251703941
  episode_reward_mean: 4.992444653187093
  episode_reward_min: 1.4771194551584548
  episodes_this_iter: 270
  episodes_total: 98550
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.084000127273612e-05
          entropy: 9.584611892700195
          entropy_coeff: 0.00013607280561700463
          kl: 0.01335859950631857
          model: {}
          policy_loss: -0.18373677134513855
          total_loss: -0.13260841369628906
          vf_explained_var: 0.9936431050300598
          vf_loss: 0.021999986842274666
    num_agent_steps_sampled: 689850
    num_agent_steps_trained: 689850
    num_steps_sampled: 689850
    num_steps_trained: 689850
  iterations_since_restore: 65
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.72959830866807
    ram_util_percent: 7.89957716701903
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10501667751152981
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1926.6924314994794
    mean_inference_ms: 2.6843345237465495
    mean_raw_obs_processing_ms: 204.75109691187956
  time_since_restore: 60687.61646485329
  time_this_iter_s: 339.97758889198303
  time_total_s: 405689.5091056824
  timers:
    learn_throughput: 736.653
    learn_time_ms: 2565.66
    load_throughput: 181779.366
    load_time_ms: 10.397
    sample_throughput: 2.153
    sample_time_ms: 877834.182
    update_time_ms: 28.39
  timestamp: 1633121186
  timesteps_since_restore: 0
  timesteps_total: 689850
  training_iteration: 365
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    365 |           405690 | 689850 |  4.99244 |              7.84382 |              1.47712 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 691740
  custom_metrics: {}
  date: 2021-10-01_13-52-18
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.870291895914523
  episode_reward_mean: 5.0040074591411905
  episode_reward_min: 1.7599813135972362
  episodes_this_iter: 270
  episodes_total: 98820
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.065000055357814e-05
          entropy: 9.458396911621094
          entropy_coeff: 0.00013507300172932446
          kl: 0.013010270893573761
          model: {}
          policy_loss: -0.19072148203849792
          total_loss: -0.13650943338871002
          vf_explained_var: 0.9927554130554199
          vf_loss: 0.02585059218108654
    num_agent_steps_sampled: 691740
    num_agent_steps_trained: 691740
    num_steps_sampled: 691740
    num_steps_trained: 691740
  iterations_since_restore: 66
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.21140529531568
    ram_util_percent: 7.899796334012222
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10503009036257938
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1919.1371685768872
    mean_inference_ms: 2.6787136123703057
    mean_raw_obs_processing_ms: 204.73049058809178
  time_since_restore: 61039.97211408615
  time_this_iter_s: 352.3556492328644
  time_total_s: 406041.86475491524
  timers:
    learn_throughput: 736.592
    learn_time_ms: 2565.87
    load_throughput: 183065.173
    load_time_ms: 10.324
    sample_throughput: 2.113
    sample_time_ms: 894451.005
    update_time_ms: 28.43
  timestamp: 1633121538
  timesteps_since_restore: 0
  timesteps_total: 691740
  training_iteration: 366
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    366 |           406042 | 691740 |  5.00401 |              7.87029 |              1.75998 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 693630
  custom_metrics: {}
  date: 2021-10-01_14-07-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.82639782716622
  episode_reward_mean: 4.981379165058649
  episode_reward_min: 2.445661247333922
  episodes_this_iter: 270
  episodes_total: 99090
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.045999983442016e-05
          entropy: 9.579293251037598
          entropy_coeff: 0.0001340731978416443
          kl: 0.015316301956772804
          model: {}
          policy_loss: -0.183057501912117
          total_loss: -0.12545625865459442
          vf_explained_var: 0.993126392364502
          vf_loss: 0.02399308793246746
    num_agent_steps_sampled: 693630
    num_agent_steps_trained: 693630
    num_steps_sampled: 693630
    num_steps_trained: 693630
  iterations_since_restore: 67
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.06197511664075
    ram_util_percent: 7.900077760497667
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10499859807881849
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1926.7839775101247
    mean_inference_ms: 2.672340390959919
    mean_raw_obs_processing_ms: 204.69193790704972
  time_since_restore: 61964.29576945305
  time_this_iter_s: 924.3236553668976
  time_total_s: 406966.18841028214
  timers:
    learn_throughput: 736.438
    learn_time_ms: 2566.408
    load_throughput: 183398.063
    load_time_ms: 10.305
    sample_throughput: 2.121
    sample_time_ms: 891190.778
    update_time_ms: 28.572
  timestamp: 1633122463
  timesteps_since_restore: 0
  timesteps_total: 693630
  training_iteration: 367
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    367 |           406966 | 693630 |  4.98138 |               7.8264 |              2.44566 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 695520
  custom_metrics: {}
  date: 2021-10-01_14-17-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.59593069818232
  episode_reward_mean: 5.012434945475789
  episode_reward_min: 1.9295800301859218
  episodes_this_iter: 270
  episodes_total: 99360
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.026999911526218e-05
          entropy: 9.642183303833008
          entropy_coeff: 0.00013307339395396411
          kl: 0.013380439952015877
          model: {}
          policy_loss: -0.197116419672966
          total_loss: -0.14915211498737335
          vf_explained_var: 0.9946435689926147
          vf_loss: 0.018765125423669815
    num_agent_steps_sampled: 695520
    num_agent_steps_trained: 695520
    num_steps_sampled: 695520
    num_steps_trained: 695520
  iterations_since_restore: 68
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.424433249370274
    ram_util_percent: 7.900629722921915
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10497234337514533
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1913.6079097367683
    mean_inference_ms: 2.666186467279273
    mean_raw_obs_processing_ms: 204.663941744469
  time_since_restore: 62534.717777729034
  time_this_iter_s: 570.4220082759857
  time_total_s: 407536.6104185581
  timers:
    learn_throughput: 735.512
    learn_time_ms: 2569.639
    load_throughput: 183582.817
    load_time_ms: 10.295
    sample_throughput: 2.123
    sample_time_ms: 890091.203
    update_time_ms: 28.803
  timestamp: 1633123033
  timesteps_since_restore: 0
  timesteps_total: 695520
  training_iteration: 368
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    368 |           407537 | 695520 |  5.01243 |              7.59593 |              1.92958 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 697410
  custom_metrics: {}
  date: 2021-10-01_14-23-38
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.592438055309902
  episode_reward_mean: 4.981383984512837
  episode_reward_min: 2.3972788442246795
  episodes_this_iter: 270
  episodes_total: 99630
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 3.0080000215093605e-05
          entropy: 9.616721153259277
          entropy_coeff: 0.00013207360461819917
          kl: 0.01327964011579752
          model: {}
          policy_loss: -0.17292681336402893
          total_loss: -0.12219572812318802
          vf_explained_var: 0.9936203360557556
          vf_loss: 0.021748511120676994
    num_agent_steps_sampled: 697410
    num_agent_steps_trained: 697410
    num_steps_sampled: 697410
    num_steps_trained: 697410
  iterations_since_restore: 69
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.796828358208955
    ram_util_percent: 7.924067164179103
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10492235959761036
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1904.645109556513
    mean_inference_ms: 2.659462996675016
    mean_raw_obs_processing_ms: 204.5945449746405
  time_since_restore: 62919.95560288429
  time_this_iter_s: 385.2378251552582
  time_total_s: 407921.8482437134
  timers:
    learn_throughput: 735.428
    learn_time_ms: 2569.932
    load_throughput: 184081.594
    load_time_ms: 10.267
    sample_throughput: 2.228
    sample_time_ms: 848167.546
    update_time_ms: 28.417
  timestamp: 1633123418
  timesteps_since_restore: 0
  timesteps_total: 697410
  training_iteration: 369
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    369 |           407922 | 697410 |  4.98138 |              7.59244 |              2.39728 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 699300
  custom_metrics: {}
  date: 2021-10-01_14-31-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.281374453121702
  episode_reward_mean: 5.005630555073035
  episode_reward_min: 2.1720592860652115
  episodes_this_iter: 270
  episodes_total: 99900
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.9889999495935626e-05
          entropy: 9.6076078414917
          entropy_coeff: 0.000131073800730519
          kl: 0.013780491426587105
          model: {}
          policy_loss: -0.186189204454422
          total_loss: -0.13347390294075012
          vf_explained_var: 0.9936736226081848
          vf_loss: 0.0225809495896101
    num_agent_steps_sampled: 699300
    num_agent_steps_trained: 699300
    num_steps_sampled: 699300
    num_steps_trained: 699300
  iterations_since_restore: 70
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.721153846153847
    ram_util_percent: 6.684615384615384
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10487842556213917
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1899.7172697486865
    mean_inference_ms: 2.654248873212714
    mean_raw_obs_processing_ms: 204.5387710176382
  time_since_restore: 63406.05289936066
  time_this_iter_s: 486.09729647636414
  time_total_s: 408407.94554018974
  timers:
    learn_throughput: 736.476
    learn_time_ms: 2566.277
    load_throughput: 185675.177
    load_time_ms: 10.179
    sample_throughput: 2.298
    sample_time_ms: 822382.583
    update_time_ms: 28.794
  timestamp: 1633123905
  timesteps_since_restore: 0
  timesteps_total: 699300
  training_iteration: 370
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    370 |           408408 | 699300 |  5.00563 |              8.28137 |              2.17206 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 701190
  custom_metrics: {}
  date: 2021-10-01_14-38-33
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.69194282069009
  episode_reward_mean: 5.088807593560567
  episode_reward_min: 2.0589065016841586
  episodes_this_iter: 270
  episodes_total: 100170
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.970000059576705e-05
          entropy: 9.454910278320312
          entropy_coeff: 0.00013007399684283882
          kl: 0.014457990415394306
          model: {}
          policy_loss: -0.19907070696353912
          total_loss: -0.14346851408481598
          vf_explained_var: 0.9932767152786255
          vf_loss: 0.023894933983683586
    num_agent_steps_sampled: 701190
    num_agent_steps_trained: 701190
    num_steps_sampled: 701190
    num_steps_trained: 701190
  iterations_since_restore: 71
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.88204225352113
    ram_util_percent: 6.8999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1048255840892923
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1901.1436163484993
    mean_inference_ms: 2.6487272473080914
    mean_raw_obs_processing_ms: 204.50006871744392
  time_since_restore: 63813.97664618492
  time_this_iter_s: 407.9237468242645
  time_total_s: 408815.869287014
  timers:
    learn_throughput: 737.283
    learn_time_ms: 2563.466
    load_throughput: 186093.618
    load_time_ms: 10.156
    sample_throughput: 2.814
    sample_time_ms: 671657.336
    update_time_ms: 28.637
  timestamp: 1633124313
  timesteps_since_restore: 0
  timesteps_total: 701190
  training_iteration: 371
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    371 |           408816 | 701190 |  5.08881 |              7.69194 |              2.05891 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 703080
  custom_metrics: {}
  date: 2021-10-01_14-44-02
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.844188136797531
  episode_reward_mean: 5.059554938830185
  episode_reward_min: 2.107765274317072
  episodes_this_iter: 270
  episodes_total: 100440
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.9509999876609072e-05
          entropy: 9.496963500976562
          entropy_coeff: 0.00012907419295515865
          kl: 0.013287504203617573
          model: {}
          policy_loss: -0.1912342756986618
          total_loss: -0.13875214755535126
          vf_explained_var: 0.9934278726577759
          vf_loss: 0.02343735285103321
    num_agent_steps_sampled: 703080
    num_agent_steps_trained: 703080
    num_steps_sampled: 703080
    num_steps_trained: 703080
  iterations_since_restore: 72
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.81157205240175
    ram_util_percent: 6.900000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1049028690728664
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1894.324145120868
    mean_inference_ms: 2.643408915529481
    mean_raw_obs_processing_ms: 204.48917726831007
  time_since_restore: 64143.02749300003
  time_this_iter_s: 329.05084681510925
  time_total_s: 409144.9201338291
  timers:
    learn_throughput: 737.014
    learn_time_ms: 2564.403
    load_throughput: 186281.22
    load_time_ms: 10.146
    sample_throughput: 3.705
    sample_time_ms: 510105.221
    update_time_ms: 28.516
  timestamp: 1633124642
  timesteps_since_restore: 0
  timesteps_total: 703080
  training_iteration: 372
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    372 |           409145 | 703080 |  5.05955 |              7.84419 |              2.10777 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 704970
  custom_metrics: {}
  date: 2021-10-01_15-02-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.816720218460635
  episode_reward_mean: 4.9312048550680725
  episode_reward_min: 2.122310802072949
  episodes_this_iter: 270
  episodes_total: 100710
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.9319999157451093e-05
          entropy: 9.6141357421875
          entropy_coeff: 0.0001280744036193937
          kl: 0.0147702069953084
          model: {}
          policy_loss: -0.17135222256183624
          total_loss: -0.11694318056106567
          vf_explained_var: 0.9935935139656067
          vf_loss: 0.021991977468132973
    num_agent_steps_sampled: 704970
    num_agent_steps_trained: 704970
    num_steps_sampled: 704970
    num_steps_trained: 704970
  iterations_since_restore: 73
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.20985732814527
    ram_util_percent: 6.991115434500648
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10486309095294094
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1894.7732078774875
    mean_inference_ms: 2.638088766461041
    mean_raw_obs_processing_ms: 204.45653574725918
  time_since_restore: 65252.07092189789
  time_this_iter_s: 1109.0434288978577
  time_total_s: 410253.963562727
  timers:
    learn_throughput: 738.117
    learn_time_ms: 2560.569
    load_throughput: 185012.161
    load_time_ms: 10.216
    sample_throughput: 3.345
    sample_time_ms: 565086.369
    update_time_ms: 28.603
  timestamp: 1633125751
  timesteps_since_restore: 0
  timesteps_total: 704970
  training_iteration: 373
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    373 |           410254 | 704970 |   4.9312 |              7.81672 |              2.12231 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 706860
  custom_metrics: {}
  date: 2021-10-01_15-40-23
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.719848685175087
  episode_reward_mean: 5.010462298913865
  episode_reward_min: 2.0273971338574666
  episodes_this_iter: 270
  episodes_total: 100980
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.9130000257282518e-05
          entropy: 9.424115180969238
          entropy_coeff: 0.00012707459973171353
          kl: 0.014100071974098682
          model: {}
          policy_loss: -0.19399814307689667
          total_loss: -0.14103920757770538
          vf_explained_var: 0.9937897324562073
          vf_loss: 0.022034788504242897
    num_agent_steps_sampled: 706860
    num_agent_steps_trained: 706860
    num_steps_sampled: 706860
    num_steps_trained: 706860
  iterations_since_restore: 74
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.82525348542459
    ram_util_percent: 6.846102661596959
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10481777563789597
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1905.8408390543682
    mean_inference_ms: 2.6328599369097163
    mean_raw_obs_processing_ms: 204.4118238511292
  time_since_restore: 67523.74263548851
  time_this_iter_s: 2271.671713590622
  time_total_s: 412525.6352763176
  timers:
    learn_throughput: 738.044
    learn_time_ms: 2560.821
    load_throughput: 184318.714
    load_time_ms: 10.254
    sample_throughput: 2.644
    sample_time_ms: 714876.788
    update_time_ms: 28.774
  timestamp: 1633128023
  timesteps_since_restore: 0
  timesteps_total: 706860
  training_iteration: 374
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.0/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    374 |           412526 | 706860 |  5.01046 |              7.71985 |               2.0274 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 708750
  custom_metrics: {}
  date: 2021-10-01_15-53-57
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.415163441761464
  episode_reward_mean: 5.019689107494179
  episode_reward_min: 2.1677882620186923
  episodes_this_iter: 270
  episodes_total: 101250
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.893999953812454e-05
          entropy: 9.535223960876465
          entropy_coeff: 0.00012607479584403336
          kl: 0.01420382410287857
          model: {}
          policy_loss: -0.1917928159236908
          total_loss: -0.13814397156238556
          vf_explained_var: 0.9938346743583679
          vf_loss: 0.022492915391921997
    num_agent_steps_sampled: 708750
    num_agent_steps_trained: 708750
    num_steps_sampled: 708750
    num_steps_trained: 708750
  iterations_since_restore: 75
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.83212709620477
    ram_util_percent: 6.814739629302736
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10478785322228114
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1904.1884715307478
    mean_inference_ms: 2.6283143073058883
    mean_raw_obs_processing_ms: 204.4583390705697
  time_since_restore: 68338.21381497383
  time_this_iter_s: 814.471179485321
  time_total_s: 413340.1064558029
  timers:
    learn_throughput: 736.718
    learn_time_ms: 2565.431
    load_throughput: 184159.854
    load_time_ms: 10.263
    sample_throughput: 2.479
    sample_time_ms: 762321.202
    update_time_ms: 28.863
  timestamp: 1633128837
  timesteps_since_restore: 0
  timesteps_total: 708750
  training_iteration: 375
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    375 |           413340 | 708750 |  5.01969 |              8.41516 |              2.16779 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 710640
  custom_metrics: {}
  date: 2021-10-01_16-00-07
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.899890456858061
  episode_reward_mean: 5.063939435220143
  episode_reward_min: 2.277094737591101
  episodes_this_iter: 270
  episodes_total: 101520
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.8750000637955964e-05
          entropy: 9.540618896484375
          entropy_coeff: 0.00012507500650826842
          kl: 0.012602861039340496
          model: {}
          policy_loss: -0.1814708411693573
          total_loss: -0.12882086634635925
          vf_explained_var: 0.9931674599647522
          vf_loss: 0.025132352486252785
    num_agent_steps_sampled: 710640
    num_agent_steps_trained: 710640
    num_steps_sampled: 710640
    num_steps_trained: 710640
  iterations_since_restore: 76
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.01070038910506
    ram_util_percent: 6.900000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10484062139106158
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1891.2104951035685
    mean_inference_ms: 2.623521600510012
    mean_raw_obs_processing_ms: 204.44463357575518
  time_since_restore: 68707.67479848862
  time_this_iter_s: 369.46098351478577
  time_total_s: 413709.5674393177
  timers:
    learn_throughput: 736.195
    learn_time_ms: 2567.256
    load_throughput: 182981.505
    load_time_ms: 10.329
    sample_throughput: 2.474
    sample_time_ms: 764030.009
    update_time_ms: 28.888
  timestamp: 1633129207
  timesteps_since_restore: 0
  timesteps_total: 710640
  training_iteration: 376
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    376 |           413710 | 710640 |  5.06394 |              7.89989 |              2.27709 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 712530
  custom_metrics: {}
  date: 2021-10-01_16-13-56
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.090444239047596
  episode_reward_mean: 5.036984219469197
  episode_reward_min: 2.030842269466002
  episodes_this_iter: 270
  episodes_total: 101790
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.8559999918797985e-05
          entropy: 9.50290584564209
          entropy_coeff: 0.00012407520262058824
          kl: 0.012612385675311089
          model: {}
          policy_loss: -0.18215878307819366
          total_loss: -0.1313703954219818
          vf_explained_var: 0.9936232566833496
          vf_loss: 0.02323487214744091
    num_agent_steps_sampled: 712530
    num_agent_steps_trained: 712530
    num_steps_sampled: 712530
    num_steps_trained: 712530
  iterations_since_restore: 77
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.53414211438475
    ram_util_percent: 6.905632582322356
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10480449614840572
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1895.0685262966488
    mean_inference_ms: 2.6192211750438923
    mean_raw_obs_processing_ms: 204.4223040069197
  time_since_restore: 69537.27190876007
  time_this_iter_s: 829.5971102714539
  time_total_s: 414539.16454958916
  timers:
    learn_throughput: 736.612
    learn_time_ms: 2565.801
    load_throughput: 182979.816
    load_time_ms: 10.329
    sample_throughput: 2.505
    sample_time_ms: 754558.841
    update_time_ms: 28.802
  timestamp: 1633130036
  timesteps_since_restore: 0
  timesteps_total: 712530
  training_iteration: 377
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    377 |           414539 | 712530 |  5.03698 |              8.09044 |              2.03084 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 714420
  custom_metrics: {}
  date: 2021-10-01_16-49-05
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.629339678486834
  episode_reward_mean: 4.961172582645867
  episode_reward_min: 2.2715907183451645
  episodes_this_iter: 270
  episodes_total: 102060
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.8369999199640006e-05
          entropy: 9.563488960266113
          entropy_coeff: 0.00012307539873290807
          kl: 0.012900193221867085
          model: {}
          policy_loss: -0.18194982409477234
          total_loss: -0.13091197609901428
          vf_explained_var: 0.9933311343193054
          vf_loss: 0.022826621308922768
    num_agent_steps_sampled: 714420
    num_agent_steps_trained: 714420
    num_steps_sampled: 714420
    num_steps_trained: 714420
  iterations_since_restore: 78
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.400068166325838
    ram_util_percent: 6.879175187457396
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10475108957247022
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1913.7850161969916
    mean_inference_ms: 2.6146713774665806
    mean_raw_obs_processing_ms: 204.39147904299656
  time_since_restore: 71645.71858501434
  time_this_iter_s: 2108.4466762542725
  time_total_s: 416647.61122584343
  timers:
    learn_throughput: 736.253
    learn_time_ms: 2567.052
    load_throughput: 182177.902
    load_time_ms: 10.374
    sample_throughput: 2.081
    sample_time_ms: 908358.672
    update_time_ms: 28.904
  timestamp: 1633132145
  timesteps_since_restore: 0
  timesteps_total: 714420
  training_iteration: 378
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    378 |           416648 | 714420 |  4.96117 |              7.62934 |              2.27159 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 716310
  custom_metrics: {}
  date: 2021-10-01_16-52-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6202355544806375
  episode_reward_mean: 5.0210980828046266
  episode_reward_min: 2.1974073425894303
  episodes_this_iter: 270
  episodes_total: 102330
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.818000029947143e-05
          entropy: 9.480622291564941
          entropy_coeff: 0.0001220755948452279
          kl: 0.013625601306557655
          model: {}
          policy_loss: -0.18412847816944122
          total_loss: -0.13246864080429077
          vf_explained_var: 0.9938221573829651
          vf_loss: 0.021776381880044937
    num_agent_steps_sampled: 716310
    num_agent_steps_trained: 716310
    num_steps_sampled: 716310
    num_steps_trained: 716310
  iterations_since_restore: 79
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.75650793650794
    ram_util_percent: 6.9
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10470863312164758
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1902.8253276444873
    mean_inference_ms: 2.610772031504359
    mean_raw_obs_processing_ms: 204.39352985686986
  time_since_restore: 71872.82361483574
  time_this_iter_s: 227.10502982139587
  time_total_s: 416874.7162556648
  timers:
    learn_throughput: 735.774
    learn_time_ms: 2568.724
    load_throughput: 181740.609
    load_time_ms: 10.399
    sample_throughput: 2.118
    sample_time_ms: 892543.621
    update_time_ms: 29.149
  timestamp: 1633132372
  timesteps_since_restore: 0
  timesteps_total: 716310
  training_iteration: 379
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    379 |           416875 | 716310 |   5.0211 |              7.62024 |              2.19741 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 718200
  custom_metrics: {}
  date: 2021-10-01_16-57-59
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.31370924878653
  episode_reward_mean: 5.095902280241024
  episode_reward_min: 2.0203330603764647
  episodes_this_iter: 270
  episodes_total: 102600
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.798999958031345e-05
          entropy: 9.400439262390137
          entropy_coeff: 0.00012107579823350534
          kl: 0.013966502621769905
          model: {}
          policy_loss: -0.19216091930866241
          total_loss: -0.1414649337530136
          vf_explained_var: 0.994659960269928
          vf_loss: 0.02001669630408287
    num_agent_steps_sampled: 718200
    num_agent_steps_trained: 718200
    num_steps_sampled: 718200
    num_steps_trained: 718200
  iterations_since_restore: 80
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.99135514018691
    ram_util_percent: 6.987383177570094
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10465647218232664
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1893.6432354592093
    mean_inference_ms: 2.606597115130397
    mean_raw_obs_processing_ms: 204.39000753729832
  time_since_restore: 72180.31395149231
  time_this_iter_s: 307.49033665657043
  time_total_s: 417182.2065923214
  timers:
    learn_throughput: 735.089
    learn_time_ms: 2571.118
    load_throughput: 177801.281
    load_time_ms: 10.63
    sample_throughput: 2.161
    sample_time_ms: 874680.682
    update_time_ms: 29.134
  timestamp: 1633132679
  timesteps_since_restore: 0
  timesteps_total: 718200
  training_iteration: 380
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    380 |           417182 | 718200 |   5.0959 |              8.31371 |              2.02033 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 720090
  custom_metrics: {}
  date: 2021-10-01_17-09-33
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.400059331386528
  episode_reward_mean: 4.963479622462912
  episode_reward_min: 2.007275176797494
  episodes_this_iter: 270
  episodes_total: 102870
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.7800000680144876e-05
          entropy: 9.521090507507324
          entropy_coeff: 0.00012007600162178278
          kl: 0.012461468577384949
          model: {}
          policy_loss: -0.19171825051307678
          total_loss: -0.14267770946025848
          vf_explained_var: 0.9935737252235413
          vf_loss: 0.02179502509534359
    num_agent_steps_sampled: 720090
    num_agent_steps_trained: 720090
    num_steps_sampled: 720090
    num_steps_trained: 720090
  iterations_since_restore: 81
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.67655601659751
    ram_util_percent: 6.997302904564315
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10457869785064233
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1893.2664546929204
    mean_inference_ms: 2.6026953147779883
    mean_raw_obs_processing_ms: 204.3839635925243
  time_since_restore: 72873.98107433319
  time_this_iter_s: 693.6671228408813
  time_total_s: 417875.8737151623
  timers:
    learn_throughput: 734.895
    learn_time_ms: 2571.795
    load_throughput: 177455.801
    load_time_ms: 10.651
    sample_throughput: 2.092
    sample_time_ms: 903254.77
    update_time_ms: 29.265
  timestamp: 1633133373
  timesteps_since_restore: 0
  timesteps_total: 720090
  training_iteration: 381
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    381 |           417876 | 720090 |  4.96348 |              8.40006 |              2.00728 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 721980
  custom_metrics: {}
  date: 2021-10-01_17-11-05
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.565409074765039
  episode_reward_mean: 5.113107522419128
  episode_reward_min: 2.279329284417925
  episodes_this_iter: 270
  episodes_total: 103140
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.7609999960986897e-05
          entropy: 9.409457206726074
          entropy_coeff: 0.0001190761977341026
          kl: 0.013920542784035206
          model: {}
          policy_loss: -0.1941250115633011
          total_loss: -0.13993006944656372
          vf_explained_var: 0.9936171770095825
          vf_loss: 0.023602640256285667
    num_agent_steps_sampled: 721980
    num_agent_steps_trained: 721980
    num_steps_sampled: 721980
    num_steps_trained: 721980
  iterations_since_restore: 82
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.265354330708654
    ram_util_percent: 6.652755905511807
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10450699956259615
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1880.1399179174018
    mean_inference_ms: 2.598549785318286
    mean_raw_obs_processing_ms: 204.38254170893805
  time_since_restore: 72965.21807956696
  time_this_iter_s: 91.23700523376465
  time_total_s: 417967.11072039604
  timers:
    learn_throughput: 735.649
    learn_time_ms: 2569.159
    load_throughput: 177933.779
    load_time_ms: 10.622
    sample_throughput: 2.149
    sample_time_ms: 879477.519
    update_time_ms: 29.352
  timestamp: 1633133465
  timesteps_since_restore: 0
  timesteps_total: 721980
  training_iteration: 382
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    382 |           417967 | 721980 |  5.11311 |              7.56541 |              2.27933 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 723870
  custom_metrics: {}
  date: 2021-10-01_17-21-01
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.833343354794044
  episode_reward_mean: 5.047744160434655
  episode_reward_min: 2.176775965649012
  episodes_this_iter: 270
  episodes_total: 103410
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.741999924182892e-05
          entropy: 9.503738403320312
          entropy_coeff: 0.00011807640112238005
          kl: 0.013307451270520687
          model: {}
          policy_loss: -0.19346792995929718
          total_loss: -0.1426577866077423
          vf_explained_var: 0.9940566420555115
          vf_loss: 0.021616250276565552
    num_agent_steps_sampled: 723870
    num_agent_steps_trained: 723870
    num_steps_sampled: 723870
    num_steps_trained: 723870
  iterations_since_restore: 83
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.126506024096386
    ram_util_percent: 6.82144578313253
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10442876963094407
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1878.0892609266762
    mean_inference_ms: 2.594754888823008
    mean_raw_obs_processing_ms: 204.37117684233328
  time_since_restore: 73561.87687516212
  time_this_iter_s: 596.6587955951691
  time_total_s: 418563.7695159912
  timers:
    learn_throughput: 735.337
    learn_time_ms: 2570.25
    load_throughput: 178162.923
    load_time_ms: 10.608
    sample_throughput: 2.282
    sample_time_ms: 828237.596
    update_time_ms: 29.385
  timestamp: 1633134061
  timesteps_since_restore: 0
  timesteps_total: 723870
  training_iteration: 383
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    383 |           418564 | 723870 |  5.04774 |              7.83334 |              2.17678 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 725760
  custom_metrics: {}
  date: 2021-10-01_17-56-47
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.705088149466954
  episode_reward_mean: 4.97696951384767
  episode_reward_min: 2.6682267417119743
  episodes_this_iter: 270
  episodes_total: 103680
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.7230000341660343e-05
          entropy: 9.401342391967773
          entropy_coeff: 0.00011707659723469988
          kl: 0.013078258372843266
          model: {}
          policy_loss: -0.17953526973724365
          total_loss: -0.12943783402442932
          vf_explained_var: 0.9938875436782837
          vf_loss: 0.021404210478067398
    num_agent_steps_sampled: 725760
    num_agent_steps_trained: 725760
    num_steps_sampled: 725760
    num_steps_trained: 725760
  iterations_since_restore: 84
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.698894101876675
    ram_util_percent: 6.892727882037534
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10439748562073639
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1879.363401482526
    mean_inference_ms: 2.590749050620414
    mean_raw_obs_processing_ms: 204.35407605029107
  time_since_restore: 75707.46609592438
  time_this_iter_s: 2145.589220762253
  time_total_s: 420709.35873675346
  timers:
    learn_throughput: 736.284
    learn_time_ms: 2566.945
    load_throughput: 178556.601
    load_time_ms: 10.585
    sample_throughput: 2.317
    sample_time_ms: 815633.192
    update_time_ms: 29.246
  timestamp: 1633136207
  timesteps_since_restore: 0
  timesteps_total: 725760
  training_iteration: 384
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    384 |           420709 | 725760 |  4.97697 |              7.70509 |              2.66823 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 727650
  custom_metrics: {}
  date: 2021-10-01_18-07-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.296404519833901
  episode_reward_mean: 4.954984004728724
  episode_reward_min: 1.7989518507834417
  episodes_this_iter: 270
  episodes_total: 103950
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.7039999622502364e-05
          entropy: 9.434713363647461
          entropy_coeff: 0.00011607680062297732
          kl: 0.013796030543744564
          model: {}
          policy_loss: -0.19757536053657532
          total_loss: -0.14689752459526062
          vf_explained_var: 0.9941959977149963
          vf_loss: 0.020343920215964317
    num_agent_steps_sampled: 727650
    num_agent_steps_trained: 727650
    num_steps_sampled: 727650
    num_steps_trained: 727650
  iterations_since_restore: 85
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.062100456621
    ram_util_percent: 6.900000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10432181127162503
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1876.8917691556735
    mean_inference_ms: 2.586855240216928
    mean_raw_obs_processing_ms: 204.33147083998622
  time_since_restore: 76337.49667549133
  time_this_iter_s: 630.0305795669556
  time_total_s: 421339.3893163204
  timers:
    learn_throughput: 739.219
    learn_time_ms: 2556.751
    load_throughput: 178318.02
    load_time_ms: 10.599
    sample_throughput: 2.371
    sample_time_ms: 797199.686
    update_time_ms: 29.31
  timestamp: 1633136837
  timesteps_since_restore: 0
  timesteps_total: 727650
  training_iteration: 385
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    385 |           421339 | 727650 |  4.95498 |               8.2964 |              1.79895 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 729540
  custom_metrics: {}
  date: 2021-10-01_18-15-24
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.8452168003782
  episode_reward_mean: 5.034216679658722
  episode_reward_min: 2.5504485867102185
  episodes_this_iter: 270
  episodes_total: 104220
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.685000072233379e-05
          entropy: 9.35504150390625
          entropy_coeff: 0.00011507699673529714
          kl: 0.014395655132830143
          model: {}
          policy_loss: -0.20592084527015686
          total_loss: -0.15019764006137848
          vf_explained_var: 0.9934700131416321
          vf_loss: 0.024004682898521423
    num_agent_steps_sampled: 729540
    num_agent_steps_trained: 729540
    num_steps_sampled: 729540
    num_steps_trained: 729540
  iterations_since_restore: 86
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.50162241887905
    ram_util_percent: 6.919026548672567
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10432133279638905
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1871.8013950791687
    mean_inference_ms: 2.582946275811014
    mean_raw_obs_processing_ms: 204.3327267026689
  time_since_restore: 76824.36403989792
  time_this_iter_s: 486.8673644065857
  time_total_s: 421826.256680727
  timers:
    learn_throughput: 740.936
    learn_time_ms: 2550.826
    load_throughput: 177997.704
    load_time_ms: 10.618
    sample_throughput: 2.336
    sample_time_ms: 808946.263
    update_time_ms: 29.303
  timestamp: 1633137324
  timesteps_since_restore: 0
  timesteps_total: 729540
  training_iteration: 386
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    386 |           421826 | 729540 |  5.03422 |              7.84522 |              2.55045 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 731430
  custom_metrics: {}
  date: 2021-10-01_18-26-32
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.8167291679777415
  episode_reward_mean: 5.033973703124068
  episode_reward_min: 2.806333246298685
  episodes_this_iter: 270
  episodes_total: 104490
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.666000000317581e-05
          entropy: 9.40942096710205
          entropy_coeff: 0.00011407720012357458
          kl: 0.012610363774001598
          model: {}
          policy_loss: -0.19778642058372498
          total_loss: -0.14853107929229736
          vf_explained_var: 0.993929386138916
          vf_loss: 0.0216007512062788
    num_agent_steps_sampled: 731430
    num_agent_steps_trained: 731430
    num_steps_sampled: 731430
    num_steps_trained: 731430
  iterations_since_restore: 87
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.98537634408602
    ram_util_percent: 6.997096774193548
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10427538808859085
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1871.5830691245471
    mean_inference_ms: 2.57871877896124
    mean_raw_obs_processing_ms: 204.32008060562765
  time_since_restore: 77492.84515452385
  time_this_iter_s: 668.4811146259308
  time_total_s: 422494.73779535294
  timers:
    learn_throughput: 741.662
    learn_time_ms: 2548.329
    load_throughput: 178865.614
    load_time_ms: 10.567
    sample_throughput: 2.384
    sample_time_ms: 792836.973
    update_time_ms: 29.548
  timestamp: 1633137992
  timesteps_since_restore: 0
  timesteps_total: 731430
  training_iteration: 387
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    387 |           422495 | 731430 |  5.03397 |              7.81673 |              2.80633 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 733320
  custom_metrics: {}
  date: 2021-10-01_18-30-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.896639781466204
  episode_reward_mean: 4.987882184272865
  episode_reward_min: 2.3302072347048846
  episodes_this_iter: 270
  episodes_total: 104760
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.646999928401783e-05
          entropy: 9.471234321594238
          entropy_coeff: 0.00011307740351185203
          kl: 0.013774358667433262
          model: {}
          policy_loss: -0.18167467415332794
          total_loss: -0.12693491578102112
          vf_explained_var: 0.9930891990661621
          vf_loss: 0.024431023746728897
    num_agent_steps_sampled: 733320
    num_agent_steps_trained: 733320
    num_steps_sampled: 733320
    num_steps_trained: 733320
  iterations_since_restore: 88
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.61592920353982
    ram_util_percent: 7.0
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10423376963340611
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1862.8842128171452
    mean_inference_ms: 2.575639111292924
    mean_raw_obs_processing_ms: 204.30612754157175
  time_since_restore: 77736.18530583382
  time_this_iter_s: 243.34015130996704
  time_total_s: 422738.0779466629
  timers:
    learn_throughput: 742.123
    learn_time_ms: 2546.746
    load_throughput: 180137.401
    load_time_ms: 10.492
    sample_throughput: 3.117
    sample_time_ms: 606329.568
    update_time_ms: 29.457
  timestamp: 1633138236
  timesteps_since_restore: 0
  timesteps_total: 733320
  training_iteration: 388
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    388 |           422738 | 733320 |  4.98788 |              7.89664 |              2.33021 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 735210
  custom_metrics: {}
  date: 2021-10-01_18-33-51
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6929288370317765
  episode_reward_mean: 5.069114870658186
  episode_reward_min: 2.3078217409979795
  episodes_this_iter: 270
  episodes_total: 105030
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.6280000383849256e-05
          entropy: 9.314722061157227
          entropy_coeff: 0.00011207759962417185
          kl: 0.012429085560142994
          model: {}
          policy_loss: -0.1857290118932724
          total_loss: -0.13822536170482635
          vf_explained_var: 0.994259774684906
          vf_loss: 0.020232606679201126
    num_agent_steps_sampled: 735210
    num_agent_steps_trained: 735210
    num_steps_sampled: 735210
    num_steps_trained: 735210
  iterations_since_restore: 89
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.58161764705883
    ram_util_percent: 6.801838235294118
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10416323679280723
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1853.658120824569
    mean_inference_ms: 2.5735087750051444
    mean_raw_obs_processing_ms: 204.27848014518833
  time_since_restore: 77931.6846177578
  time_this_iter_s: 195.4993119239807
  time_total_s: 422933.5772585869
  timers:
    learn_throughput: 742.603
    learn_time_ms: 2545.102
    load_throughput: 179838.669
    load_time_ms: 10.509
    sample_throughput: 3.133
    sample_time_ms: 603171.02
    update_time_ms: 29.154
  timestamp: 1633138431
  timesteps_since_restore: 0
  timesteps_total: 735210
  training_iteration: 389
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    389 |           422934 | 735210 |  5.06911 |              7.69293 |              2.30782 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 737100
  custom_metrics: {}
  date: 2021-10-01_18-55-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.700602445759423
  episode_reward_mean: 5.016632633448531
  episode_reward_min: 2.7792395343525063
  episodes_this_iter: 270
  episodes_total: 105300
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.6089999664691277e-05
          entropy: 9.429282188415527
          entropy_coeff: 0.0001110778030124493
          kl: 0.01206221990287304
          model: {}
          policy_loss: -0.17466816306114197
          total_loss: -0.12754718959331512
          vf_explained_var: 0.9940844178199768
          vf_loss: 0.020689114928245544
    num_agent_steps_sampled: 737100
    num_agent_steps_trained: 737100
    num_steps_sampled: 737100
    num_steps_trained: 737100
  iterations_since_restore: 90
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.9149833518313
    ram_util_percent: 6.855771365149834
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10408577731475889
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1852.6465953154423
    mean_inference_ms: 2.570298566276794
    mean_raw_obs_processing_ms: 204.25005993815208
  time_since_restore: 79227.16215348244
  time_this_iter_s: 1295.47753572464
  time_total_s: 424229.0547943115
  timers:
    learn_throughput: 742.842
    learn_time_ms: 2544.281
    load_throughput: 183234.855
    load_time_ms: 10.315
    sample_throughput: 2.692
    sample_time_ms: 701969.482
    update_time_ms: 30.219
  timestamp: 1633139727
  timesteps_since_restore: 0
  timesteps_total: 737100
  training_iteration: 390
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    390 |           424229 | 737100 |  5.01663 |               7.7006 |              2.77924 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 738990
  custom_metrics: {}
  date: 2021-10-01_19-04-35
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.832610173915408
  episode_reward_mean: 5.049245941704687
  episode_reward_min: 2.2474612661215745
  episodes_this_iter: 270
  episodes_total: 105570
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.59000007645227e-05
          entropy: 9.323817253112793
          entropy_coeff: 0.00011007799912476912
          kl: 0.015018997713923454
          model: {}
          policy_loss: -0.1813390552997589
          total_loss: -0.12397996336221695
          vf_explained_var: 0.9933118224143982
          vf_loss: 0.02417028322815895
    num_agent_steps_sampled: 738990
    num_agent_steps_trained: 738990
    num_steps_sampled: 738990
    num_steps_trained: 738990
  iterations_since_restore: 91
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.241415465268677
    ram_util_percent: 7.000786369593708
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10407743682528892
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1850.5719928783008
    mean_inference_ms: 2.566897250295292
    mean_raw_obs_processing_ms: 204.24084465453484
  time_since_restore: 79775.62467956543
  time_this_iter_s: 548.4625260829926
  time_total_s: 424777.5173203945
  timers:
    learn_throughput: 741.895
    learn_time_ms: 2547.53
    load_throughput: 183120.57
    load_time_ms: 10.321
    sample_throughput: 2.749
    sample_time_ms: 687445.366
    update_time_ms: 30.241
  timestamp: 1633140275
  timesteps_since_restore: 0
  timesteps_total: 738990
  training_iteration: 391
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    391 |           424778 | 738990 |  5.04925 |              7.83261 |              2.24746 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 740880
  custom_metrics: {}
  date: 2021-10-01_19-08-23
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.393937445064978
  episode_reward_mean: 5.052236286977076
  episode_reward_min: 2.015334664617748
  episodes_this_iter: 270
  episodes_total: 105840
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.5710000045364723e-05
          entropy: 9.32985782623291
          entropy_coeff: 0.00010907820251304656
          kl: 0.014105471782386303
          model: {}
          policy_loss: -0.19549934566020966
          total_loss: -0.14183832705020905
          vf_explained_var: 0.9937433004379272
          vf_loss: 0.022544678300619125
    num_agent_steps_sampled: 740880
    num_agent_steps_trained: 740880
    num_steps_sampled: 740880
    num_steps_trained: 740880
  iterations_since_restore: 92
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.25867507886435
    ram_util_percent: 7.799999999999998
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10402543008442232
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1841.5387366909517
    mean_inference_ms: 2.5637411897909925
    mean_raw_obs_processing_ms: 204.2064481030503
  time_since_restore: 80003.60135507584
  time_this_iter_s: 227.9766755104065
  time_total_s: 425005.4939959049
  timers:
    learn_throughput: 739.88
    learn_time_ms: 2554.468
    load_throughput: 182812.713
    load_time_ms: 10.338
    sample_throughput: 2.696
    sample_time_ms: 701112.549
    update_time_ms: 30.199
  timestamp: 1633140503
  timesteps_since_restore: 0
  timesteps_total: 740880
  training_iteration: 392
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    392 |           425005 | 740880 |  5.05224 |              8.39394 |              2.01533 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 742770
  custom_metrics: {}
  date: 2021-10-01_19-18-49
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.298504566824974
  episode_reward_mean: 5.032426069515395
  episode_reward_min: 2.0974076007603664
  episodes_this_iter: 270
  episodes_total: 106110
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.5519999326206744e-05
          entropy: 9.361260414123535
          entropy_coeff: 0.00010807839862536639
          kl: 0.012701300904154778
          model: {}
          policy_loss: -0.19694583117961884
          total_loss: -0.14656628668308258
          vf_explained_var: 0.9937737584114075
          vf_loss: 0.022456098347902298
    num_agent_steps_sampled: 742770
    num_agent_steps_trained: 742770
    num_steps_sampled: 742770
    num_steps_trained: 742770
  iterations_since_restore: 93
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.8073478760046
    ram_util_percent: 7.748220436280136
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10395853384104743
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1840.0328300915544
    mean_inference_ms: 2.559873726446495
    mean_raw_obs_processing_ms: 204.19552622285727
  time_since_restore: 80629.32157945633
  time_this_iter_s: 625.7202243804932
  time_total_s: 425631.2142202854
  timers:
    learn_throughput: 738.586
    learn_time_ms: 2558.944
    load_throughput: 182833.795
    load_time_ms: 10.337
    sample_throughput: 2.685
    sample_time_ms: 704015.151
    update_time_ms: 30.202
  timestamp: 1633141129
  timesteps_since_restore: 0
  timesteps_total: 742770
  training_iteration: 393
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    393 |           425631 | 742770 |  5.03243 |               8.2985 |              2.09741 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 744660
  custom_metrics: {}
  date: 2021-10-01_19-31-48
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.008184538357055
  episode_reward_mean: 5.084413550366949
  episode_reward_min: 2.1471321391044893
  episodes_this_iter: 270
  episodes_total: 106380
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.533000042603817e-05
          entropy: 9.29604434967041
          entropy_coeff: 0.00010707860201364383
          kl: 0.014096210710704327
          model: {}
          policy_loss: -0.18586288392543793
          total_loss: -0.13048461079597473
          vf_explained_var: 0.9933204650878906
          vf_loss: 0.024260738864541054
    num_agent_steps_sampled: 744660
    num_agent_steps_trained: 744660
    num_steps_sampled: 744660
    num_steps_trained: 744660
  iterations_since_restore: 94
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.61491146318732
    ram_util_percent: 7.860018639328984
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10389820784275892
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1833.6975290287253
    mean_inference_ms: 2.556801579821187
    mean_raw_obs_processing_ms: 204.18229115023055
  time_since_restore: 81408.27306103706
  time_this_iter_s: 778.9514815807343
  time_total_s: 426410.16570186615
  timers:
    learn_throughput: 737.118
    learn_time_ms: 2564.041
    load_throughput: 181922.037
    load_time_ms: 10.389
    sample_throughput: 3.331
    sample_time_ms: 567345.803
    update_time_ms: 30.184
  timestamp: 1633141908
  timesteps_since_restore: 0
  timesteps_total: 744660
  training_iteration: 394
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    394 |           426410 | 744660 |  5.08441 |              8.00818 |              2.14713 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 746550
  custom_metrics: {}
  date: 2021-10-01_19-50-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.861653505472175
  episode_reward_mean: 5.050494525179937
  episode_reward_min: 2.3967365847034916
  episodes_this_iter: 270
  episodes_total: 106650
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.513999970688019e-05
          entropy: 9.267038345336914
          entropy_coeff: 0.00010607879812596366
          kl: 0.013482136651873589
          model: {}
          policy_loss: -0.19481337070465088
          total_loss: -0.14296555519104004
          vf_explained_var: 0.993823230266571
          vf_loss: 0.02211686782538891
    num_agent_steps_sampled: 746550
    num_agent_steps_trained: 746550
    num_steps_sampled: 746550
    num_steps_trained: 746550
  iterations_since_restore: 95
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.03474853037231
    ram_util_percent: 7.729849771391248
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10384618066688524
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1833.3310670571632
    mean_inference_ms: 2.5533748594545664
    mean_raw_obs_processing_ms: 204.15770177779885
  time_since_restore: 82507.91585755348
  time_this_iter_s: 1099.6427965164185
  time_total_s: 427509.80849838257
  timers:
    learn_throughput: 734.417
    learn_time_ms: 2573.469
    load_throughput: 166394.17
    load_time_ms: 11.359
    sample_throughput: 3.077
    sample_time_ms: 614296.154
    update_time_ms: 30.209
  timestamp: 1633143008
  timesteps_since_restore: 0
  timesteps_total: 746550
  training_iteration: 395
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 21.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    395 |           427510 | 746550 |  5.05049 |              7.86165 |              2.39674 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 748440
  custom_metrics: {}
  date: 2021-10-01_20-08-07
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.905408246056999
  episode_reward_mean: 5.088966759743238
  episode_reward_min: 2.301689414659739
  episodes_this_iter: 270
  episodes_total: 106920
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.4950000806711614e-05
          entropy: 9.334847450256348
          entropy_coeff: 0.0001050790015142411
          kl: 0.013369319960474968
          model: {}
          policy_loss: -0.17001010477542877
          total_loss: -0.12082042545080185
          vf_explained_var: 0.9944148659706116
          vf_loss: 0.019713610410690308
    num_agent_steps_sampled: 748440
    num_agent_steps_trained: 748440
    num_steps_sampled: 748440
    num_steps_trained: 748440
  iterations_since_restore: 96
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.50093209054594
    ram_util_percent: 7.352729693741678
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10385318973020764
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1835.4407166758465
    mean_inference_ms: 2.549481539625808
    mean_raw_obs_processing_ms: 204.1269206659337
  time_since_restore: 83586.89107871056
  time_this_iter_s: 1078.975221157074
  time_total_s: 428588.78371953964
  timers:
    learn_throughput: 732.623
    learn_time_ms: 2579.772
    load_throughput: 167131.225
    load_time_ms: 11.308
    sample_throughput: 2.806
    sample_time_ms: 673500.926
    update_time_ms: 30.229
  timestamp: 1633144087
  timesteps_since_restore: 0
  timesteps_total: 748440
  training_iteration: 396
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 21.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    396 |           428589 | 748440 |  5.08897 |              7.90541 |              2.30169 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 750330
  custom_metrics: {}
  date: 2021-10-01_20-14-30
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.78639022365327
  episode_reward_mean: 5.039635082983492
  episode_reward_min: 1.9267354710731825
  episodes_this_iter: 270
  episodes_total: 107190
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.4760000087553635e-05
          entropy: 9.262103080749512
          entropy_coeff: 0.00010407919762656093
          kl: 0.01406911015510559
          model: {}
          policy_loss: -0.1756075769662857
          total_loss: -0.12255845963954926
          vf_explained_var: 0.9938826560974121
          vf_loss: 0.021961912512779236
    num_agent_steps_sampled: 750330
    num_agent_steps_trained: 750330
    num_steps_sampled: 750330
    num_steps_trained: 750330
  iterations_since_restore: 97
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.780524344569294
    ram_util_percent: 7.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10378717924572411
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1833.7576739042709
    mean_inference_ms: 2.5470118652207354
    mean_raw_obs_processing_ms: 204.126886589446
  time_since_restore: 83970.2576675415
  time_this_iter_s: 383.3665888309479
  time_total_s: 428972.1503083706
  timers:
    learn_throughput: 725.294
    learn_time_ms: 2605.839
    load_throughput: 166362.044
    load_time_ms: 11.361
    sample_throughput: 2.93
    sample_time_ms: 644963.857
    update_time_ms: 29.91
  timestamp: 1633144470
  timesteps_since_restore: 0
  timesteps_total: 750330
  training_iteration: 397
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 21.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    397 |           428972 | 750330 |  5.03964 |              7.78639 |              1.92674 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 752220
  custom_metrics: {}
  date: 2021-10-01_20-32-54
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.092075950446315
  episode_reward_mean: 4.985287482327237
  episode_reward_min: 2.272149678160238
  episodes_this_iter: 270
  episodes_total: 107460
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.4569999368395656e-05
          entropy: 9.351335525512695
          entropy_coeff: 0.00010307940101483837
          kl: 0.01273316890001297
          model: {}
          policy_loss: -0.19414523243904114
          total_loss: -0.13963282108306885
          vf_explained_var: 0.9923132061958313
          vf_loss: 0.026468584313988686
    num_agent_steps_sampled: 752220
    num_agent_steps_trained: 752220
    num_steps_sampled: 752220
    num_steps_trained: 752220
  iterations_since_restore: 98
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 8.316753585397656
    ram_util_percent: 6.290221642764016
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10372468102969061
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1835.4694014760178
    mean_inference_ms: 2.5440032499319067
    mean_raw_obs_processing_ms: 204.11815988033803
  time_since_restore: 85073.60082268715
  time_this_iter_s: 1103.3431551456451
  time_total_s: 430075.49346351624
  timers:
    learn_throughput: 725.601
    learn_time_ms: 2604.738
    load_throughput: 165187.887
    load_time_ms: 11.442
    sample_throughput: 2.586
    sample_time_ms: 730964.323
    update_time_ms: 30.18
  timestamp: 1633145574
  timesteps_since_restore: 0
  timesteps_total: 752220
  training_iteration: 398
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    398 |           430075 | 752220 |  4.98529 |              8.09208 |              2.27215 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 754110
  custom_metrics: {}
  date: 2021-10-01_20-48-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.735518656193824
  episode_reward_mean: 5.046474139904434
  episode_reward_min: 2.190364226755328
  episodes_this_iter: 270
  episodes_total: 107730
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.438000046822708e-05
          entropy: 9.186686515808105
          entropy_coeff: 0.0001020795971271582
          kl: 0.012908666394650936
          model: {}
          policy_loss: -0.18783578276634216
          total_loss: -0.13585753738880157
          vf_explained_var: 0.993453323841095
          vf_loss: 0.023508446291089058
    num_agent_steps_sampled: 754110
    num_agent_steps_trained: 754110
    num_steps_sampled: 754110
    num_steps_trained: 754110
  iterations_since_restore: 99
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.2027301092043683
    ram_util_percent: 5.8999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10365899431902516
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1834.0392554400614
    mean_inference_ms: 2.543037427742143
    mean_raw_obs_processing_ms: 204.16240377634205
  time_since_restore: 85996.08029961586
  time_this_iter_s: 922.4794769287109
  time_total_s: 430997.97294044495
  timers:
    learn_throughput: 725.99
    learn_time_ms: 2603.34
    load_throughput: 165555.296
    load_time_ms: 11.416
    sample_throughput: 2.352
    sample_time_ms: 803663.078
    update_time_ms: 30.114
  timestamp: 1633146496
  timesteps_since_restore: 0
  timesteps_total: 754110
  training_iteration: 399
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    399 |           430998 | 754110 |  5.04647 |              7.73552 |              2.19036 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 756000
  custom_metrics: {}
  date: 2021-10-01_20-55-47
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.830795820975956
  episode_reward_mean: 5.126669907578453
  episode_reward_min: 2.21438490813268
  episodes_this_iter: 270
  episodes_total: 108000
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.4189999749069102e-05
          entropy: 9.314337730407715
          entropy_coeff: 0.00010107980051543564
          kl: 0.013483643531799316
          model: {}
          policy_loss: -0.19588090479373932
          total_loss: -0.14528052508831024
          vf_explained_var: 0.9944638013839722
          vf_loss: 0.020824432373046875
    num_agent_steps_sampled: 756000
    num_agent_steps_trained: 756000
    num_steps_sampled: 756000
    num_steps_trained: 756000
  iterations_since_restore: 100
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 4.632591414944356
    ram_util_percent: 5.8999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10361026790438482
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1833.3737543882442
    mean_inference_ms: 2.5419800105107386
    mean_raw_obs_processing_ms: 204.16951023067216
  time_since_restore: 86447.11832904816
  time_this_iter_s: 451.03802943229675
  time_total_s: 431449.01096987724
  timers:
    learn_throughput: 727.869
    learn_time_ms: 2596.621
    load_throughput: 166160.491
    load_time_ms: 11.375
    sample_throughput: 2.628
    sample_time_ms: 719227.023
    update_time_ms: 28.986
  timestamp: 1633146947
  timesteps_since_restore: 0
  timesteps_total: 756000
  training_iteration: 400
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    400 |           431449 | 756000 |  5.12667 |               7.8308 |              2.21438 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 757890
  custom_metrics: {}
  date: 2021-10-01_21-13-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.766170309865696
  episode_reward_mean: 4.997737474186162
  episode_reward_min: 2.4050662059803654
  episodes_this_iter: 270
  episodes_total: 108270
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.4000000848900527e-05
          entropy: 9.235955238342285
          entropy_coeff: 0.00010007999662775546
          kl: 0.014294085092842579
          model: {}
          policy_loss: -0.19949495792388916
          total_loss: -0.14652982354164124
          vf_explained_var: 0.9937663674354553
          vf_loss: 0.021325793117284775
    num_agent_steps_sampled: 757890
    num_agent_steps_trained: 757890
    num_steps_sampled: 757890
    num_steps_trained: 757890
  iterations_since_restore: 101
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.528224687933426
    ram_util_percent: 5.8999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1035892936887268
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1832.003103447684
    mean_inference_ms: 2.5412758244470908
    mean_raw_obs_processing_ms: 204.21377463952953
  time_since_restore: 87484.24261140823
  time_this_iter_s: 1037.124282360077
  time_total_s: 432486.1352522373
  timers:
    learn_throughput: 731.054
    learn_time_ms: 2585.308
    load_throughput: 162839.084
    load_time_ms: 11.607
    sample_throughput: 2.461
    sample_time_ms: 768104.364
    update_time_ms: 29.412
  timestamp: 1633147984
  timesteps_since_restore: 0
  timesteps_total: 757890
  training_iteration: 401
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    401 |           432486 | 757890 |  4.99774 |              7.76617 |              2.40507 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 759780
  custom_metrics: {}
  date: 2021-10-01_21-18-18
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.952333194741832
  episode_reward_mean: 5.065412933830137
  episode_reward_min: 2.268334848473953
  episodes_this_iter: 270
  episodes_total: 108540
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.3810000129742548e-05
          entropy: 9.255609512329102
          entropy_coeff: 9.90802000160329e-05
          kl: 0.012652273289859295
          model: {}
          policy_loss: -0.1871362030506134
          total_loss: -0.1383461207151413
          vf_explained_var: 0.9943316578865051
          vf_loss: 0.02088366635143757
    num_agent_steps_sampled: 759780
    num_agent_steps_trained: 759780
    num_steps_sampled: 759780
    num_steps_trained: 759780
  iterations_since_restore: 102
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 2.1016091954022986
    ram_util_percent: 5.9
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10361988943713117
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1826.7075936265264
    mean_inference_ms: 2.5408400052678197
    mean_raw_obs_processing_ms: 204.2616259646052
  time_since_restore: 87797.4533264637
  time_this_iter_s: 313.2107150554657
  time_total_s: 432799.3459672928
  timers:
    learn_throughput: 734.413
    learn_time_ms: 2573.485
    load_throughput: 162545.922
    load_time_ms: 11.627
    sample_throughput: 2.434
    sample_time_ms: 776639.26
    update_time_ms: 29.658
  timestamp: 1633148298
  timesteps_since_restore: 0
  timesteps_total: 759780
  training_iteration: 402
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    402 |           432799 | 759780 |  5.06541 |              7.95233 |              2.26833 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 761670
  custom_metrics: {}
  date: 2021-10-01_21-51-44
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.471446177745436
  episode_reward_mean: 5.06857743418781
  episode_reward_min: 2.231451294275606
  episodes_this_iter: 270
  episodes_total: 108810
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.361999941058457e-05
          entropy: 9.208780288696289
          entropy_coeff: 9.808040340431035e-05
          kl: 0.012382264249026775
          model: {}
          policy_loss: -0.19433633983135223
          total_loss: -0.1439041644334793
          vf_explained_var: 0.9935433268547058
          vf_loss: 0.023127062246203423
    num_agent_steps_sampled: 761670
    num_agent_steps_trained: 761670
    num_steps_sampled: 761670
    num_steps_trained: 761670
  iterations_since_restore: 103
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 0.9578192252510761
    ram_util_percent: 5.899999999999999
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1035855972455704
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1833.0313802423284
    mean_inference_ms: 2.539855890892598
    mean_raw_obs_processing_ms: 204.301754986064
  time_since_restore: 89803.4440715313
  time_this_iter_s: 2005.9907450675964
  time_total_s: 434805.3367123604
  timers:
    learn_throughput: 736.131
    learn_time_ms: 2567.479
    load_throughput: 159882.589
    load_time_ms: 11.821
    sample_throughput: 2.066
    sample_time_ms: 914671.428
    update_time_ms: 30.257
  timestamp: 1633150304
  timesteps_since_restore: 0
  timesteps_total: 761670
  training_iteration: 403
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    403 |           434805 | 761670 |  5.06858 |              8.47145 |              2.23145 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 763560
  custom_metrics: {}
  date: 2021-10-01_21-56-42
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.877236555829015
  episode_reward_mean: 5.022046002819667
  episode_reward_min: 2.3387332708364243
  episodes_this_iter: 270
  episodes_total: 109080
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.3430000510415994e-05
          entropy: 9.20328426361084
          entropy_coeff: 9.708059951663017e-05
          kl: 0.013569985516369343
          model: {}
          policy_loss: -0.1806793361902237
          total_loss: -0.12809957563877106
          vf_explained_var: 0.9935994744300842
          vf_loss: 0.02255910076200962
    num_agent_steps_sampled: 763560
    num_agent_steps_trained: 763560
    num_steps_sampled: 763560
    num_steps_trained: 763560
  iterations_since_restore: 104
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 3.1615942028985504
    ram_util_percent: 5.8999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10355664286228858
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1829.6445201768665
    mean_inference_ms: 2.5385364470022114
    mean_raw_obs_processing_ms: 204.33091804945414
  time_since_restore: 90101.64600372314
  time_this_iter_s: 298.20193219184875
  time_total_s: 435103.53864455223
  timers:
    learn_throughput: 737.87
    learn_time_ms: 2561.426
    load_throughput: 159655.575
    load_time_ms: 11.838
    sample_throughput: 2.181
    sample_time_ms: 866602.623
    update_time_ms: 30.531
  timestamp: 1633150602
  timesteps_since_restore: 0
  timesteps_total: 763560
  training_iteration: 404
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    404 |           435104 | 763560 |  5.02205 |              7.87724 |              2.33873 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 765450
  custom_metrics: {}
  date: 2021-10-01_22-21-57
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.088007466281255
  episode_reward_mean: 5.0772603407725425
  episode_reward_min: 2.4425821286773046
  episodes_this_iter: 270
  episodes_total: 109350
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.3239999791258015e-05
          entropy: 9.154295921325684
          entropy_coeff: 9.608080290490761e-05
          kl: 0.013610090129077435
          model: {}
          policy_loss: -0.18746808171272278
          total_loss: -0.13571226596832275
          vf_explained_var: 0.9940235018730164
          vf_loss: 0.021629871800541878
    num_agent_steps_sampled: 765450
    num_agent_steps_trained: 765450
    num_steps_sampled: 765450
    num_steps_trained: 765450
  iterations_since_restore: 105
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 0.7197437114380636
    ram_util_percent: 5.8999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1035292084898945
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1837.1829151167778
    mean_inference_ms: 2.5368622105284344
    mean_raw_obs_processing_ms: 204.33052167295284
  time_since_restore: 91617.1389298439
  time_this_iter_s: 1515.492926120758
  time_total_s: 436619.031570673
  timers:
    learn_throughput: 738.497
    learn_time_ms: 2559.253
    load_throughput: 172555.133
    load_time_ms: 10.953
    sample_throughput: 2.081
    sample_time_ms: 908190.671
    update_time_ms: 30.882
  timestamp: 1633152117
  timesteps_since_restore: 0
  timesteps_total: 765450
  training_iteration: 405
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    405 |           436619 | 765450 |  5.07726 |              8.08801 |              2.44258 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 767340
  custom_metrics: {}
  date: 2021-10-01_22-47-03
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.644920243899342
  episode_reward_mean: 4.9984962619479925
  episode_reward_min: 2.04713004784568
  episodes_this_iter: 270
  episodes_total: 109620
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.305000089108944e-05
          entropy: 9.102629661560059
          entropy_coeff: 9.508099901722744e-05
          kl: 0.01235619094222784
          model: {}
          policy_loss: -0.19202296435832977
          total_loss: -0.14261896908283234
          vf_explained_var: 0.9936937093734741
          vf_loss: 0.022120529785752296
    num_agent_steps_sampled: 767340
    num_agent_steps_trained: 767340
    num_steps_sampled: 767340
    num_steps_trained: 767340
  iterations_since_restore: 106
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 0.8920172084130021
    ram_util_percent: 5.8999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10350014001209105
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1837.2998395259285
    mean_inference_ms: 2.535527807650297
    mean_raw_obs_processing_ms: 204.35952077409638
  time_since_restore: 93122.6708381176
  time_this_iter_s: 1505.531908273697
  time_total_s: 438124.5634789467
  timers:
    learn_throughput: 739.247
    learn_time_ms: 2556.655
    load_throughput: 169782.965
    load_time_ms: 11.132
    sample_throughput: 1.988
    sample_time_ms: 950847.889
    update_time_ms: 31.309
  timestamp: 1633153623
  timesteps_since_restore: 0
  timesteps_total: 767340
  training_iteration: 406
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    406 |           438125 | 767340 |   4.9985 |              7.64492 |              2.04713 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 769230
  custom_metrics: {}
  date: 2021-10-01_22-51-01
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6559103581342764
  episode_reward_mean: 5.033562470327191
  episode_reward_min: 1.6994377276533914
  episodes_this_iter: 270
  episodes_total: 109890
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.286000017193146e-05
          entropy: 9.148567199707031
          entropy_coeff: 9.408120240550488e-05
          kl: 0.0119421212002635
          model: {}
          policy_loss: -0.18278400599956512
          total_loss: -0.13211974501609802
          vf_explained_var: 0.9934028387069702
          vf_loss: 0.024319320917129517
    num_agent_steps_sampled: 769230
    num_agent_steps_trained: 769230
    num_steps_sampled: 769230
    num_steps_trained: 769230
  iterations_since_restore: 107
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 3.406948640483383
    ram_util_percent: 5.899999999999999
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10347285864601947
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1830.2554852955484
    mean_inference_ms: 2.533656753844664
    mean_raw_obs_processing_ms: 204.38227874408634
  time_since_restore: 93360.34863591194
  time_this_iter_s: 237.67779779434204
  time_total_s: 438362.241276741
  timers:
    learn_throughput: 747.226
    learn_time_ms: 2529.357
    load_throughput: 169806.605
    load_time_ms: 11.13
    sample_throughput: 2.019
    sample_time_ms: 936305.873
    update_time_ms: 31.486
  timestamp: 1633153861
  timesteps_since_restore: 0
  timesteps_total: 769230
  training_iteration: 407
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    407 |           438362 | 769230 |  5.03356 |              7.65591 |              1.69944 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 771120
  custom_metrics: {}
  date: 2021-10-01_22-53-19
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.7224216414601425
  episode_reward_mean: 5.085219655097604
  episode_reward_min: 2.1161179716241705
  episodes_this_iter: 270
  episodes_total: 110160
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.266999945277348e-05
          entropy: 9.095624923706055
          entropy_coeff: 9.308139851782471e-05
          kl: 0.015211637131869793
          model: {}
          policy_loss: -0.18950627744197845
          total_loss: -0.13405285775661469
          vf_explained_var: 0.9941889643669128
          vf_loss: 0.0216460432857275
    num_agent_steps_sampled: 771120
    num_agent_steps_trained: 771120
    num_steps_sampled: 771120
    num_steps_trained: 771120
  iterations_since_restore: 108
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.145026178010473
    ram_util_percent: 5.899999999999998
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10343796036525975
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1821.6305493485554
    mean_inference_ms: 2.532595606002664
    mean_raw_obs_processing_ms: 204.41638783537095
  time_since_restore: 93498.1968934536
  time_this_iter_s: 137.8482575416565
  time_total_s: 438500.0895342827
  timers:
    learn_throughput: 746.554
    learn_time_ms: 2531.633
    load_throughput: 168865.407
    load_time_ms: 11.192
    sample_throughput: 2.251
    sample_time_ms: 839753.706
    update_time_ms: 31.779
  timestamp: 1633153999
  timesteps_since_restore: 0
  timesteps_total: 771120
  training_iteration: 408
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.4/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    408 |           438500 | 771120 |  5.08522 |              7.72242 |              2.11612 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 773010
  custom_metrics: {}
  date: 2021-10-01_23-05-34
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.934734379097838
  episode_reward_mean: 5.103300753908148
  episode_reward_min: 2.3735864833141886
  episodes_this_iter: 270
  episodes_total: 110430
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.2480000552604906e-05
          entropy: 9.080592155456543
          entropy_coeff: 9.208160190610215e-05
          kl: 0.012613642029464245
          model: {}
          policy_loss: -0.17542104423046112
          total_loss: -0.12649130821228027
          vf_explained_var: 0.9944815635681152
          vf_loss: 0.021030442789196968
    num_agent_steps_sampled: 773010
    num_agent_steps_trained: 773010
    num_steps_sampled: 773010
    num_steps_trained: 773010
  iterations_since_restore: 109
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 3.6325513196480936
    ram_util_percent: 5.9
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1033974739375782
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1823.30753778792
    mean_inference_ms: 2.532252855034201
    mean_raw_obs_processing_ms: 204.4435153622449
  time_since_restore: 94233.60230016708
  time_this_iter_s: 735.4054067134857
  time_total_s: 439235.49494099617
  timers:
    learn_throughput: 746.467
    learn_time_ms: 2531.926
    load_throughput: 168005.056
    load_time_ms: 11.25
    sample_throughput: 2.302
    sample_time_ms: 821045.769
    update_time_ms: 32.291
  timestamp: 1633154734
  timesteps_since_restore: 0
  timesteps_total: 773010
  training_iteration: 409
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    409 |           439235 | 773010 |   5.1033 |              7.93473 |              2.37359 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 774900
  custom_metrics: {}
  date: 2021-10-01_23-13-57
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.651240600948512
  episode_reward_mean: 5.079953214786096
  episode_reward_min: 2.080097948635623
  episodes_this_iter: 270
  episodes_total: 110700
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.2289999833446927e-05
          entropy: 9.042813301086426
          entropy_coeff: 9.108179801842198e-05
          kl: 0.013080266304314137
          model: {}
          policy_loss: -0.17224882543087006
          total_loss: -0.1226462572813034
          vf_explained_var: 0.9942947030067444
          vf_loss: 0.020627737045288086
    num_agent_steps_sampled: 774900
    num_agent_steps_trained: 774900
    num_steps_sampled: 774900
    num_steps_trained: 774900
  iterations_since_restore: 110
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 3.2577968526466377
    ram_util_percent: 5.900143061516451
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10336096761754238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1818.046770356666
    mean_inference_ms: 2.531236658690655
    mean_raw_obs_processing_ms: 204.47413320424712
  time_since_restore: 94736.55532813072
  time_this_iter_s: 502.9530279636383
  time_total_s: 439738.4479689598
  timers:
    learn_throughput: 745.832
    learn_time_ms: 2534.082
    load_throughput: 165889.967
    load_time_ms: 11.393
    sample_throughput: 2.287
    sample_time_ms: 826234.501
    update_time_ms: 32.151
  timestamp: 1633155237
  timesteps_since_restore: 0
  timesteps_total: 774900
  training_iteration: 410
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    410 |           439738 | 774900 |  5.07995 |              7.65124 |               2.0801 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 776790
  custom_metrics: {}
  date: 2021-10-01_23-27-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.397544704753175
  episode_reward_mean: 5.116728597226521
  episode_reward_min: 2.1830631599082055
  episodes_this_iter: 270
  episodes_total: 110970
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.209999911428895e-05
          entropy: 9.09399700164795
          entropy_coeff: 9.008200140669942e-05
          kl: 0.012263186275959015
          model: {}
          policy_loss: -0.19010496139526367
          total_loss: -0.1405530422925949
          vf_explained_var: 0.9939267039299011
          vf_loss: 0.022434035316109657
    num_agent_steps_sampled: 776790
    num_agent_steps_trained: 776790
    num_steps_sampled: 776790
    num_steps_trained: 776790
  iterations_since_restore: 111
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.584043517679057
    ram_util_percent: 5.928014505893018
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1033602831859709
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1817.044801708629
    mean_inference_ms: 2.530085867013735
    mean_raw_obs_processing_ms: 204.49150318842183
  time_since_restore: 95529.28320860863
  time_this_iter_s: 792.7278804779053
  time_total_s: 440531.1758494377
  timers:
    learn_throughput: 744.285
    learn_time_ms: 2539.35
    load_throughput: 167218.657
    load_time_ms: 11.303
    sample_throughput: 2.357
    sample_time_ms: 801789.563
    update_time_ms: 32.029
  timestamp: 1633156030
  timesteps_since_restore: 0
  timesteps_total: 776790
  training_iteration: 411
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    411 |           440531 | 776790 |  5.11673 |              8.39754 |              2.18306 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 778680
  custom_metrics: {}
  date: 2021-10-01_23-51-37
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.846785022864426
  episode_reward_mean: 5.16974060025127
  episode_reward_min: 2.2084221511057294
  episodes_this_iter: 270
  episodes_total: 111240
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.1910000214120373e-05
          entropy: 8.999292373657227
          entropy_coeff: 8.908219751901925e-05
          kl: 0.013071194291114807
          model: {}
          policy_loss: -0.17388400435447693
          total_loss: -0.12072356790304184
          vf_explained_var: 0.9936922788619995
          vf_loss: 0.02418428100645542
    num_agent_steps_sampled: 778680
    num_agent_steps_trained: 778680
    num_steps_sampled: 778680
    num_steps_trained: 778680
  iterations_since_restore: 112
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 0.7395782246199116
    ram_util_percent: 5.904070622854341
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1033536891965621
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1817.0636105711317
    mean_inference_ms: 2.528827418839321
    mean_raw_obs_processing_ms: 204.48638832136052
  time_since_restore: 96996.54466962814
  time_this_iter_s: 1467.261461019516
  time_total_s: 441998.43731045723
  timers:
    learn_throughput: 742.176
    learn_time_ms: 2546.567
    load_throughput: 165209.576
    load_time_ms: 11.44
    sample_throughput: 2.061
    sample_time_ms: 917187.083
    update_time_ms: 32.199
  timestamp: 1633157497
  timesteps_since_restore: 0
  timesteps_total: 778680
  training_iteration: 412
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    412 |           441998 | 778680 |  5.16974 |              7.84679 |              2.20842 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 780570
  custom_metrics: {}
  date: 2021-10-02_00-03-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.888933384896969
  episode_reward_mean: 5.085563396574851
  episode_reward_min: 1.8595452188663522
  episodes_this_iter: 270
  episodes_total: 111510
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.1719999494962394e-05
          entropy: 9.061878204345703
          entropy_coeff: 8.808240090729669e-05
          kl: 0.01325924601405859
          model: {}
          policy_loss: -0.19412852823734283
          total_loss: -0.14354552328586578
          vf_explained_var: 0.994206428527832
          vf_loss: 0.021174976602196693
    num_agent_steps_sampled: 780570
    num_agent_steps_trained: 780570
    num_steps_sampled: 780570
    num_steps_trained: 780570
  iterations_since_restore: 113
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 1.1459803921568625
    ram_util_percent: 5.900000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10331607869566108
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1811.9160996113517
    mean_inference_ms: 2.5281293293837432
    mean_raw_obs_processing_ms: 204.51272008170304
  time_since_restore: 97731.23742985725
  time_this_iter_s: 734.6927602291107
  time_total_s: 442733.13007068634
  timers:
    learn_throughput: 742.959
    learn_time_ms: 2543.88
    load_throughput: 167428.444
    load_time_ms: 11.288
    sample_throughput: 2.392
    sample_time_ms: 790059.607
    update_time_ms: 31.695
  timestamp: 1633158232
  timesteps_since_restore: 0
  timesteps_total: 780570
  training_iteration: 413
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    413 |           442733 | 780570 |  5.08556 |              7.88893 |              1.85955 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 782460
  custom_metrics: {}
  date: 2021-10-02_00-07-21
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.6921815390310915
  episode_reward_mean: 5.081162790560596
  episode_reward_min: 2.182270994454187
  episodes_this_iter: 270
  episodes_total: 111780
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.153000059479382e-05
          entropy: 9.02212142944336
          entropy_coeff: 8.708259701961651e-05
          kl: 0.012951409444212914
          model: {}
          policy_loss: -0.19253088533878326
          total_loss: -0.14144529402256012
          vf_explained_var: 0.993798553943634
          vf_loss: 0.02236633375287056
    num_agent_steps_sampled: 782460
    num_agent_steps_trained: 782460
    num_steps_sampled: 782460
    num_steps_trained: 782460
  iterations_since_restore: 114
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 5.284536082474226
    ram_util_percent: 5.899999999999999
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10336419282490017
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1806.30663017906
    mean_inference_ms: 2.5266481315623235
    mean_raw_obs_processing_ms: 204.54288476827057
  time_since_restore: 97940.27298569679
  time_this_iter_s: 209.03555583953857
  time_total_s: 442942.1656265259
  timers:
    learn_throughput: 742.831
    learn_time_ms: 2544.32
    load_throughput: 167156.599
    load_time_ms: 11.307
    sample_throughput: 2.42
    sample_time_ms: 781142.718
    update_time_ms: 31.806
  timestamp: 1633158441
  timesteps_since_restore: 0
  timesteps_total: 782460
  training_iteration: 414
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    414 |           442942 | 782460 |  5.08116 |              7.69218 |              2.18227 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 784350
  custom_metrics: {}
  date: 2021-10-02_00-16-19
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.824467393668441
  episode_reward_mean: 5.101570777941448
  episode_reward_min: 2.0753894539086475
  episodes_this_iter: 270
  episodes_total: 112050
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.133999987563584e-05
          entropy: 8.98230266571045
          entropy_coeff: 8.608280040789396e-05
          kl: 0.013010979630053043
          model: {}
          policy_loss: -0.18514443933963776
          total_loss: -0.13375553488731384
          vf_explained_var: 0.9938507080078125
          vf_loss: 0.022521523758769035
    num_agent_steps_sampled: 784350
    num_agent_steps_trained: 784350
    num_steps_sampled: 784350
    num_steps_trained: 784350
  iterations_since_restore: 115
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 3.497459893048129
    ram_util_percent: 6.244919786096255
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10332325413262325
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1803.7511666210191
    mean_inference_ms: 2.525455020917082
    mean_raw_obs_processing_ms: 204.5615936834615
  time_since_restore: 98478.42575979233
  time_this_iter_s: 538.1527740955353
  time_total_s: 443480.3184006214
  timers:
    learn_throughput: 743.991
    learn_time_ms: 2540.354
    load_throughput: 167511.94
    load_time_ms: 11.283
    sample_throughput: 2.766
    sample_time_ms: 683413.513
    update_time_ms: 31.579
  timestamp: 1633158979
  timesteps_since_restore: 0
  timesteps_total: 784350
  training_iteration: 415
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 35.3/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    415 |           443480 | 784350 |  5.10157 |              7.82447 |              2.07539 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 786240
  custom_metrics: {}
  date: 2021-10-02_00-19-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.461455802461003
  episode_reward_mean: 5.1574033024629475
  episode_reward_min: 2.316264003052096
  episodes_this_iter: 270
  episodes_total: 112320
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.114999915647786e-05
          entropy: 8.899421691894531
          entropy_coeff: 8.508299652021378e-05
          kl: 0.012380993925035
          model: {}
          policy_loss: -0.1794481724500656
          total_loss: -0.12930329144001007
          vf_explained_var: 0.9939704537391663
          vf_loss: 0.02269662357866764
    num_agent_steps_sampled: 786240
    num_agent_steps_trained: 786240
    num_steps_sampled: 786240
    num_steps_trained: 786240
  iterations_since_restore: 116
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 7.25495867768595
    ram_util_percent: 13.300000000000004
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10327363299508381
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1796.0163020109524
    mean_inference_ms: 2.5239674668706047
    mean_raw_obs_processing_ms: 204.5803846353819
  time_since_restore: 98652.4366402626
  time_this_iter_s: 174.01088047027588
  time_total_s: 443654.3292810917
  timers:
    learn_throughput: 743.43
    learn_time_ms: 2542.272
    load_throughput: 169588.28
    load_time_ms: 11.145
    sample_throughput: 3.435
    sample_time_ms: 550259.451
    update_time_ms: 31.602
  timestamp: 1633159153
  timesteps_since_restore: 0
  timesteps_total: 786240
  training_iteration: 416
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 35.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    416 |           443654 | 786240 |   5.1574 |              8.46146 |              2.31626 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 788130
  custom_metrics: {}
  date: 2021-10-02_00-36-37
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.02119106237407
  episode_reward_mean: 5.161346269860905
  episode_reward_min: 2.26510703521143
  episodes_this_iter: 270
  episodes_total: 112590
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.0960000256309286e-05
          entropy: 8.938759803771973
          entropy_coeff: 8.408319990849122e-05
          kl: 0.012360751628875732
          model: {}
          policy_loss: -0.16846130788326263
          total_loss: -0.11848674714565277
          vf_explained_var: 0.9940606951713562
          vf_loss: 0.02256680093705654
    num_agent_steps_sampled: 788130
    num_agent_steps_trained: 788130
    num_steps_sampled: 788130
    num_steps_trained: 788130
  iterations_since_restore: 117
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 4.613227146814404
    ram_util_percent: 14.021537396121886
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10323009849136121
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1797.7378525168356
    mean_inference_ms: 2.5230919302184662
    mean_raw_obs_processing_ms: 204.59716365563276
  time_since_restore: 99696.09081172943
  time_this_iter_s: 1043.6541714668274
  time_total_s: 444697.9834525585
  timers:
    learn_throughput: 742.741
    learn_time_ms: 2544.628
    load_throughput: 168074.873
    load_time_ms: 11.245
    sample_throughput: 2.996
    sample_time_ms: 630853.919
    update_time_ms: 32.019
  timestamp: 1633160197
  timesteps_since_restore: 0
  timesteps_total: 788130
  training_iteration: 417
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 39.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    417 |           444698 | 788130 |  5.16135 |              8.02119 |              2.26511 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 790020
  custom_metrics: {}
  date: 2021-10-02_00-43-05
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.884110762346491
  episode_reward_mean: 5.1790064699726415
  episode_reward_min: 2.145160776821842
  episodes_this_iter: 270
  episodes_total: 112860
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.0769999537151307e-05
          entropy: 8.932406425476074
          entropy_coeff: 8.308340329676867e-05
          kl: 0.012999004684388638
          model: {}
          policy_loss: -0.18320809304714203
          total_loss: -0.13193035125732422
          vf_explained_var: 0.9940919876098633
          vf_loss: 0.02240653708577156
    num_agent_steps_sampled: 790020
    num_agent_steps_trained: 790020
    num_steps_sampled: 790020
    num_steps_trained: 790020
  iterations_since_restore: 118
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 6.691280148423006
    ram_util_percent: 14.900000000000004
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10319793839465442
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1794.844275345037
    mean_inference_ms: 2.5215761684660385
    mean_raw_obs_processing_ms: 204.5878061571185
  time_since_restore: 100083.87428808212
  time_this_iter_s: 387.78347635269165
  time_total_s: 445085.7669289112
  timers:
    learn_throughput: 742.593
    learn_time_ms: 2545.135
    load_throughput: 168454.893
    load_time_ms: 11.22
    sample_throughput: 2.882
    sample_time_ms: 655847.442
    update_time_ms: 31.621
  timestamp: 1633160585
  timesteps_since_restore: 0
  timesteps_total: 790020
  training_iteration: 418
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 39.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    418 |           445086 | 790020 |  5.17901 |              7.88411 |              2.14516 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 791910
  custom_metrics: {}
  date: 2021-10-02_00-56-39
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.810828488607065
  episode_reward_mean: 5.0848621047163824
  episode_reward_min: 2.111106517815658
  episodes_this_iter: 270
  episodes_total: 113130
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.058000063698273e-05
          entropy: 8.937609672546387
          entropy_coeff: 8.208359940908849e-05
          kl: 0.011885441839694977
          model: {}
          policy_loss: -0.1840198040008545
          total_loss: -0.1360655128955841
          vf_explained_var: 0.9941058158874512
          vf_loss: 0.02161143347620964
    num_agent_steps_sampled: 791910
    num_agent_steps_trained: 791910
    num_steps_sampled: 791910
    num_steps_trained: 791910
  iterations_since_restore: 119
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 13.604416961130744
    ram_util_percent: 15.385247349823324
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10316853585866484
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1807.0008947556394
    mean_inference_ms: 2.5203953110524675
    mean_raw_obs_processing_ms: 204.60869948927387
  time_since_restore: 100898.32391428947
  time_this_iter_s: 814.4496262073517
  time_total_s: 445900.21655511856
  timers:
    learn_throughput: 741.482
    learn_time_ms: 2548.949
    load_throughput: 168835.916
    load_time_ms: 11.194
    sample_throughput: 2.847
    sample_time_ms: 663748.57
    update_time_ms: 31.375
  timestamp: 1633161399
  timesteps_since_restore: 0
  timesteps_total: 791910
  training_iteration: 419
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 47.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    419 |           445900 | 791910 |  5.08486 |              7.81083 |              2.11111 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 793800
  custom_metrics: {}
  date: 2021-10-02_01-45-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.05861048470113
  episode_reward_mean: 5.08253198776545
  episode_reward_min: 1.8703321994089985
  episodes_this_iter: 270
  episodes_total: 113400
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.0389999917824753e-05
          entropy: 8.961252212524414
          entropy_coeff: 8.108380279736593e-05
          kl: 0.012897881679236889
          model: {}
          policy_loss: -0.16884338855743408
          total_loss: -0.12125319987535477
          vf_explained_var: 0.9947565197944641
          vf_loss: 0.018933819606900215
    num_agent_steps_sampled: 793800
    num_agent_steps_trained: 793800
    num_steps_sampled: 793800
    num_steps_trained: 793800
  iterations_since_restore: 120
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 9.991845811712379
    ram_util_percent: 17.975240919199408
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1031552770572894
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1814.0708858517642
    mean_inference_ms: 2.5195871233758895
    mean_raw_obs_processing_ms: 204.61937116531774
  time_since_restore: 103809.05012965202
  time_this_iter_s: 2910.726215362549
  time_total_s: 448810.9427704811
  timers:
    learn_throughput: 740.386
    learn_time_ms: 2552.723
    load_throughput: 167839.651
    load_time_ms: 11.261
    sample_throughput: 2.09
    sample_time_ms: 904521.325
    update_time_ms: 31.658
  timestamp: 1633164310
  timesteps_since_restore: 0
  timesteps_total: 793800
  training_iteration: 420
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 46.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    420 |           448811 | 793800 |  5.08253 |              8.05861 |              1.87033 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 795690
  custom_metrics: {}
  date: 2021-10-02_02-08-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.470598274265301
  episode_reward_mean: 5.092174376530213
  episode_reward_min: 2.3033210438037997
  episodes_this_iter: 270
  episodes_total: 113670
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.0199999198666774e-05
          entropy: 8.869677543640137
          entropy_coeff: 8.008399890968576e-05
          kl: 0.01150821428745985
          model: {}
          policy_loss: -0.19010262191295624
          total_loss: -0.13940724730491638
          vf_explained_var: 0.9931429028511047
          vf_loss: 0.025188561528921127
    num_agent_steps_sampled: 795690
    num_agent_steps_trained: 795690
    num_steps_sampled: 795690
    num_steps_trained: 795690
  iterations_since_restore: 121
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 10.176545733265202
    ram_util_percent: 17.900051098620338
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10315274093029447
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1819.266736383687
    mean_inference_ms: 2.518583687617976
    mean_raw_obs_processing_ms: 204.64926556187433
  time_since_restore: 105215.27445030212
  time_this_iter_s: 1406.2243206501007
  time_total_s: 450217.1670911312
  timers:
    learn_throughput: 739.996
    learn_time_ms: 2554.068
    load_throughput: 170904.873
    load_time_ms: 11.059
    sample_throughput: 1.957
    sample_time_ms: 965869.821
    update_time_ms: 31.481
  timestamp: 1633165716
  timesteps_since_restore: 0
  timesteps_total: 795690
  training_iteration: 421
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 46.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    421 |           450217 | 795690 |  5.09217 |               8.4706 |              2.30332 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 797580
  custom_metrics: {}
  date: 2021-10-02_02-16-19
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.71440059380143
  episode_reward_mean: 5.077795779672605
  episode_reward_min: 2.0570429810109347
  episodes_this_iter: 270
  episodes_total: 113940
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 2.00100002984982e-05
          entropy: 8.808023452758789
          entropy_coeff: 7.90842022979632e-05
          kl: 0.012420500628650188
          model: {}
          policy_loss: -0.17931529879570007
          total_loss: -0.1299627274274826
          vf_explained_var: 0.9940182566642761
          vf_loss: 0.021753666922450066
    num_agent_steps_sampled: 797580
    num_agent_steps_trained: 797580
    num_steps_sampled: 797580
    num_steps_trained: 797580
  iterations_since_restore: 122
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 13.133592534992227
    ram_util_percent: 17.900000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.103135703457034
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1816.2273376363078
    mean_inference_ms: 2.5176629199009892
    mean_raw_obs_processing_ms: 204.66802810358266
  time_since_restore: 105677.98687505722
  time_this_iter_s: 462.71242475509644
  time_total_s: 450679.8795158863
  timers:
    learn_throughput: 740.392
    learn_time_ms: 2552.701
    load_throughput: 158619.877
    load_time_ms: 11.915
    sample_throughput: 2.184
    sample_time_ms: 865415.556
    update_time_ms: 31.219
  timestamp: 1633166179
  timesteps_since_restore: 0
  timesteps_total: 797580
  training_iteration: 422
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 46.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    422 |           450680 | 797580 |   5.0778 |               7.7144 |              2.05704 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 799470
  custom_metrics: {}
  date: 2021-10-02_02-31-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.869608204482339
  episode_reward_mean: 5.105364057805312
  episode_reward_min: 2.3922824730058148
  episodes_this_iter: 270
  episodes_total: 114210
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.981999957934022e-05
          entropy: 8.81414794921875
          entropy_coeff: 7.808439841028303e-05
          kl: 0.012697756290435791
          model: {}
          policy_loss: -0.19534771144390106
          total_loss: -0.14521348476409912
          vf_explained_var: 0.9941578507423401
          vf_loss: 0.021895404905080795
    num_agent_steps_sampled: 799470
    num_agent_steps_trained: 799470
    num_steps_sampled: 799470
    num_steps_trained: 799470
  iterations_since_restore: 123
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 10.340698619008936
    ram_util_percent: 17.900000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10312246063226518
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1817.0202515679334
    mean_inference_ms: 2.5163659952884108
    mean_raw_obs_processing_ms: 204.69789445760267
  time_since_restore: 106562.51235032082
  time_this_iter_s: 884.5254752635956
  time_total_s: 451564.4049911499
  timers:
    learn_throughput: 739.572
    learn_time_ms: 2555.533
    load_throughput: 158897.125
    load_time_ms: 11.894
    sample_throughput: 2.147
    sample_time_ms: 880396.99
    update_time_ms: 31.162
  timestamp: 1633167064
  timesteps_since_restore: 0
  timesteps_total: 799470
  training_iteration: 423
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 46.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    423 |           451564 | 799470 |  5.10536 |              7.86961 |              2.39228 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 801360
  custom_metrics: {}
  date: 2021-10-02_03-11-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.718974939172254
  episode_reward_mean: 5.078220861237893
  episode_reward_min: 1.9797983574409446
  episodes_this_iter: 270
  episodes_total: 114480
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.9630000679171644e-05
          entropy: 8.681270599365234
          entropy_coeff: 7.708460179856047e-05
          kl: 0.013506735675036907
          model: {}
          policy_loss: -0.15871287882328033
          total_loss: -0.10470640659332275
          vf_explained_var: 0.9933525323867798
          vf_loss: 0.023905621841549873
    num_agent_steps_sampled: 801360
    num_agent_steps_trained: 801360
    num_steps_sampled: 801360
    num_steps_trained: 801360
  iterations_since_restore: 124
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 16.267582742316783
    ram_util_percent: 15.39110520094563
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10309432975752612
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1826.051736977586
    mean_inference_ms: 2.515204053089922
    mean_raw_obs_processing_ms: 204.71907104062393
  time_since_restore: 108995.10349750519
  time_this_iter_s: 2432.591147184372
  time_total_s: 453996.9961383343
  timers:
    learn_throughput: 736.005
    learn_time_ms: 2567.917
    load_throughput: 159602.537
    load_time_ms: 11.842
    sample_throughput: 1.714
    sample_time_ms: 1102740.006
    update_time_ms: 30.737
  timestamp: 1633169496
  timesteps_since_restore: 0
  timesteps_total: 801360
  training_iteration: 424
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    424 |           453997 | 801360 |  5.07822 |              7.71897 |               1.9798 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 803250
  custom_metrics: {}
  date: 2021-10-02_03-54-14
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.821457897055479
  episode_reward_mean: 5.000957154014225
  episode_reward_min: 2.204687005249299
  episodes_this_iter: 270
  episodes_total: 114750
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.9439999960013665e-05
          entropy: 8.78132438659668
          entropy_coeff: 7.60847979108803e-05
          kl: 0.01384005881845951
          model: {}
          policy_loss: -0.20414474606513977
          total_loss: -0.14995287358760834
          vf_explained_var: 0.9933794736862183
          vf_loss: 0.02333059348165989
    num_agent_steps_sampled: 803250
    num_agent_steps_trained: 803250
    num_steps_sampled: 803250
    num_steps_trained: 803250
  iterations_since_restore: 125
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.09435234616465
    ram_util_percent: 6.200000000000003
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10307973130699351
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1835.2174084821515
    mean_inference_ms: 2.5139264422786094
    mean_raw_obs_processing_ms: 204.70498000178304
  time_since_restore: 111552.6485877037
  time_this_iter_s: 2557.545090198517
  time_total_s: 456554.5412285328
  timers:
    learn_throughput: 732.31
    learn_time_ms: 2580.875
    load_throughput: 160591.266
    load_time_ms: 11.769
    sample_throughput: 1.449
    sample_time_ms: 1304665.517
    update_time_ms: 30.514
  timestamp: 1633172054
  timesteps_since_restore: 0
  timesteps_total: 803250
  training_iteration: 425
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    425 |           456555 | 803250 |  5.00096 |              7.82146 |              2.20469 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 805140
  custom_metrics: {}
  date: 2021-10-02_04-00-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.030444176611168
  episode_reward_mean: 5.107228184583619
  episode_reward_min: 2.266098003220964
  episodes_this_iter: 270
  episodes_total: 115020
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.9249999240855686e-05
          entropy: 8.69874095916748
          entropy_coeff: 7.508500129915774e-05
          kl: 0.013034077361226082
          model: {}
          policy_loss: -0.18576230108737946
          total_loss: -0.1336497813463211
          vf_explained_var: 0.9937124252319336
          vf_loss: 0.023072410374879837
    num_agent_steps_sampled: 805140
    num_agent_steps_trained: 805140
    num_steps_sampled: 805140
    num_steps_trained: 805140
  iterations_since_restore: 126
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.33457249070632
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10307738168356413
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1836.176930061231
    mean_inference_ms: 2.5115094651656586
    mean_raw_obs_processing_ms: 204.6872787327497
  time_since_restore: 111938.68940615654
  time_this_iter_s: 386.0408184528351
  time_total_s: 456940.5820469856
  timers:
    learn_throughput: 731.075
    learn_time_ms: 2585.234
    load_throughput: 160538.58
    load_time_ms: 11.773
    sample_throughput: 1.425
    sample_time_ms: 1325864.537
    update_time_ms: 29.902
  timestamp: 1633172440
  timesteps_since_restore: 0
  timesteps_total: 805140
  training_iteration: 426
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    426 |           456941 | 805140 |  5.10723 |              8.03044 |               2.2661 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 807030
  custom_metrics: {}
  date: 2021-10-02_04-09-37
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.624498869521572
  episode_reward_mean: 5.0908058292411225
  episode_reward_min: 1.9910301377672255
  episodes_this_iter: 270
  episodes_total: 115290
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.906000034068711e-05
          entropy: 8.661870956420898
          entropy_coeff: 7.408519741147757e-05
          kl: 0.012132174335420132
          model: {}
          policy_loss: -0.17586268484592438
          total_loss: -0.1270795613527298
          vf_explained_var: 0.9939804077148438
          vf_loss: 0.02178623154759407
    num_agent_steps_sampled: 807030
    num_agent_steps_trained: 807030
    num_steps_sampled: 807030
    num_steps_trained: 807030
  iterations_since_restore: 127
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.39264705882353
    ram_util_percent: 6.185160427807488
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10305254105920211
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1836.0586672176878
    mean_inference_ms: 2.50924836513891
    mean_raw_obs_processing_ms: 204.67931857510234
  time_since_restore: 112476.16322493553
  time_this_iter_s: 537.4738187789917
  time_total_s: 457478.0558657646
  timers:
    learn_throughput: 729.33
    learn_time_ms: 2591.42
    load_throughput: 161829.837
    load_time_ms: 11.679
    sample_throughput: 1.482
    sample_time_ms: 1275241.511
    update_time_ms: 29.307
  timestamp: 1633172977
  timesteps_since_restore: 0
  timesteps_total: 807030
  training_iteration: 427
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    427 |           457478 | 807030 |  5.09081 |               7.6245 |              1.99103 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 808920
  custom_metrics: {}
  date: 2021-10-02_04-17-50
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.410292745274953
  episode_reward_mean: 5.100797946292244
  episode_reward_min: 1.7993560345184778
  episodes_this_iter: 270
  episodes_total: 115560
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.8869999621529132e-05
          entropy: 8.778122901916504
          entropy_coeff: 7.308540079975501e-05
          kl: 0.012149051763117313
          model: {}
          policy_loss: -0.18034912645816803
          total_loss: -0.13370555639266968
          vf_explained_var: 0.994471549987793
          vf_loss: 0.019608089700341225
    num_agent_steps_sampled: 808920
    num_agent_steps_trained: 808920
    num_steps_sampled: 808920
    num_steps_trained: 808920
  iterations_since_restore: 128
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.17456140350877
    ram_util_percent: 6.1000000000000005
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10300850405778453
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1834.0269711752844
    mean_inference_ms: 2.5069856443726444
    mean_raw_obs_processing_ms: 204.6765801020648
  time_since_restore: 112968.17787098885
  time_this_iter_s: 492.0146460533142
  time_total_s: 457970.07051181793
  timers:
    learn_throughput: 727.39
    learn_time_ms: 2598.329
    load_throughput: 163326.23
    load_time_ms: 11.572
    sample_throughput: 1.47
    sample_time_ms: 1285658.368
    update_time_ms: 29.177
  timestamp: 1633173470
  timesteps_since_restore: 0
  timesteps_total: 808920
  training_iteration: 428
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    428 |           457970 | 808920 |   5.1008 |              8.41029 |              1.79936 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 810810
  custom_metrics: {}
  date: 2021-10-02_04-20-35
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.578059333831141
  episode_reward_mean: 5.021853704031139
  episode_reward_min: 2.013432199706093
  episodes_this_iter: 270
  episodes_total: 115830
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.8680000721360557e-05
          entropy: 8.668055534362793
          entropy_coeff: 7.208559691207483e-05
          kl: 0.012829666957259178
          model: {}
          policy_loss: -0.16815508902072906
          total_loss: -0.1160028725862503
          vf_explained_var: 0.9934633374214172
          vf_loss: 0.023549476638436317
    num_agent_steps_sampled: 810810
    num_agent_steps_trained: 810810
    num_steps_sampled: 810810
    num_steps_trained: 810810
  iterations_since_restore: 129
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.428138528138525
    ram_util_percent: 6.1203463203463215
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10297360932649363
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1826.9226335209125
    mean_inference_ms: 2.5045449306292027
    mean_raw_obs_processing_ms: 204.65480044693967
  time_since_restore: 113133.72658085823
  time_this_iter_s: 165.54870986938477
  time_total_s: 458135.6192216873
  timers:
    learn_throughput: 726.075
    learn_time_ms: 2603.038
    load_throughput: 163225.341
    load_time_ms: 11.579
    sample_throughput: 1.548
    sample_time_ms: 1220763.71
    update_time_ms: 28.913
  timestamp: 1633173635
  timesteps_since_restore: 0
  timesteps_total: 810810
  training_iteration: 429
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    429 |           458136 | 810810 |  5.02185 |              7.57806 |              2.01343 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 812700
  custom_metrics: {}
  date: 2021-10-02_04-29-55
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.62253659775676
  episode_reward_mean: 5.0211462923804415
  episode_reward_min: 2.273247148557449
  episodes_this_iter: 270
  episodes_total: 116100
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.8490000002202578e-05
          entropy: 8.634404182434082
          entropy_coeff: 7.108580030035228e-05
          kl: 0.01233160961419344
          model: {}
          policy_loss: -0.18012641370296478
          total_loss: -0.12717987596988678
          vf_explained_var: 0.9928044080734253
          vf_loss: 0.025467364117503166
    num_agent_steps_sampled: 812700
    num_agent_steps_trained: 812700
    num_steps_sampled: 812700
    num_steps_trained: 812700
  iterations_since_restore: 130
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.87650834403081
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10293172039916701
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1821.6860889896302
    mean_inference_ms: 2.502500620295053
    mean_raw_obs_processing_ms: 204.63373475948876
  time_since_restore: 113693.79173970222
  time_this_iter_s: 560.0651588439941
  time_total_s: 458695.6843805313
  timers:
    learn_throughput: 726.315
    learn_time_ms: 2602.176
    load_throughput: 165228.859
    load_time_ms: 11.439
    sample_throughput: 1.917
    sample_time_ms: 985699.77
    update_time_ms: 28.636
  timestamp: 1633174195
  timesteps_since_restore: 0
  timesteps_total: 812700
  training_iteration: 430
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    430 |           458696 | 812700 |  5.02115 |              7.62254 |              2.27325 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 814590
  custom_metrics: {}
  date: 2021-10-02_04-38-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.589876119665589
  episode_reward_mean: 5.125691808183376
  episode_reward_min: 2.238728925635729
  episodes_this_iter: 270
  episodes_total: 116370
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.82999992830446e-05
          entropy: 8.644366264343262
          entropy_coeff: 7.00859964126721e-05
          kl: 0.012659099884331226
          model: {}
          policy_loss: -0.17354951798915863
          total_loss: -0.12595322728157043
          vf_explained_var: 0.9948895573616028
          vf_loss: 0.019363131374120712
    num_agent_steps_sampled: 814590
    num_agent_steps_trained: 814590
    num_steps_sampled: 814590
    num_steps_trained: 814590
  iterations_since_restore: 131
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.91179775280899
    ram_util_percent: 6.198735955056181
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10291141125421759
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1818.3259737047686
    mean_inference_ms: 2.500392074769748
    mean_raw_obs_processing_ms: 204.62638746514332
  time_since_restore: 114205.19761824608
  time_this_iter_s: 511.40587854385376
  time_total_s: 459207.09025907516
  timers:
    learn_throughput: 725.858
    learn_time_ms: 2603.814
    load_throughput: 165086.061
    load_time_ms: 11.449
    sample_throughput: 2.109
    sample_time_ms: 896216.736
    update_time_ms: 28.418
  timestamp: 1633174707
  timesteps_since_restore: 0
  timesteps_total: 814590
  training_iteration: 431
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    431 |           459207 | 814590 |  5.12569 |              7.58988 |              2.23873 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 816480
  custom_metrics: {}
  date: 2021-10-02_04-56-58
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.884588181169559
  episode_reward_mean: 5.004251965653016
  episode_reward_min: 2.243288055891356
  episodes_this_iter: 270
  episodes_total: 116640
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.8110000382876024e-05
          entropy: 8.701772689819336
          entropy_coeff: 6.908619980094954e-05
          kl: 0.012308359146118164
          model: {}
          policy_loss: -0.1959913820028305
          total_loss: -0.14660118520259857
          vf_explained_var: 0.9939603805541992
          vf_loss: 0.021951397880911827
    num_agent_steps_sampled: 816480
    num_agent_steps_trained: 816480
    num_steps_sampled: 816480
    num_steps_trained: 816480
  iterations_since_restore: 132
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.48939197930142
    ram_util_percent: 6.200000000000003
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10288101290122531
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1821.0303792796271
    mean_inference_ms: 2.4982207722573424
    mean_raw_obs_processing_ms: 204.60650759305102
  time_since_restore: 115316.4415178299
  time_this_iter_s: 1111.2438995838165
  time_total_s: 460318.334158659
  timers:
    learn_throughput: 726.533
    learn_time_ms: 2601.395
    load_throughput: 180421.522
    load_time_ms: 10.475
    sample_throughput: 1.967
    sample_time_ms: 961073.859
    update_time_ms: 28.106
  timestamp: 1633175818
  timesteps_since_restore: 0
  timesteps_total: 816480
  training_iteration: 432
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    432 |           460318 | 816480 |  5.00425 |              7.88459 |              2.24329 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 818370
  custom_metrics: {}
  date: 2021-10-02_05-05-11
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.41265321406888
  episode_reward_mean: 5.07000024487421
  episode_reward_min: 2.3049436230486315
  episodes_this_iter: 270
  episodes_total: 116910
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.7919999663718045e-05
          entropy: 8.581949234008789
          entropy_coeff: 6.808640318922698e-05
          kl: 0.012164448387920856
          model: {}
          policy_loss: -0.19185471534729004
          total_loss: -0.1427844613790512
          vf_explained_var: 0.9940689206123352
          vf_loss: 0.02194243110716343
    num_agent_steps_sampled: 818370
    num_agent_steps_trained: 818370
    num_steps_sampled: 818370
    num_steps_trained: 818370
  iterations_since_restore: 133
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.94256559766764
    ram_util_percent: 6.199854227405251
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10286711861567242
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1816.9485510851914
    mean_inference_ms: 2.4959725771094226
    mean_raw_obs_processing_ms: 204.58284243516206
  time_since_restore: 115809.41915345192
  time_this_iter_s: 492.97763562202454
  time_total_s: 460811.311794281
  timers:
    learn_throughput: 727.326
    learn_time_ms: 2598.558
    load_throughput: 182337.554
    load_time_ms: 10.365
    sample_throughput: 2.05
    sample_time_ms: 921922.038
    update_time_ms: 27.824
  timestamp: 1633176311
  timesteps_since_restore: 0
  timesteps_total: 818370
  training_iteration: 433
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    433 |           460811 | 818370 |     5.07 |              8.41265 |              2.30494 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 820260
  custom_metrics: {}
  date: 2021-10-02_05-08-31
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.015835365194372
  episode_reward_mean: 5.045360049342377
  episode_reward_min: 2.2856797847201076
  episodes_this_iter: 270
  episodes_total: 117180
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.773000076354947e-05
          entropy: 8.5513334274292
          entropy_coeff: 6.708659930154681e-05
          kl: 0.012255650945007801
          model: {}
          policy_loss: -0.18496191501617432
          total_loss: -0.13565266132354736
          vf_explained_var: 0.9939618706703186
          vf_loss: 0.02196304313838482
    num_agent_steps_sampled: 820260
    num_agent_steps_trained: 820260
    num_steps_sampled: 820260
    num_steps_trained: 820260
  iterations_since_restore: 134
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 43.57347670250896
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10285656854741708
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1810.544143268521
    mean_inference_ms: 2.4941204242438704
    mean_raw_obs_processing_ms: 204.55790122768752
  time_since_restore: 116009.44272661209
  time_this_iter_s: 200.0235731601715
  time_total_s: 461011.3353674412
  timers:
    learn_throughput: 730.038
    learn_time_ms: 2588.906
    load_throughput: 183743.239
    load_time_ms: 10.286
    sample_throughput: 2.705
    sample_time_ms: 698675.591
    update_time_ms: 27.74
  timestamp: 1633176511
  timesteps_since_restore: 0
  timesteps_total: 820260
  training_iteration: 434
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    434 |           461011 | 820260 |  5.04536 |              8.01584 |              2.28568 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 822150
  custom_metrics: {}
  date: 2021-10-02_05-24-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.769175996812017
  episode_reward_mean: 5.076747665513014
  episode_reward_min: 2.0841903720638544
  episodes_this_iter: 270
  episodes_total: 117450
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.754000004439149e-05
          entropy: 8.520219802856445
          entropy_coeff: 6.608680268982425e-05
          kl: 0.012892679311335087
          model: {}
          policy_loss: -0.17811906337738037
          total_loss: -0.12758268415927887
          vf_explained_var: 0.994080662727356
          vf_loss: 0.021728333085775375
    num_agent_steps_sampled: 822150
    num_agent_steps_trained: 822150
    num_steps_sampled: 822150
    num_steps_trained: 822150
  iterations_since_restore: 135
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.55962406015038
    ram_util_percent: 6.200000000000003
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10281836316212761
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1810.788830273122
    mean_inference_ms: 2.4926155404581136
    mean_raw_obs_processing_ms: 204.53872162052173
  time_since_restore: 116965.35258030891
  time_this_iter_s: 955.9098536968231
  time_total_s: 461967.245221138
  timers:
    learn_throughput: 731.04
    learn_time_ms: 2585.358
    load_throughput: 183954.725
    load_time_ms: 10.274
    sample_throughput: 3.51
    sample_time_ms: 538515.783
    update_time_ms: 27.47
  timestamp: 1633177467
  timesteps_since_restore: 0
  timesteps_total: 822150
  training_iteration: 435
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    435 |           461967 | 822150 |  5.07675 |              7.76918 |              2.08419 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 824040
  custom_metrics: {}
  date: 2021-10-02_05-36-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.875802715682869
  episode_reward_mean: 5.088567764904858
  episode_reward_min: 2.160117239499976
  episodes_this_iter: 270
  episodes_total: 117720
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.734999932523351e-05
          entropy: 8.427783012390137
          entropy_coeff: 6.508699880214408e-05
          kl: 0.012590301223099232
          model: {}
          policy_loss: -0.16284099221229553
          total_loss: -0.1133134514093399
          vf_explained_var: 0.9942154288291931
          vf_loss: 0.021393803879618645
    num_agent_steps_sampled: 824040
    num_agent_steps_trained: 824040
    num_steps_sampled: 824040
    num_steps_trained: 824040
  iterations_since_restore: 136
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.712375249500994
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10279368529157991
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1806.3956007443398
    mean_inference_ms: 2.4910188075855753
    mean_raw_obs_processing_ms: 204.6113440190157
  time_since_restore: 117685.3792514801
  time_this_iter_s: 720.0266711711884
  time_total_s: 462687.2718923092
  timers:
    learn_throughput: 731.279
    learn_time_ms: 2584.515
    load_throughput: 185512.234
    load_time_ms: 10.188
    sample_throughput: 3.305
    sample_time_ms: 571915.884
    update_time_ms: 27.616
  timestamp: 1633178187
  timesteps_since_restore: 0
  timesteps_total: 824040
  training_iteration: 436
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    436 |           462687 | 824040 |  5.08857 |               7.8758 |              2.16012 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 825930
  custom_metrics: {}
  date: 2021-10-02_05-40-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.8456550621978645
  episode_reward_mean: 5.081789170671584
  episode_reward_min: 2.2583539734019786
  episodes_this_iter: 270
  episodes_total: 117990
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.7160000425064936e-05
          entropy: 8.603510856628418
          entropy_coeff: 6.408720219042152e-05
          kl: 0.012850495055317879
          model: {}
          policy_loss: -0.20102623105049133
          total_loss: -0.15060392022132874
          vf_explained_var: 0.9940951466560364
          vf_loss: 0.02169865369796753
    num_agent_steps_sampled: 825930
    num_agent_steps_trained: 825930
    num_steps_sampled: 825930
    num_steps_trained: 825930
  iterations_since_restore: 137
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.855555555555554
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1027672876069388
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1802.4052434934556
    mean_inference_ms: 2.489181398951968
    mean_raw_obs_processing_ms: 204.59366947526956
  time_since_restore: 117950.65430665016
  time_this_iter_s: 265.2750551700592
  time_total_s: 462952.54694747925
  timers:
    learn_throughput: 733.804
    learn_time_ms: 2575.618
    load_throughput: 187088.393
    load_time_ms: 10.102
    sample_throughput: 3.47
    sample_time_ms: 544705.107
    update_time_ms: 27.633
  timestamp: 1633178452
  timesteps_since_restore: 0
  timesteps_total: 825930
  training_iteration: 437
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 19.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    437 |           462953 | 825930 |  5.08179 |              7.84566 |              2.25835 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 827820
  custom_metrics: {}
  date: 2021-10-02_05-53-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.107583423971523
  episode_reward_mean: 5.09903602701755
  episode_reward_min: 2.2206276905291547
  episodes_this_iter: 270
  episodes_total: 118260
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.6969999705906957e-05
          entropy: 8.46451187133789
          entropy_coeff: 6.308739830274135e-05
          kl: 0.01242061611264944
          model: {}
          policy_loss: -0.19128195941448212
          total_loss: -0.1414663940668106
          vf_explained_var: 0.9940754175186157
          vf_loss: 0.022053871303796768
    num_agent_steps_sampled: 827820
    num_agent_steps_trained: 827820
    num_steps_sampled: 827820
    num_steps_trained: 827820
  iterations_since_restore: 138
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.64312015503876
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10272465503939164
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1804.5980778043076
    mean_inference_ms: 2.4871156602639384
    mean_raw_obs_processing_ms: 204.57119883421282
  time_since_restore: 118693.74448871613
  time_this_iter_s: 743.0901820659637
  time_total_s: 463695.6371295452
  timers:
    learn_throughput: 737.082
    learn_time_ms: 2564.165
    load_throughput: 186787.745
    load_time_ms: 10.118
    sample_throughput: 3.317
    sample_time_ms: 569823.951
    update_time_ms: 27.333
  timestamp: 1633179196
  timesteps_since_restore: 0
  timesteps_total: 827820
  training_iteration: 438
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    438 |           463696 | 827820 |  5.09904 |              8.10758 |              2.22063 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 829710
  custom_metrics: {}
  date: 2021-10-02_06-06-11
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.770812719130773
  episode_reward_mean: 4.93369422156868
  episode_reward_min: 2.0519261779644005
  episodes_this_iter: 270
  episodes_total: 118530
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.6780000805738382e-05
          entropy: 8.672040939331055
          entropy_coeff: 6.208760169101879e-05
          kl: 0.012035531923174858
          model: {}
          policy_loss: -0.19080083072185516
          total_loss: -0.13911189138889313
          vf_explained_var: 0.9929604530334473
          vf_loss: 0.02480892650783062
    num_agent_steps_sampled: 829710
    num_agent_steps_trained: 829710
    num_steps_sampled: 829710
    num_steps_trained: 829710
  iterations_since_restore: 139
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.68146431881372
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10268835192452883
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1808.2136693227833
    mean_inference_ms: 2.485241831266242
    mean_raw_obs_processing_ms: 204.55429149115298
  time_since_restore: 119469.55938601494
  time_this_iter_s: 775.8148972988129
  time_total_s: 464471.452026844
  timers:
    learn_throughput: 739.69
    learn_time_ms: 2555.125
    load_throughput: 187989.484
    load_time_ms: 10.054
    sample_throughput: 2.996
    sample_time_ms: 630859.855
    update_time_ms: 27.274
  timestamp: 1633179971
  timesteps_since_restore: 0
  timesteps_total: 829710
  training_iteration: 439
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    439 |           464471 | 829710 |  4.93369 |              7.77081 |              2.05193 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 831600
  custom_metrics: {}
  date: 2021-10-02_06-17-52
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.474644705041825
  episode_reward_mean: 5.061179264074367
  episode_reward_min: 2.2187145499333316
  episodes_this_iter: 270
  episodes_total: 118800
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.6590000086580403e-05
          entropy: 8.555834770202637
          entropy_coeff: 6.108779780333862e-05
          kl: 0.011455868370831013
          model: {}
          policy_loss: -0.19668400287628174
          total_loss: -0.1472247689962387
          vf_explained_var: 0.9934922456741333
          vf_loss: 0.023884017020463943
    num_agent_steps_sampled: 831600
    num_agent_steps_trained: 831600
    num_steps_sampled: 831600
    num_steps_trained: 831600
  iterations_since_restore: 140
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.666086065573765
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10271269527578461
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1811.2978251475206
    mean_inference_ms: 2.483583711993785
    mean_raw_obs_processing_ms: 204.53632914256963
  time_since_restore: 120170.502856493
  time_this_iter_s: 700.9434704780579
  time_total_s: 465172.3954973221
  timers:
    learn_throughput: 739.917
    learn_time_ms: 2554.341
    load_throughput: 189227.11
    load_time_ms: 9.988
    sample_throughput: 2.93
    sample_time_ms: 644947.285
    update_time_ms: 27.149
  timestamp: 1633180672
  timesteps_since_restore: 0
  timesteps_total: 831600
  training_iteration: 440
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.1/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    440 |           465172 | 831600 |  5.06118 |              8.47464 |              2.21871 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 833490
  custom_metrics: {}
  date: 2021-10-02_06-25-20
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.729975252796459
  episode_reward_mean: 5.042904105409119
  episode_reward_min: 2.185027352562587
  episodes_this_iter: 270
  episodes_total: 119070
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.6399999367422424e-05
          entropy: 8.446102142333984
          entropy_coeff: 6.008800119161606e-05
          kl: 0.0117963757365942
          model: {}
          policy_loss: -0.1854672133922577
          total_loss: -0.13586576282978058
          vf_explained_var: 0.9933391213417053
          vf_loss: 0.023235326632857323
    num_agent_steps_sampled: 833490
    num_agent_steps_trained: 833490
    num_steps_sampled: 833490
    num_steps_trained: 833490
  iterations_since_restore: 141
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.038424437299035
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10266982542199816
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1809.6843765596802
    mean_inference_ms: 2.4817002381844753
    mean_raw_obs_processing_ms: 204.51708897726112
  time_since_restore: 120617.87578868866
  time_this_iter_s: 447.37293219566345
  time_total_s: 465619.76842951775
  timers:
    learn_throughput: 741.91
    learn_time_ms: 2547.477
    load_throughput: 190579.139
    load_time_ms: 9.917
    sample_throughput: 2.96
    sample_time_ms: 638550.887
    update_time_ms: 27.021
  timestamp: 1633181120
  timesteps_since_restore: 0
  timesteps_total: 833490
  training_iteration: 441
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 20.2/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    441 |           465620 | 833490 |   5.0429 |              7.72998 |              2.18503 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 835380
  custom_metrics: {}
  date: 2021-10-02_09-53-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.757907177060223
  episode_reward_mean: 5.039113172568827
  episode_reward_min: 2.222126845024655
  episodes_this_iter: 270
  episodes_total: 119340
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.621000046725385e-05
          entropy: 8.476451873779297
          entropy_coeff: 5.908820094191469e-05
          kl: 0.012339450418949127
          model: {}
          policy_loss: -0.16183719038963318
          total_loss: -0.11124740540981293
          vf_explained_var: 0.9935646653175354
          vf_loss: 0.022979846224188805
    num_agent_steps_sampled: 835380
    num_agent_steps_trained: 835380
    num_steps_sampled: 835380
    num_steps_trained: 835380
  iterations_since_restore: 142
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.64694746533173
    ram_util_percent: 6.727228263996779
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10265324467944781
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1856.855319148879
    mean_inference_ms: 2.480009582711605
    mean_raw_obs_processing_ms: 204.50527710042059
  time_since_restore: 133114.13759374619
  time_this_iter_s: 12496.261805057526
  time_total_s: 478116.0302345753
  timers:
    learn_throughput: 742.534
    learn_time_ms: 2545.337
    load_throughput: 191822.894
    load_time_ms: 9.853
    sample_throughput: 1.064
    sample_time_ms: 1777054.01
    update_time_ms: 27.796
  timestamp: 1633193616
  timesteps_since_restore: 0
  timesteps_total: 835380
  training_iteration: 442
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    442 |           478116 | 835380 |  5.03911 |              7.75791 |              2.22213 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 837270
  custom_metrics: {}
  date: 2021-10-02_10-22-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.928874403429574
  episode_reward_mean: 5.119893716365772
  episode_reward_min: 2.116985900947115
  episodes_this_iter: 270
  episodes_total: 119610
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.601999974809587e-05
          entropy: 8.47813892364502
          entropy_coeff: 5.808840069221333e-05
          kl: 0.011692498810589314
          model: {}
          policy_loss: -0.19572122395038605
          total_loss: -0.14571033418178558
          vf_explained_var: 0.9935551881790161
          vf_loss: 0.02386638894677162
    num_agent_steps_sampled: 837270
    num_agent_steps_trained: 837270
    num_steps_sampled: 837270
    num_steps_trained: 837270
  iterations_since_restore: 143
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.008721174004194
    ram_util_percent: 7.559580712788259
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1026192791695906
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1863.217340946083
    mean_inference_ms: 2.4784496648277092
    mean_raw_obs_processing_ms: 204.50735933401845
  time_since_restore: 134828.19101285934
  time_this_iter_s: 1714.0534191131592
  time_total_s: 479830.08365368843
  timers:
    learn_throughput: 742.032
    learn_time_ms: 2547.062
    load_throughput: 190712.101
    load_time_ms: 9.91
    sample_throughput: 0.995
    sample_time_ms: 1899159.208
    update_time_ms: 27.896
  timestamp: 1633195330
  timesteps_since_restore: 0
  timesteps_total: 837270
  training_iteration: 443
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    443 |           479830 | 837270 |  5.11989 |              7.92887 |              2.11699 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 839160
  custom_metrics: {}
  date: 2021-10-02_10-30-21
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.03894204061403
  episode_reward_mean: 5.061712990453331
  episode_reward_min: 2.314822337439356
  episodes_this_iter: 270
  episodes_total: 119880
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.5830000847927295e-05
          entropy: 8.409615516662598
          entropy_coeff: 5.708860044251196e-05
          kl: 0.012206261046230793
          model: {}
          policy_loss: -0.18597492575645447
          total_loss: -0.13775043189525604
          vf_explained_var: 0.9941286444664001
          vf_loss: 0.020897189155220985
    num_agent_steps_sampled: 839160
    num_agent_steps_trained: 839160
    num_steps_sampled: 839160
    num_steps_trained: 839160
  iterations_since_restore: 144
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.98770131771596
    ram_util_percent: 6.578038067349929
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10261787201327284
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1860.6528252763242
    mean_inference_ms: 2.477229255516983
    mean_raw_obs_processing_ms: 204.50807791902022
  time_since_restore: 135318.55758476257
  time_this_iter_s: 490.36657190322876
  time_total_s: 480320.45022559166
  timers:
    learn_throughput: 743.122
    learn_time_ms: 2543.325
    load_throughput: 189431.044
    load_time_ms: 9.977
    sample_throughput: 0.98
    sample_time_ms: 1928196.943
    update_time_ms: 27.64
  timestamp: 1633195821
  timesteps_since_restore: 0
  timesteps_total: 839160
  training_iteration: 444
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    444 |           480320 | 839160 |  5.06171 |              8.03894 |              2.31482 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 841050
  custom_metrics: {}
  date: 2021-10-02_10-48-17
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.848529649458842
  episode_reward_mean: 5.029707716483343
  episode_reward_min: 2.130252358366131
  episodes_this_iter: 270
  episodes_total: 120150
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.5640000128769316e-05
          entropy: 8.418943405151367
          entropy_coeff: 5.6088800192810595e-05
          kl: 0.011671299114823341
          model: {}
          policy_loss: -0.17024902999401093
          total_loss: -0.12151429802179337
          vf_explained_var: 0.9937810897827148
          vf_loss: 0.022618291899561882
    num_agent_steps_sampled: 841050
    num_agent_steps_trained: 841050
    num_steps_sampled: 841050
    num_steps_trained: 841050
  iterations_since_restore: 145
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.985914552736986
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10261117823982137
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1866.7190262179042
    mean_inference_ms: 2.475759408592097
    mean_raw_obs_processing_ms: 204.4986613698467
  time_since_restore: 136394.94537067413
  time_this_iter_s: 1076.38778591156
  time_total_s: 481396.8380115032
  timers:
    learn_throughput: 746.252
    learn_time_ms: 2532.655
    load_throughput: 189513.013
    load_time_ms: 9.973
    sample_throughput: 0.974
    sample_time_ms: 1940255.568
    update_time_ms: 27.828
  timestamp: 1633196897
  timesteps_since_restore: 0
  timesteps_total: 841050
  training_iteration: 445
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    445 |           481397 | 841050 |  5.02971 |              7.84853 |              2.13025 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 842940
  custom_metrics: {}
  date: 2021-10-02_11-04-39
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.719375443632859
  episode_reward_mean: 5.091763566211097
  episode_reward_min: 2.402796306482488
  episodes_this_iter: 270
  episodes_total: 120420
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.5449999409611337e-05
          entropy: 8.217855453491211
          entropy_coeff: 5.508899994310923e-05
          kl: 0.012404191307723522
          model: {}
          policy_loss: -0.19509966671466827
          total_loss: -0.14625592529773712
          vf_explained_var: 0.9942676424980164
          vf_loss: 0.021038159728050232
    num_agent_steps_sampled: 842940
    num_agent_steps_trained: 842940
    num_steps_sampled: 842940
    num_steps_trained: 842940
  iterations_since_restore: 146
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.35822970007315
    ram_util_percent: 6.19246525237747
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10259446680659154
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1869.0595485399158
    mean_inference_ms: 2.4742450505119944
    mean_raw_obs_processing_ms: 204.4864064537614
  time_since_restore: 137376.89973425865
  time_this_iter_s: 981.9543635845184
  time_total_s: 482378.79237508774
  timers:
    learn_throughput: 748.211
    learn_time_ms: 2526.025
    load_throughput: 188594.152
    load_time_ms: 10.022
    sample_throughput: 0.961
    sample_time_ms: 1966454.624
    update_time_ms: 27.749
  timestamp: 1633197879
  timesteps_since_restore: 0
  timesteps_total: 842940
  training_iteration: 446
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    446 |           482379 | 842940 |  5.09176 |              7.71938 |               2.4028 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 844830
  custom_metrics: {}
  date: 2021-10-02_11-08-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.886349248278401
  episode_reward_mean: 5.117841396332267
  episode_reward_min: 2.255195206548535
  episodes_this_iter: 270
  episodes_total: 120690
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.526000050944276e-05
          entropy: 8.362116813659668
          entropy_coeff: 5.408919969340786e-05
          kl: 0.012150741182267666
          model: {}
          policy_loss: -0.1667361557483673
          total_loss: -0.11843805015087128
          vf_explained_var: 0.9943230748176575
          vf_loss: 0.021069521084427834
    num_agent_steps_sampled: 844830
    num_agent_steps_trained: 844830
    num_steps_sampled: 844830
    num_steps_trained: 844830
  iterations_since_restore: 147
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.47269841269842
    ram_util_percent: 6.1000000000000005
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10258238620890833
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1864.0467725993983
    mean_inference_ms: 2.472607887409008
    mean_raw_obs_processing_ms: 204.47171480843983
  time_since_restore: 137603.23037433624
  time_this_iter_s: 226.33064007759094
  time_total_s: 482605.1230151653
  timers:
    learn_throughput: 747.995
    learn_time_ms: 2526.755
    load_throughput: 187575.354
    load_time_ms: 10.076
    sample_throughput: 0.963
    sample_time_ms: 1962558.309
    update_time_ms: 28.926
  timestamp: 1633198106
  timesteps_since_restore: 0
  timesteps_total: 844830
  training_iteration: 447
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    447 |           482605 | 844830 |  5.11784 |              7.88635 |               2.2552 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 846720
  custom_metrics: {}
  date: 2021-10-02_11-24-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.792043461598618
  episode_reward_mean: 5.053462442286865
  episode_reward_min: 1.9483027681144078
  episodes_this_iter: 270
  episodes_total: 120960
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.5069999790284783e-05
          entropy: 8.32570743560791
          entropy_coeff: 5.30893994437065e-05
          kl: 0.011832037940621376
          model: {}
          policy_loss: -0.1942167580127716
          total_loss: -0.14327414333820343
          vf_explained_var: 0.9931766390800476
          vf_loss: 0.024429770186543465
    num_agent_steps_sampled: 846720
    num_agent_steps_trained: 846720
    num_steps_sampled: 846720
    num_steps_trained: 846720
  iterations_since_restore: 148
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.695788667687594
    ram_util_percent: 6.169372128637062
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10255357659467303
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1868.4982581608115
    mean_inference_ms: 2.470991694161112
    mean_raw_obs_processing_ms: 204.4551032963383
  time_since_restore: 138541.6703336239
  time_this_iter_s: 938.4399592876434
  time_total_s: 483543.562974453
  timers:
    learn_throughput: 746.793
    learn_time_ms: 2530.822
    load_throughput: 187978.34
    load_time_ms: 10.054
    sample_throughput: 0.954
    sample_time_ms: 1982089.4
    update_time_ms: 29.007
  timestamp: 1633199044
  timesteps_since_restore: 0
  timesteps_total: 846720
  training_iteration: 448
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 16.9/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    448 |           483544 | 846720 |  5.05346 |              7.79204 |               1.9483 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 848610
  custom_metrics: {}
  date: 2021-10-02_11-42-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.087933660285366
  episode_reward_mean: 5.034652244311744
  episode_reward_min: 2.401956225076069
  episodes_this_iter: 270
  episodes_total: 121230
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.4879999980621506e-05
          entropy: 8.411789894104004
          entropy_coeff: 5.208959919400513e-05
          kl: 0.01170422974973917
          model: {}
          policy_loss: -0.16576017439365387
          total_loss: -0.11739881336688995
          vf_explained_var: 0.9938737154006958
          vf_loss: 0.022135812789201736
    num_agent_steps_sampled: 848610
    num_agent_steps_trained: 848610
    num_steps_sampled: 848610
    num_steps_trained: 848610
  iterations_since_restore: 149
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.54948717948718
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10255695872345834
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1867.4905896785529
    mean_inference_ms: 2.4697716186314387
    mean_raw_obs_processing_ms: 204.45349398694907
  time_since_restore: 139663.17780423164
  time_this_iter_s: 1121.5074706077576
  time_total_s: 484665.07044506073
  timers:
    learn_throughput: 746.289
    learn_time_ms: 2532.532
    load_throughput: 187965.859
    load_time_ms: 10.055
    sample_throughput: 0.937
    sample_time_ms: 2016656.69
    update_time_ms: 29.089
  timestamp: 1633200166
  timesteps_since_restore: 0
  timesteps_total: 848610
  training_iteration: 449
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    449 |           484665 | 848610 |  5.03465 |              8.08793 |              2.40196 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 850500
  custom_metrics: {}
  date: 2021-10-02_11-55-38
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.45391757290423
  episode_reward_mean: 5.001923513947964
  episode_reward_min: 2.2571346611166443
  episodes_this_iter: 270
  episodes_total: 121500
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.4690000170958228e-05
          entropy: 8.392709732055664
          entropy_coeff: 5.1089798944303766e-05
          kl: 0.011769996955990791
          model: {}
          policy_loss: -0.19516293704509735
          total_loss: -0.14522777497768402
          vf_explained_var: 0.9934500455856323
          vf_loss: 0.02355041541159153
    num_agent_steps_sampled: 850500
    num_agent_steps_trained: 850500
    num_steps_sampled: 850500
    num_steps_trained: 850500
  iterations_since_restore: 150
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.19813953488372
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10253132594802887
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1868.8885101912344
    mean_inference_ms: 2.468089657928307
    mean_raw_obs_processing_ms: 204.44103670891053
  time_since_restore: 140435.61665391922
  time_this_iter_s: 772.4388496875763
  time_total_s: 485437.5092947483
  timers:
    learn_throughput: 747.736
    learn_time_ms: 2527.629
    load_throughput: 187456.479
    load_time_ms: 10.082
    sample_throughput: 0.934
    sample_time_ms: 2023812.557
    update_time_ms: 28.958
  timestamp: 1633200938
  timesteps_since_restore: 0
  timesteps_total: 850500
  training_iteration: 450
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    450 |           485438 | 850500 |  5.00192 |              8.45392 |              2.25713 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 852390
  custom_metrics: {}
  date: 2021-10-02_12-02-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.04106075829559
  episode_reward_mean: 5.036889717031735
  episode_reward_min: 2.367048813231253
  episodes_this_iter: 270
  episodes_total: 121770
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.4500000361294951e-05
          entropy: 8.386497497558594
          entropy_coeff: 5.00899986946024e-05
          kl: 0.011640859767794609
          model: {}
          policy_loss: -0.1829725056886673
          total_loss: -0.13399918377399445
          vf_explained_var: 0.9935336112976074
          vf_loss: 0.022874068468809128
    num_agent_steps_sampled: 852390
    num_agent_steps_trained: 852390
    num_steps_sampled: 852390
    num_steps_trained: 852390
  iterations_since_restore: 151
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.19726962457337
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10249888058792596
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1867.6035972009365
    mean_inference_ms: 2.466731783325206
    mean_raw_obs_processing_ms: 204.43661863609097
  time_since_restore: 140862.08677363396
  time_this_iter_s: 426.47011971473694
  time_total_s: 485863.97941446304
  timers:
    learn_throughput: 747.224
    learn_time_ms: 2529.362
    load_throughput: 186789.946
    load_time_ms: 10.118
    sample_throughput: 0.935
    sample_time_ms: 2021720.434
    update_time_ms: 28.926
  timestamp: 1633201365
  timesteps_since_restore: 0
  timesteps_total: 852390
  training_iteration: 451
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    451 |           485864 | 852390 |  5.03689 |              8.04106 |              2.36705 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 854280
  custom_metrics: {}
  date: 2021-10-02_12-16-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.891018888750337
  episode_reward_mean: 5.148878022742154
  episode_reward_min: 2.207259443469714
  episodes_this_iter: 270
  episodes_total: 122040
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.4309999642136972e-05
          entropy: 8.190571784973145
          entropy_coeff: 4.9090198444901034e-05
          kl: 0.012142213061451912
          model: {}
          policy_loss: -0.18188098073005676
          total_loss: -0.13343757390975952
          vf_explained_var: 0.994411826133728
          vf_loss: 0.021184001117944717
    num_agent_steps_sampled: 854280
    num_agent_steps_trained: 854280
    num_steps_sampled: 854280
    num_steps_trained: 854280
  iterations_since_restore: 152
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.529237668161436
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10248175029530002
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1865.8253500929968
    mean_inference_ms: 2.465442833629113
    mean_raw_obs_processing_ms: 204.42661637584484
  time_since_restore: 141663.1531765461
  time_this_iter_s: 801.0664029121399
  time_total_s: 486665.0458173752
  timers:
    learn_throughput: 746.135
    learn_time_ms: 2533.052
    load_throughput: 186910.18
    load_time_ms: 10.112
    sample_throughput: 2.218
    sample_time_ms: 852197.55
    update_time_ms: 28.13
  timestamp: 1633202166
  timesteps_since_restore: 0
  timesteps_total: 854280
  training_iteration: 452
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    452 |           486665 | 854280 |  5.14888 |              7.89102 |              2.20726 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 856170
  custom_metrics: {}
  date: 2021-10-02_12-28-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.821040339616905
  episode_reward_mean: 4.999533400918632
  episode_reward_min: 2.0519033991384097
  episodes_this_iter: 270
  episodes_total: 122310
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.4119999832473695e-05
          entropy: 8.265115737915039
          entropy_coeff: 4.809039819519967e-05
          kl: 0.01212124340236187
          model: {}
          policy_loss: -0.1706310361623764
          total_loss: -0.12396775186061859
          vf_explained_var: 0.9945193529129028
          vf_loss: 0.019447049126029015
    num_agent_steps_sampled: 856170
    num_agent_steps_trained: 856170
    num_steps_sampled: 856170
    num_steps_trained: 856170
  iterations_since_restore: 153
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.240199999999994
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10246247616115174
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1864.5359957165222
    mean_inference_ms: 2.4638936673772216
    mean_raw_obs_processing_ms: 204.4236492891032
  time_since_restore: 142381.57718777657
  time_this_iter_s: 718.4240112304688
  time_total_s: 487383.46982860565
  timers:
    learn_throughput: 746.263
    learn_time_ms: 2532.618
    load_throughput: 187828.686
    load_time_ms: 10.062
    sample_throughput: 2.511
    sample_time_ms: 752635.455
    update_time_ms: 28.134
  timestamp: 1633202884
  timesteps_since_restore: 0
  timesteps_total: 856170
  training_iteration: 453
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    453 |           487383 | 856170 |  4.99953 |              7.82104 |               2.0519 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 858060
  custom_metrics: {}
  date: 2021-10-02_12-45-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.393061328121203
  episode_reward_mean: 5.030807337155586
  episode_reward_min: 1.989424114262407
  episodes_this_iter: 270
  episodes_total: 122580
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.3930000022810418e-05
          entropy: 8.241326332092285
          entropy_coeff: 4.709060158347711e-05
          kl: 0.012306525371968746
          model: {}
          policy_loss: -0.1729750633239746
          total_loss: -0.12306290119886398
          vf_explained_var: 0.9940905570983887
          vf_loss: 0.022264430299401283
    num_agent_steps_sampled: 858060
    num_agent_steps_trained: 858060
    num_steps_sampled: 858060
    num_steps_trained: 858060
  iterations_since_restore: 154
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.754666666666665
    ram_util_percent: 6.200000000000003
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10243950580514459
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1868.5924093720716
    mean_inference_ms: 2.462944227656926
    mean_raw_obs_processing_ms: 204.41607524389937
  time_since_restore: 143405.8034658432
  time_this_iter_s: 1024.2262780666351
  time_total_s: 488407.6961066723
  timers:
    learn_throughput: 746.185
    learn_time_ms: 2532.886
    load_throughput: 188080.919
    load_time_ms: 10.049
    sample_throughput: 2.345
    sample_time_ms: 806021.368
    update_time_ms: 28.258
  timestamp: 1633203908
  timesteps_since_restore: 0
  timesteps_total: 858060
  training_iteration: 454
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    454 |           488408 | 858060 |  5.03081 |              8.39306 |              1.98942 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 859950
  custom_metrics: {}
  date: 2021-10-02_13-06-30
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.319940928478815
  episode_reward_mean: 5.1576074782873915
  episode_reward_min: 2.3469525239301468
  episodes_this_iter: 270
  episodes_total: 122850
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.3740000213147141e-05
          entropy: 8.159422874450684
          entropy_coeff: 4.6090801333775744e-05
          kl: 0.012164069339632988
          model: {}
          policy_loss: -0.16927874088287354
          total_loss: -0.11746106296777725
          vf_explained_var: 0.9933610558509827
          vf_loss: 0.024482499808073044
    num_agent_steps_sampled: 859950
    num_agent_steps_trained: 859950
    num_steps_sampled: 859950
    num_steps_trained: 859950
  iterations_since_restore: 155
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.51587212563095
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10245019655757888
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1873.4628419218648
    mean_inference_ms: 2.4617538442061297
    mean_raw_obs_processing_ms: 204.41561485180023
  time_since_restore: 144687.11850261688
  time_this_iter_s: 1281.3150367736816
  time_total_s: 489689.01114344597
  timers:
    learn_throughput: 745.807
    learn_time_ms: 2534.169
    load_throughput: 187533.198
    load_time_ms: 10.078
    sample_throughput: 2.287
    sample_time_ms: 826512.829
    update_time_ms: 28.376
  timestamp: 1633205190
  timesteps_since_restore: 0
  timesteps_total: 859950
  training_iteration: 455
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    455 |           489689 | 859950 |  5.15761 |              8.31994 |              2.34695 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 861840
  custom_metrics: {}
  date: 2021-10-02_13-24-40
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.73491720495605
  episode_reward_mean: 5.017583893164049
  episode_reward_min: 2.026054361549159
  episodes_this_iter: 270
  episodes_total: 123120
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.3550000403483864e-05
          entropy: 8.24928092956543
          entropy_coeff: 4.509100108407438e-05
          kl: 0.01146127749234438
          model: {}
          policy_loss: -0.1938372403383255
          total_loss: -0.14336559176445007
          vf_explained_var: 0.9932760000228882
          vf_loss: 0.02473340928554535
    num_agent_steps_sampled: 861840
    num_agent_steps_trained: 861840
    num_steps_sampled: 861840
    num_steps_trained: 861840
  iterations_since_restore: 156
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.38655240606461
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10244464882350249
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1879.8817589667124
    mean_inference_ms: 2.460305653383029
    mean_raw_obs_processing_ms: 204.40751157541257
  time_since_restore: 145777.55796027184
  time_this_iter_s: 1090.439457654953
  time_total_s: 490779.4506011009
  timers:
    learn_throughput: 746.172
    learn_time_ms: 2532.93
    load_throughput: 187990.822
    load_time_ms: 10.054
    sample_throughput: 2.257
    sample_time_ms: 837362.658
    update_time_ms: 28.493
  timestamp: 1633206280
  timesteps_since_restore: 0
  timesteps_total: 861840
  training_iteration: 456
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    456 |           490779 | 861840 |  5.01758 |              7.73492 |              2.02605 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 863730
  custom_metrics: {}
  date: 2021-10-02_13-34-11
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.728757628417852
  episode_reward_mean: 5.0922413738618415
  episode_reward_min: 1.9731900900560304
  episodes_this_iter: 270
  episodes_total: 123390
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.3359999684325885e-05
          entropy: 8.227620124816895
          entropy_coeff: 4.409120083437301e-05
          kl: 0.011626921594142914
          model: {}
          policy_loss: -0.19566604495048523
          total_loss: -0.14816054701805115
          vf_explained_var: 0.9941006898880005
          vf_loss: 0.02138068899512291
    num_agent_steps_sampled: 863730
    num_agent_steps_trained: 863730
    num_steps_sampled: 863730
    num_steps_trained: 863730
  iterations_since_restore: 157
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.58100628930818
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10242199136005642
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1877.2045173646306
    mean_inference_ms: 2.4589742109227934
    mean_raw_obs_processing_ms: 204.3991490299295
  time_since_restore: 146348.35682225227
  time_this_iter_s: 570.7988619804382
  time_total_s: 491350.24946308136
  timers:
    learn_throughput: 746.177
    learn_time_ms: 2532.912
    load_throughput: 188751.32
    load_time_ms: 10.013
    sample_throughput: 2.168
    sample_time_ms: 871810.873
    update_time_ms: 27.297
  timestamp: 1633206851
  timesteps_since_restore: 0
  timesteps_total: 863730
  training_iteration: 457
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    457 |           491350 | 863730 |  5.09224 |              7.72876 |              1.97319 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 865620
  custom_metrics: {}
  date: 2021-10-02_13-47-45
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.004546006642695
  episode_reward_mean: 5.152552062982415
  episode_reward_min: 2.178773431038135
  episodes_this_iter: 270
  episodes_total: 123660
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.3169999874662608e-05
          entropy: 8.192595481872559
          entropy_coeff: 4.3091400584671646e-05
          kl: 0.011042537167668343
          model: {}
          policy_loss: -0.18276600539684296
          total_loss: -0.13529527187347412
          vf_explained_var: 0.9938983917236328
          vf_loss: 0.022667482495307922
    num_agent_steps_sampled: 865620
    num_agent_steps_trained: 865620
    num_steps_sampled: 865620
    num_steps_trained: 865620
  iterations_since_restore: 158
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.82347749338041
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10241052898546096
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1875.3270667207103
    mean_inference_ms: 2.4579186185817097
    mean_raw_obs_processing_ms: 204.39287503751913
  time_since_restore: 147162.53432917595
  time_this_iter_s: 814.1775069236755
  time_total_s: 492164.42697000504
  timers:
    learn_throughput: 745.969
    learn_time_ms: 2533.617
    load_throughput: 188659.233
    load_time_ms: 10.018
    sample_throughput: 2.199
    sample_time_ms: 859383.714
    update_time_ms: 27.196
  timestamp: 1633207665
  timesteps_since_restore: 0
  timesteps_total: 865620
  training_iteration: 458
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    458 |           492164 | 865620 |  5.15255 |              8.00455 |              2.17877 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 867510
  custom_metrics: {}
  date: 2021-10-02_13-53-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.901023718567523
  episode_reward_mean: 5.072833810640278
  episode_reward_min: 2.226550950905768
  episodes_this_iter: 270
  episodes_total: 123930
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.298000006499933e-05
          entropy: 8.05717945098877
          entropy_coeff: 4.209160033497028e-05
          kl: 0.011340283788740635
          model: {}
          policy_loss: -0.187875434756279
          total_loss: -0.1394534856081009
          vf_explained_var: 0.9937278032302856
          vf_loss: 0.022926533594727516
    num_agent_steps_sampled: 867510
    num_agent_steps_trained: 867510
    num_steps_sampled: 867510
    num_steps_trained: 867510
  iterations_since_restore: 159
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.159130434782604
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10239084829311318
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1869.5935386410672
    mean_inference_ms: 2.456560985643409
    mean_raw_obs_processing_ms: 204.39096039083674
  time_since_restore: 147493.53046917915
  time_this_iter_s: 330.99614000320435
  time_total_s: 492495.42311000824
  timers:
    learn_throughput: 745.918
    learn_time_ms: 2533.79
    load_throughput: 188853.395
    load_time_ms: 10.008
    sample_throughput: 2.422
    sample_time_ms: 780332.589
    update_time_ms: 27.135
  timestamp: 1633207996
  timesteps_since_restore: 0
  timesteps_total: 867510
  training_iteration: 459
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    459 |           492495 | 867510 |  5.07283 |              7.90102 |              2.22655 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 869400
  custom_metrics: {}
  date: 2021-10-02_14-11-07
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.376916313446088
  episode_reward_mean: 5.060867863547359
  episode_reward_min: 2.4304374395810107
  episodes_this_iter: 270
  episodes_total: 124200
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.2790000255336054e-05
          entropy: 8.16270923614502
          entropy_coeff: 4.1091800085268915e-05
          kl: 0.011710860766470432
          model: {}
          policy_loss: -0.19352957606315613
          total_loss: -0.14214754104614258
          vf_explained_var: 0.9929458498954773
          vf_loss: 0.025038642808794975
    num_agent_steps_sampled: 869400
    num_agent_steps_trained: 869400
    num_steps_sampled: 869400
    num_steps_trained: 869400
  iterations_since_restore: 160
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.349865771812084
    ram_util_percent: 6.200000000000001
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10237578570873282
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1878.0179133946306
    mean_inference_ms: 2.455482234353064
    mean_raw_obs_processing_ms: 204.38662288835428
  time_since_restore: 148564.26297736168
  time_this_iter_s: 1070.7325081825256
  time_total_s: 493566.15561819077
  timers:
    learn_throughput: 744.898
    learn_time_ms: 2537.26
    load_throughput: 189422.444
    load_time_ms: 9.978
    sample_throughput: 2.333
    sample_time_ms: 810158.063
    update_time_ms: 27.164
  timestamp: 1633209067
  timesteps_since_restore: 0
  timesteps_total: 869400
  training_iteration: 460
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    460 |           493566 | 869400 |  5.06087 |              8.37692 |              2.43044 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 871290
  custom_metrics: {}
  date: 2021-10-02_14-19-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.829699337338789
  episode_reward_mean: 5.070397644944405
  episode_reward_min: 2.1147683335915657
  episodes_this_iter: 270
  episodes_total: 124470
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.2600000445672777e-05
          entropy: 8.172964096069336
          entropy_coeff: 4.009199983556755e-05
          kl: 0.012518192641437054
          model: {}
          policy_loss: -0.1638338565826416
          total_loss: -0.11424721777439117
          vf_explained_var: 0.9939888715744019
          vf_loss: 0.02139628864824772
    num_agent_steps_sampled: 871290
    num_agent_steps_trained: 871290
    num_steps_sampled: 871290
    num_steps_trained: 871290
  iterations_since_restore: 161
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.68744460856721
    ram_util_percent: 6.200000000000002
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10235457190486749
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1874.4600491321291
    mean_inference_ms: 2.454030371421055
    mean_raw_obs_processing_ms: 204.3856893122622
  time_since_restore: 149050.4652583599
  time_this_iter_s: 486.20228099823
  time_total_s: 494052.357899189
  timers:
    learn_throughput: 744.478
    learn_time_ms: 2538.693
    load_throughput: 189118.766
    load_time_ms: 9.994
    sample_throughput: 2.316
    sample_time_ms: 816129.934
    update_time_ms: 27.233
  timestamp: 1633209553
  timesteps_since_restore: 0
  timesteps_total: 871290
  training_iteration: 461
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    461 |           494052 | 871290 |   5.0704 |               7.8297 |              2.11477 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 873180
  custom_metrics: {}
  date: 2021-10-02_14-27-16
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.789775768532051
  episode_reward_mean: 5.090829587620373
  episode_reward_min: 2.044642808716157
  episodes_this_iter: 270
  episodes_total: 124740
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.2409999726514798e-05
          entropy: 8.121044158935547
          entropy_coeff: 3.909219958586618e-05
          kl: 0.01073732040822506
          model: {}
          policy_loss: -0.18674233555793762
          total_loss: -0.14271481335163116
          vf_explained_var: 0.9945635199546814
          vf_loss: 0.01988402009010315
    num_agent_steps_sampled: 873180
    num_agent_steps_trained: 873180
    num_steps_sampled: 873180
    num_steps_trained: 873180
  iterations_since_restore: 162
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.998956780924
    ram_util_percent: 6.200000000000003
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10235348946141842
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1872.7172534332658
    mean_inference_ms: 2.4533271618315005
    mean_raw_obs_processing_ms: 204.3779703910851
  time_since_restore: 149532.5079021454
  time_this_iter_s: 482.0426437854767
  time_total_s: 494534.4005429745
  timers:
    learn_throughput: 745.48
    learn_time_ms: 2535.278
    load_throughput: 188960.535
    load_time_ms: 10.002
    sample_throughput: 2.41
    sample_time_ms: 784231.122
    update_time_ms: 27.267
  timestamp: 1633210036
  timesteps_since_restore: 0
  timesteps_total: 873180
  training_iteration: 462
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    462 |           494534 | 873180 |  5.09083 |              7.78978 |              2.04464 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 875070
  custom_metrics: {}
  date: 2021-10-02_14-45-29
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.712098711111164
  episode_reward_mean: 5.04627742268269
  episode_reward_min: 2.314830192730599
  episodes_this_iter: 270
  episodes_total: 125010
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.221999991685152e-05
          entropy: 8.157389640808105
          entropy_coeff: 3.809239933616482e-05
          kl: 0.011346513405442238
          model: {}
          policy_loss: -0.16836878657341003
          total_loss: -0.11766080558300018
          vf_explained_var: 0.9929829239845276
          vf_loss: 0.025169936940073967
    num_agent_steps_sampled: 875070
    num_agent_steps_trained: 875070
    num_steps_sampled: 875070
    num_steps_trained: 875070
  iterations_since_restore: 163
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.34168310322156
    ram_util_percent: 6.558316896778436
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10233434192961144
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1872.877035015518
    mean_inference_ms: 2.452610677767789
    mean_raw_obs_processing_ms: 204.37616089986577
  time_since_restore: 150625.91576361656
  time_this_iter_s: 1093.4078614711761
  time_total_s: 495627.80840444565
  timers:
    learn_throughput: 746.113
    learn_time_ms: 2533.13
    load_throughput: 187538.965
    load_time_ms: 10.078
    sample_throughput: 2.3
    sample_time_ms: 821731.904
    update_time_ms: 27.068
  timestamp: 1633211129
  timesteps_since_restore: 0
  timesteps_total: 875070
  training_iteration: 463
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    463 |           495628 | 875070 |  5.04628 |               7.7121 |              2.31483 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 876960
  custom_metrics: {}
  date: 2021-10-02_14-55-48
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.013728375056216
  episode_reward_mean: 5.1017979099130155
  episode_reward_min: 2.0659454235905312
  episodes_this_iter: 270
  episodes_total: 125280
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.2030000107188243e-05
          entropy: 8.03580093383789
          entropy_coeff: 3.709259908646345e-05
          kl: 0.011061037890613079
          model: {}
          policy_loss: -0.18599221110343933
          total_loss: -0.1385151892900467
          vf_explained_var: 0.9938725233078003
          vf_loss: 0.022576654329895973
    num_agent_steps_sampled: 876960
    num_agent_steps_trained: 876960
    num_steps_sampled: 876960
    num_steps_trained: 876960
  iterations_since_restore: 164
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.80266821345707
    ram_util_percent: 6.788167053364269
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1023360339276541
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1870.857839360306
    mean_inference_ms: 2.451341033287493
    mean_raw_obs_processing_ms: 204.37048098813366
  time_since_restore: 151245.08958554268
  time_this_iter_s: 619.173821926117
  time_total_s: 496246.98222637177
  timers:
    learn_throughput: 746.054
    learn_time_ms: 2533.329
    load_throughput: 187970.316
    load_time_ms: 10.055
    sample_throughput: 2.419
    sample_time_ms: 781226.24
    update_time_ms: 27.095
  timestamp: 1633211748
  timesteps_since_restore: 0
  timesteps_total: 876960
  training_iteration: 464
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    464 |           496247 | 876960 |   5.1018 |              8.01373 |              2.06595 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 878850
  custom_metrics: {}
  date: 2021-10-02_15-19-53
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.488166943907315
  episode_reward_mean: 5.0835541912190285
  episode_reward_min: 2.222089100024881
  episodes_this_iter: 270
  episodes_total: 125550
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.1840000297524966e-05
          entropy: 8.029545783996582
          entropy_coeff: 3.6092798836762086e-05
          kl: 0.010991846211254597
          model: {}
          policy_loss: -0.1588178426027298
          total_loss: -0.11206646263599396
          vf_explained_var: 0.9940604567527771
          vf_loss: 0.022000398486852646
    num_agent_steps_sampled: 878850
    num_agent_steps_trained: 878850
    num_steps_sampled: 878850
    num_steps_trained: 878850
  iterations_since_restore: 165
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.419492789656886
    ram_util_percent: 6.795872700149178
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1023358958026206
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1871.7797608024553
    mean_inference_ms: 2.45010668756427
    mean_raw_obs_processing_ms: 204.35309422551654
  time_since_restore: 152689.78200387955
  time_this_iter_s: 1444.6924183368683
  time_total_s: 497691.67464470863
  timers:
    learn_throughput: 747.131
    learn_time_ms: 2529.678
    load_throughput: 188797.173
    load_time_ms: 10.011
    sample_throughput: 2.37
    sample_time_ms: 797567.86
    update_time_ms: 26.873
  timestamp: 1633213193
  timesteps_since_restore: 0
  timesteps_total: 878850
  training_iteration: 465
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    465 |           497692 | 878850 |  5.08355 |              8.48817 |              2.22209 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 880740
  custom_metrics: {}
  date: 2021-10-02_15-35-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.900465204595814
  episode_reward_mean: 5.0423115345293255
  episode_reward_min: 2.1634128537830994
  episodes_this_iter: 270
  episodes_total: 125820
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.1649999578366987e-05
          entropy: 7.8964948654174805
          entropy_coeff: 3.509299858706072e-05
          kl: 0.011000094003975391
          model: {}
          policy_loss: -0.18078050017356873
          total_loss: -0.13437584042549133
          vf_explained_var: 0.9939152002334595
          vf_loss: 0.021622177213430405
    num_agent_steps_sampled: 880740
    num_agent_steps_trained: 880740
    num_steps_sampled: 880740
    num_steps_trained: 880740
  iterations_since_restore: 166
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.433125000000004
    ram_util_percent: 6.799999999999999
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10233064497291022
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1867.7484547331342
    mean_inference_ms: 2.4492323467903336
    mean_raw_obs_processing_ms: 204.34680414826497
  time_since_restore: 153609.9404978752
  time_this_iter_s: 920.1584939956665
  time_total_s: 498611.8331387043
  timers:
    learn_throughput: 746.031
    learn_time_ms: 2533.408
    load_throughput: 188764.804
    load_time_ms: 10.012
    sample_throughput: 2.421
    sample_time_ms: 780536.209
    update_time_ms: 26.862
  timestamp: 1633214113
  timesteps_since_restore: 0
  timesteps_total: 880740
  training_iteration: 466
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    466 |           498612 | 880740 |  5.04231 |              7.90047 |              2.16341 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 882630
  custom_metrics: {}
  date: 2021-10-02_15-39-55
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.310950554765718
  episode_reward_mean: 5.020481249827702
  episode_reward_min: 2.1554425595168376
  episodes_this_iter: 270
  episodes_total: 126090
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.145999976870371e-05
          entropy: 7.903953552246094
          entropy_coeff: 3.4093198337359354e-05
          kl: 0.01183959748595953
          model: {}
          policy_loss: -0.1861196905374527
          total_loss: -0.1369677633047104
          vf_explained_var: 0.9936814308166504
          vf_loss: 0.022449297830462456
    num_agent_steps_sampled: 882630
    num_agent_steps_trained: 882630
    num_steps_sampled: 882630
    num_steps_trained: 882630
  iterations_since_restore: 167
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.85496183206107
    ram_util_percent: 6.799999999999999
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10231657609436258
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1862.9753072314702
    mean_inference_ms: 2.448242846598843
    mean_raw_obs_processing_ms: 204.33456232573695
  time_since_restore: 153892.04196834564
  time_this_iter_s: 282.10147047042847
  time_total_s: 498893.9346091747
  timers:
    learn_throughput: 747.002
    learn_time_ms: 2530.113
    load_throughput: 188107.25
    load_time_ms: 10.047
    sample_throughput: 2.514
    sample_time_ms: 751669.461
    update_time_ms: 26.912
  timestamp: 1633214395
  timesteps_since_restore: 0
  timesteps_total: 882630
  training_iteration: 467
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    467 |           498894 | 882630 |  5.02048 |              8.31095 |              2.15544 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 884520
  custom_metrics: {}
  date: 2021-10-02_15-51-10
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.863772097152468
  episode_reward_mean: 5.117487633532582
  episode_reward_min: 2.0106816409370007
  episodes_this_iter: 270
  episodes_total: 126360
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.1269999959040433e-05
          entropy: 7.9097394943237305
          entropy_coeff: 3.3093401725636795e-05
          kl: 0.011818431317806244
          model: {}
          policy_loss: -0.17608141899108887
          total_loss: -0.12350110709667206
          vf_explained_var: 0.993196427822113
          vf_loss: 0.02591821923851967
    num_agent_steps_sampled: 884520
    num_agent_steps_trained: 884520
    num_steps_sampled: 884520
    num_steps_trained: 884520
  iterations_since_restore: 168
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.656549520766774
    ram_util_percent: 6.681469648562298
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1023177548475293
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1859.4193741832823
    mean_inference_ms: 2.4472770085067355
    mean_raw_obs_processing_ms: 204.3402167506375
  time_since_restore: 154566.7669107914
  time_this_iter_s: 674.724942445755
  time_total_s: 499568.6595516205
  timers:
    learn_throughput: 748.23
    learn_time_ms: 2525.961
    load_throughput: 188915.953
    load_time_ms: 10.004
    sample_throughput: 2.562
    sample_time_ms: 737728.507
    update_time_ms: 27.014
  timestamp: 1633215070
  timesteps_since_restore: 0
  timesteps_total: 884520
  training_iteration: 468
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    468 |           499569 | 884520 |  5.11749 |              7.86377 |              2.01068 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 886410
  custom_metrics: {}
  date: 2021-10-02_15-58-13
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.416617241937788
  episode_reward_mean: 5.123854066786461
  episode_reward_min: 2.40752795390528
  episodes_this_iter: 270
  episodes_total: 126630
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.1080000149377156e-05
          entropy: 7.849085330963135
          entropy_coeff: 3.209360147593543e-05
          kl: 0.012254437431693077
          model: {}
          policy_loss: -0.17405281960964203
          total_loss: -0.12462542206048965
          vf_explained_var: 0.9941431879997253
          vf_loss: 0.021762171760201454
    num_agent_steps_sampled: 886410
    num_agent_steps_trained: 886410
    num_steps_sampled: 886410
    num_steps_trained: 886410
  iterations_since_restore: 169
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.13554421768708
    ram_util_percent: 6.1000000000000005
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10231225606852705
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1858.899587763348
    mean_inference_ms: 2.4467881380318826
    mean_raw_obs_processing_ms: 204.35885873382787
  time_since_restore: 154989.31378602982
  time_this_iter_s: 422.5468752384186
  time_total_s: 499991.2064268589
  timers:
    learn_throughput: 749.041
    learn_time_ms: 2523.226
    load_throughput: 188003.305
    load_time_ms: 10.053
    sample_throughput: 2.531
    sample_time_ms: 746886.245
    update_time_ms: 26.94
  timestamp: 1633215493
  timesteps_since_restore: 0
  timesteps_total: 886410
  training_iteration: 469
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    469 |           499991 | 886410 |  5.12385 |              8.41662 |              2.40753 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 888300
  custom_metrics: {}
  date: 2021-10-02_16-03-33
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.243550727207804
  episode_reward_mean: 5.043251887261518
  episode_reward_min: 2.109287256768247
  episodes_this_iter: 270
  episodes_total: 126900
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.0890000339713879e-05
          entropy: 7.96439266204834
          entropy_coeff: 3.1093801226234064e-05
          kl: 0.011415034532546997
          model: {}
          policy_loss: -0.17380844056606293
          total_loss: -0.12237317115068436
          vf_explained_var: 0.9928010702133179
          vf_loss: 0.025678005069494247
    num_agent_steps_sampled: 888300
    num_agent_steps_trained: 888300
    num_steps_sampled: 888300
    num_steps_trained: 888300
  iterations_since_restore: 170
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.935135135135134
    ram_util_percent: 6.099999999999999
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10230731372209306
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1855.5739867414086
    mean_inference_ms: 2.4460435123174507
    mean_raw_obs_processing_ms: 204.3679018554825
  time_since_restore: 155309.86162638664
  time_this_iter_s: 320.5478403568268
  time_total_s: 500311.7542672157
  timers:
    learn_throughput: 749.41
    learn_time_ms: 2521.985
    load_throughput: 188073.333
    load_time_ms: 10.049
    sample_throughput: 2.813
    sample_time_ms: 671869.009
    update_time_ms: 26.975
  timestamp: 1633215813
  timesteps_since_restore: 0
  timesteps_total: 888300
  training_iteration: 470
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    470 |           500312 | 888300 |  5.04325 |              8.24355 |              2.10929 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 890190
  custom_metrics: {}
  date: 2021-10-02_16-10-37
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.620802035539249
  episode_reward_mean: 5.0893933482503
  episode_reward_min: 2.160876186412813
  episodes_this_iter: 270
  episodes_total: 127170
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.06999996205559e-05
          entropy: 7.847729206085205
          entropy_coeff: 3.0093999157543294e-05
          kl: 0.011236504651606083
          model: {}
          policy_loss: -0.15891262888908386
          total_loss: -0.11275710165500641
          vf_explained_var: 0.9942740201950073
          vf_loss: 0.020793553441762924
    num_agent_steps_sampled: 890190
    num_agent_steps_trained: 890190
    num_steps_sampled: 890190
    num_steps_trained: 890190
  iterations_since_restore: 171
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.876779661016954
    ram_util_percent: 6.1000000000000005
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10229237623634302
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1852.6097023438238
    mean_inference_ms: 2.4450363202538576
    mean_raw_obs_processing_ms: 204.35196924456622
  time_since_restore: 155733.58306717873
  time_this_iter_s: 423.72144079208374
  time_total_s: 500735.4757080078
  timers:
    learn_throughput: 748.275
    learn_time_ms: 2525.81
    load_throughput: 189019.559
    load_time_ms: 9.999
    sample_throughput: 2.839
    sample_time_ms: 665617.346
    update_time_ms: 27.047
  timestamp: 1633216237
  timesteps_since_restore: 0
  timesteps_total: 890190
  training_iteration: 471
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    471 |           500735 | 890190 |  5.08939 |               7.6208 |              2.16088 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 892080
  custom_metrics: {}
  date: 2021-10-02_16-15-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.865018978996835
  episode_reward_mean: 5.149223393454222
  episode_reward_min: 2.1670029526957126
  episodes_this_iter: 270
  episodes_total: 127440
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.0509999810892623e-05
          entropy: 7.766664981842041
          entropy_coeff: 2.9094200726831332e-05
          kl: 0.011724400334060192
          model: {}
          policy_loss: -0.18592512607574463
          total_loss: -0.13592056930065155
          vf_explained_var: 0.9935871362686157
          vf_loss: 0.023520881310105324
    num_agent_steps_sampled: 892080
    num_agent_steps_trained: 892080
    num_steps_sampled: 892080
    num_steps_trained: 892080
  iterations_since_restore: 172
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.88897849462365
    ram_util_percent: 6.1000000000000005
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10227063828860054
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1847.6062166315658
    mean_inference_ms: 2.4442542235797005
    mean_raw_obs_processing_ms: 204.3550411680536
  time_since_restore: 156000.89141774178
  time_this_iter_s: 267.3083505630493
  time_total_s: 501002.78405857086
  timers:
    learn_throughput: 747.38
    learn_time_ms: 2528.833
    load_throughput: 189141.778
    load_time_ms: 9.993
    sample_throughput: 2.934
    sample_time_ms: 644140.87
    update_time_ms: 27.18
  timestamp: 1633216504
  timesteps_since_restore: 0
  timesteps_total: 892080
  training_iteration: 472
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    472 |           501003 | 892080 |  5.14922 |              7.86502 |                2.167 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 893970
  custom_metrics: {}
  date: 2021-10-02_16-17-20
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.690667630268279
  episode_reward_mean: 5.148882101167614
  episode_reward_min: 2.132330621834118
  episodes_this_iter: 270
  episodes_total: 127710
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.0320000001229346e-05
          entropy: 7.841015815734863
          entropy_coeff: 2.8094400477129966e-05
          kl: 0.011619031429290771
          model: {}
          policy_loss: -0.16747629642486572
          total_loss: -0.11753739416599274
          vf_explained_var: 0.9936839938163757
          vf_loss: 0.023689579218626022
    num_agent_steps_sampled: 893970
    num_agent_steps_trained: 893970
    num_steps_sampled: 893970
    num_steps_trained: 893970
  iterations_since_restore: 173
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.34574468085106
    ram_util_percent: 6.1000000000000005
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10224745180695
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1841.857332208127
    mean_inference_ms: 2.443616056148524
    mean_raw_obs_processing_ms: 204.3434773798182
  time_since_restore: 156136.16509222984
  time_this_iter_s: 135.27367448806763
  time_total_s: 501138.05773305893
  timers:
    learn_throughput: 747.067
    learn_time_ms: 2529.894
    load_throughput: 189066.894
    load_time_ms: 9.996
    sample_throughput: 3.447
    sample_time_ms: 548326.5
    update_time_ms: 27.354
  timestamp: 1633216640
  timesteps_since_restore: 0
  timesteps_total: 893970
  training_iteration: 473
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    473 |           501138 | 893970 |  5.14888 |              7.69067 |              2.13233 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 895860
  custom_metrics: {}
  date: 2021-10-02_16-39-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.858971967180662
  episode_reward_mean: 5.1466091110904655
  episode_reward_min: 2.3599075939768484
  episodes_this_iter: 270
  episodes_total: 127980
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 1.0130000191566069e-05
          entropy: 7.743880271911621
          entropy_coeff: 2.70946002274286e-05
          kl: 0.011662071570754051
          model: {}
          policy_loss: -0.15408527851104736
          total_loss: -0.10288622230291367
          vf_explained_var: 0.9932910203933716
          vf_loss: 0.024841202422976494
    num_agent_steps_sampled: 895860
    num_agent_steps_trained: 895860
    num_steps_sampled: 895860
    num_steps_trained: 895860
  iterations_since_restore: 174
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.80473829201102
    ram_util_percent: 6.131349862258954
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10223490635412681
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1843.8877558823776
    mean_inference_ms: 2.4426317024372994
    mean_raw_obs_processing_ms: 204.3351050827227
  time_since_restore: 157440.20586395264
  time_this_iter_s: 1304.0407717227936
  time_total_s: 502442.0985047817
  timers:
    learn_throughput: 746.692
    learn_time_ms: 2531.165
    load_throughput: 189441.003
    load_time_ms: 9.977
    sample_throughput: 3.064
    sample_time_ms: 616812.231
    update_time_ms: 27.228
  timestamp: 1633217944
  timesteps_since_restore: 0
  timesteps_total: 895860
  training_iteration: 474
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.6/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    474 |           502442 | 895860 |  5.14661 |              7.85897 |              2.35991 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 897750
  custom_metrics: {}
  date: 2021-10-02_16-43-23
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.905552969451225
  episode_reward_mean: 5.180453617320878
  episode_reward_min: 2.2713346475460936
  episodes_this_iter: 270
  episodes_total: 128250
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.940000381902792e-06
          entropy: 7.657962799072266
          entropy_coeff: 2.6094799977727234e-05
          kl: 0.011548315174877644
          model: {}
          policy_loss: -0.18050529062747955
          total_loss: -0.13254974782466888
          vf_explained_var: 0.9940958619117737
          vf_loss: 0.021846862509846687
    num_agent_steps_sampled: 897750
    num_agent_steps_trained: 897750
    num_steps_sampled: 897750
    num_steps_trained: 897750
  iterations_since_restore: 175
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.505000000000003
    ram_util_percent: 6.1
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.102226604859009
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1838.64358391498
    mean_inference_ms: 2.4418542320864454
    mean_raw_obs_processing_ms: 204.33769590023098
  time_since_restore: 157698.98361754417
  time_this_iter_s: 258.7777535915375
  time_total_s: 502700.87625837326
  timers:
    learn_throughput: 745.096
    learn_time_ms: 2536.586
    load_throughput: 187280.219
    load_time_ms: 10.092
    sample_throughput: 3.794
    sample_time_ms: 498215.188
    update_time_ms: 27.378
  timestamp: 1633218203
  timesteps_since_restore: 0
  timesteps_total: 897750
  training_iteration: 475
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    475 |           502701 | 897750 |  5.18045 |              7.90555 |              2.27133 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 899640
  custom_metrics: {}
  date: 2021-10-02_16-49-43
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.59575280757282
  episode_reward_mean: 5.0542361001295575
  episode_reward_min: 2.121981177668184
  episodes_this_iter: 270
  episodes_total: 128520
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.749999662744813e-06
          entropy: 7.761352062225342
          entropy_coeff: 2.509499972802587e-05
          kl: 0.010657569393515587
          model: {}
          policy_loss: -0.16951225697994232
          total_loss: -0.12212107330560684
          vf_explained_var: 0.993503987789154
          vf_loss: 0.023306703194975853
    num_agent_steps_sampled: 899640
    num_agent_steps_trained: 899640
    num_steps_sampled: 899640
    num_steps_trained: 899640
  iterations_since_restore: 176
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.09943289224953
    ram_util_percent: 6.1000000000000005
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10221239949282908
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1836.5515220135696
    mean_inference_ms: 2.4410112199144964
    mean_raw_obs_processing_ms: 204.32500862303274
  time_since_restore: 158079.7413032055
  time_this_iter_s: 380.7576856613159
  time_total_s: 503081.6339440346
  timers:
    learn_throughput: 746.338
    learn_time_ms: 2532.366
    load_throughput: 184114.515
    load_time_ms: 10.265
    sample_throughput: 4.254
    sample_time_ms: 444278.873
    update_time_ms: 27.381
  timestamp: 1633218583
  timesteps_since_restore: 0
  timesteps_total: 899640
  training_iteration: 476
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    476 |           503082 | 899640 |  5.05424 |              7.59575 |              2.12198 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 901530
  custom_metrics: {}
  date: 2021-10-02_16-54-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.623961738903318
  episode_reward_mean: 5.126969088819005
  episode_reward_min: 2.7869754996771197
  episodes_this_iter: 270
  episodes_total: 128790
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.559999853081536e-06
          entropy: 7.7766218185424805
          entropy_coeff: 2.4095199478324503e-05
          kl: 0.011754900217056274
          model: {}
          policy_loss: -0.1897563487291336
          total_loss: -0.1417662799358368
          vf_explained_var: 0.9942346811294556
          vf_loss: 0.021398309618234634
    num_agent_steps_sampled: 901530
    num_agent_steps_trained: 901530
    num_steps_sampled: 901530
    num_steps_trained: 901530
  iterations_since_restore: 177
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.391327913279138
    ram_util_percent: 6.1000000000000005
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10221180415649697
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1832.1029256641978
    mean_inference_ms: 2.4403675025715503
    mean_raw_obs_processing_ms: 204.29943840882657
  time_since_restore: 158344.83165478706
  time_this_iter_s: 265.0903515815735
  time_total_s: 503346.72429561615
  timers:
    learn_throughput: 745.32
    learn_time_ms: 2535.823
    load_throughput: 184071.336
    load_time_ms: 10.268
    sample_throughput: 4.27
    sample_time_ms: 442573.688
    update_time_ms: 27.309
  timestamp: 1633218848
  timesteps_since_restore: 0
  timesteps_total: 901530
  training_iteration: 477
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 18.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    477 |           503347 | 901530 |  5.12697 |              7.62396 |              2.78698 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 903420
  custom_metrics: {}
  date: 2021-10-02_17-51-41
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.885870061732209
  episode_reward_mean: 5.044849359055209
  episode_reward_min: 1.989034666237726
  episodes_this_iter: 270
  episodes_total: 129060
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.370000043418258e-06
          entropy: 7.733990669250488
          entropy_coeff: 2.3095399228623137e-05
          kl: 0.011967012658715248
          model: {}
          policy_loss: -0.17924904823303223
          total_loss: -0.12885500490665436
          vf_explained_var: 0.9936065673828125
          vf_loss: 0.02331029810011387
    num_agent_steps_sampled: 903420
    num_agent_steps_trained: 903420
    num_steps_sampled: 903420
    num_steps_trained: 903420
  iterations_since_restore: 178
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.83307628178408
    ram_util_percent: 6.397957482284284
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10221078196036756
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1836.2179529891644
    mean_inference_ms: 2.4394272054211013
    mean_raw_obs_processing_ms: 204.283152348314
  time_since_restore: 161797.4967236519
  time_this_iter_s: 3452.6650688648224
  time_total_s: 506799.389364481
  timers:
    learn_throughput: 745.261
    learn_time_ms: 2536.024
    load_throughput: 181764.778
    load_time_ms: 10.398
    sample_throughput: 2.624
    sample_time_ms: 720367.197
    update_time_ms: 27.602
  timestamp: 1633222301
  timesteps_since_restore: 0
  timesteps_total: 903420
  training_iteration: 478
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    478 |           506799 | 903420 |  5.04485 |              7.88587 |              1.98903 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 905310
  custom_metrics: {}
  date: 2021-10-02_18-14-32
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.843359995215663
  episode_reward_mean: 5.065264638625071
  episode_reward_min: 2.0676528810972115
  episodes_this_iter: 270
  episodes_total: 129330
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 9.180000233754981e-06
          entropy: 7.653119087219238
          entropy_coeff: 2.2095600797911175e-05
          kl: 0.011066108010709286
          model: {}
          policy_loss: -0.16220273077487946
          total_loss: -0.11422931402921677
          vf_explained_var: 0.99368816614151
          vf_loss: 0.022932544350624084
    num_agent_steps_sampled: 905310
    num_agent_steps_trained: 905310
    num_steps_sampled: 905310
    num_steps_trained: 905310
  iterations_since_restore: 179
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.39690451206716
    ram_util_percent: 6.4
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10222376936546348
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1839.0884131686719
    mean_inference_ms: 2.438709699908038
    mean_raw_obs_processing_ms: 204.27205537055136
  time_since_restore: 163168.41489458084
  time_this_iter_s: 1370.918170928955
  time_total_s: 508170.3075354099
  timers:
    learn_throughput: 745.93
    learn_time_ms: 2533.749
    load_throughput: 181966.719
    load_time_ms: 10.387
    sample_throughput: 2.318
    sample_time_ms: 815206.806
    update_time_ms: 27.623
  timestamp: 1633223672
  timesteps_since_restore: 0
  timesteps_total: 905310
  training_iteration: 479
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    479 |           508170 | 905310 |  5.06526 |              7.84336 |              2.06765 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 907200
  custom_metrics: {}
  date: 2021-10-02_18-52-58
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.577665990016799
  episode_reward_mean: 5.058186517877481
  episode_reward_min: 2.0708400084271936
  episodes_this_iter: 270
  episodes_total: 129600
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.990000424091704e-06
          entropy: 7.682064533233643
          entropy_coeff: 2.109580054820981e-05
          kl: 0.01088166143745184
          model: {}
          policy_loss: -0.17709387838840485
          total_loss: -0.1313132494688034
          vf_explained_var: 0.994183361530304
          vf_loss: 0.021152889356017113
    num_agent_steps_sampled: 907200
    num_agent_steps_trained: 907200
    num_steps_sampled: 907200
    num_steps_trained: 907200
  iterations_since_restore: 180
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.955670103092785
    ram_util_percent: 6.399625117150889
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10222831251499807
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1842.9805437856899
    mean_inference_ms: 2.4383839765518975
    mean_raw_obs_processing_ms: 204.2804836184813
  time_since_restore: 165474.10625219345
  time_this_iter_s: 2305.69135761261
  time_total_s: 510475.99889302254
  timers:
    learn_throughput: 747.138
    learn_time_ms: 2529.655
    load_throughput: 181643.579
    load_time_ms: 10.405
    sample_throughput: 1.864
    sample_time_ms: 1013725.253
    update_time_ms: 27.605
  timestamp: 1633225978
  timesteps_since_restore: 0
  timesteps_total: 907200
  training_iteration: 480
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    480 |           510476 | 907200 |  5.05819 |              7.57767 |              2.07084 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 909090
  custom_metrics: {}
  date: 2021-10-02_18-56-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.431027295283707
  episode_reward_mean: 5.048494285102784
  episode_reward_min: 2.301177648477845
  episodes_this_iter: 270
  episodes_total: 129870
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.799999704933725e-06
          entropy: 7.729367733001709
          entropy_coeff: 2.0096000298508443e-05
          kl: 0.011448368430137634
          model: {}
          policy_loss: -0.178996279835701
          total_loss: -0.13092291355133057
          vf_explained_var: 0.9939225316047668
          vf_loss: 0.02214786782860756
    num_agent_steps_sampled: 909090
    num_agent_steps_trained: 909090
    num_steps_sampled: 909090
    num_steps_trained: 909090
  iterations_since_restore: 181
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.34271523178808
    ram_util_percent: 6.3999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10222763687413572
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1838.9676660658524
    mean_inference_ms: 2.4378210894420036
    mean_raw_obs_processing_ms: 204.27631518413907
  time_since_restore: 165692.00185465813
  time_this_iter_s: 217.8956024646759
  time_total_s: 510693.8944954872
  timers:
    learn_throughput: 749.789
    learn_time_ms: 2520.71
    load_throughput: 179854.581
    load_time_ms: 10.508
    sample_throughput: 1.903
    sample_time_ms: 993151.067
    update_time_ms: 27.492
  timestamp: 1633226196
  timesteps_since_restore: 0
  timesteps_total: 909090
  training_iteration: 481
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    481 |           510694 | 909090 |  5.04849 |              8.43103 |              2.30118 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 910980
  custom_metrics: {}
  date: 2021-10-02_19-05-00
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.054943263304068
  episode_reward_mean: 5.085832267054569
  episode_reward_min: 2.06833728510225
  episodes_this_iter: 270
  episodes_total: 130140
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.609999895270448e-06
          entropy: 7.619256019592285
          entropy_coeff: 1.9096200048807077e-05
          kl: 0.010567502118647099
          model: {}
          policy_loss: -0.1595391482114792
          total_loss: -0.11113613843917847
          vf_explained_var: 0.9933209419250488
          vf_loss: 0.024474412202835083
    num_agent_steps_sampled: 910980
    num_agent_steps_trained: 910980
    num_steps_sampled: 910980
    num_steps_trained: 910980
  iterations_since_restore: 182
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.009809663250365
    ram_util_percent: 6.39809663250366
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10223212619796626
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1838.7649307452177
    mean_inference_ms: 2.43690328333494
    mean_raw_obs_processing_ms: 204.261841747728
  time_since_restore: 166195.7564854622
  time_this_iter_s: 503.7546308040619
  time_total_s: 511197.6491262913
  timers:
    learn_throughput: 750.324
    learn_time_ms: 2518.91
    load_throughput: 179360.966
    load_time_ms: 10.537
    sample_throughput: 1.859
    sample_time_ms: 1016796.849
    update_time_ms: 27.558
  timestamp: 1633226700
  timesteps_since_restore: 0
  timesteps_total: 910980
  training_iteration: 482
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    482 |           511198 | 910980 |  5.08583 |              8.05494 |              2.06834 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 912870
  custom_metrics: {}
  date: 2021-10-02_19-10-04
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.192625671152001
  episode_reward_mean: 5.014553627394574
  episode_reward_min: 2.340041611813436
  episodes_this_iter: 270
  episodes_total: 130410
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.420000085607171e-06
          entropy: 7.670865058898926
          entropy_coeff: 1.809639979910571e-05
          kl: 0.01067398488521576
          model: {}
          policy_loss: -0.18053027987480164
          total_loss: -0.13445402681827545
          vf_explained_var: 0.9939275979995728
          vf_loss: 0.021898407489061356
    num_agent_steps_sampled: 912870
    num_agent_steps_trained: 912870
    num_steps_sampled: 912870
    num_steps_trained: 912870
  iterations_since_restore: 183
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 40.121985815602834
    ram_util_percent: 6.3999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10223614063754377
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1835.1640408401645
    mean_inference_ms: 2.436112498973839
    mean_raw_obs_processing_ms: 204.2534596006904
  time_since_restore: 166500.46424365044
  time_this_iter_s: 304.7077581882477
  time_total_s: 511502.3568844795
  timers:
    learn_throughput: 751.156
    learn_time_ms: 2516.122
    load_throughput: 179615.777
    load_time_ms: 10.522
    sample_throughput: 1.828
    sample_time_ms: 1033743.023
    update_time_ms: 27.54
  timestamp: 1633227004
  timesteps_since_restore: 0
  timesteps_total: 912870
  training_iteration: 483
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    483 |           511502 | 912870 |  5.01455 |              8.19263 |              2.34004 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 914760
  custom_metrics: {}
  date: 2021-10-02_19-17-58
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.427151135149215
  episode_reward_mean: 5.152986743617054
  episode_reward_min: 2.014535707228793
  episodes_this_iter: 270
  episodes_total: 130680
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.230000275943894e-06
          entropy: 7.547680854797363
          entropy_coeff: 1.7096599549404345e-05
          kl: 0.011349260807037354
          model: {}
          policy_loss: -0.19585084915161133
          total_loss: -0.14846982061862946
          vf_explained_var: 0.9943322539329529
          vf_loss: 0.021655017510056496
    num_agent_steps_sampled: 914760
    num_agent_steps_trained: 914760
    num_steps_sampled: 914760
    num_steps_trained: 914760
  iterations_since_restore: 184
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.97108066971081
    ram_util_percent: 6.399999999999999
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10224227158003087
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1832.033074106526
    mean_inference_ms: 2.435423606587795
    mean_raw_obs_processing_ms: 204.23603789437902
  time_since_restore: 166973.59040927887
  time_this_iter_s: 473.1261656284332
  time_total_s: 511975.48305010796
  timers:
    learn_throughput: 752.952
    learn_time_ms: 2510.121
    load_throughput: 179080.982
    load_time_ms: 10.554
    sample_throughput: 1.988
    sample_time_ms: 950657.131
    update_time_ms: 27.458
  timestamp: 1633227478
  timesteps_since_restore: 0
  timesteps_total: 914760
  training_iteration: 484
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    484 |           511975 | 914760 |  5.15299 |              8.42715 |              2.01454 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 916650
  custom_metrics: {}
  date: 2021-10-02_19-31-06
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.42184437652551
  episode_reward_mean: 5.112290631747228
  episode_reward_min: 2.04147093877788
  episodes_this_iter: 270
  episodes_total: 130950
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 8.039999556785915e-06
          entropy: 7.611337184906006
          entropy_coeff: 1.609679929970298e-05
          kl: 0.01038637850433588
          model: {}
          policy_loss: -0.16652081906795502
          total_loss: -0.1192597821354866
          vf_explained_var: 0.9935131669044495
          vf_loss: 0.023722080513834953
    num_agent_steps_sampled: 916650
    num_agent_steps_trained: 916650
    num_steps_sampled: 916650
    num_steps_trained: 916650
  iterations_since_restore: 185
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 42.31622607110301
    ram_util_percent: 6.374567000911576
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10225580471932061
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1832.5703559674384
    mean_inference_ms: 2.434630994961567
    mean_raw_obs_processing_ms: 204.4355621246827
  time_since_restore: 167761.92850732803
  time_this_iter_s: 788.3380980491638
  time_total_s: 512763.8211481571
  timers:
    learn_throughput: 751.324
    learn_time_ms: 2515.558
    load_throughput: 181104.105
    load_time_ms: 10.436
    sample_throughput: 1.883
    sample_time_ms: 1003607.418
    update_time_ms: 27.588
  timestamp: 1633228266
  timesteps_since_restore: 0
  timesteps_total: 916650
  training_iteration: 485
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    485 |           512764 | 916650 |  5.11229 |              8.42184 |              2.04147 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 918540
  custom_metrics: {}
  date: 2021-10-02_19-40-27
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.756084857786887
  episode_reward_mean: 5.080888773176588
  episode_reward_min: 2.294155925826653
  episodes_this_iter: 270
  episodes_total: 131220
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.849999747122638e-06
          entropy: 7.48311185836792
          entropy_coeff: 1.5096999959496316e-05
          kl: 0.01061504427343607
          model: {}
          policy_loss: -0.17792293429374695
          total_loss: -0.12972433865070343
          vf_explained_var: 0.9931268692016602
          vf_loss: 0.02412915788590908
    num_agent_steps_sampled: 918540
    num_agent_steps_trained: 918540
    num_steps_sampled: 918540
    num_steps_trained: 918540
  iterations_since_restore: 186
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.80847240051348
    ram_util_percent: 6.4
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10227114760885023
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1830.1423589929198
    mean_inference_ms: 2.433964618192838
    mean_raw_obs_processing_ms: 204.42713590779098
  time_since_restore: 168322.95671772957
  time_this_iter_s: 561.028210401535
  time_total_s: 513324.84935855865
  timers:
    learn_throughput: 750.817
    learn_time_ms: 2517.258
    load_throughput: 183522.041
    load_time_ms: 10.298
    sample_throughput: 1.85
    sample_time_ms: 1021632.738
    update_time_ms: 27.457
  timestamp: 1633228827
  timesteps_since_restore: 0
  timesteps_total: 918540
  training_iteration: 486
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    486 |           513325 | 918540 |  5.08089 |              7.75608 |              2.29416 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 920430
  custom_metrics: {}
  date: 2021-10-02_20-03-03
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.707161200887897
  episode_reward_mean: 5.10984465625295
  episode_reward_min: 2.466564186993941
  episodes_this_iter: 270
  episodes_total: 131490
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.65999993745936e-06
          entropy: 7.450272083282471
          entropy_coeff: 1.409719970979495e-05
          kl: 0.010239674709737301
          model: {}
          policy_loss: -0.1695222407579422
          total_loss: -0.12104902416467667
          vf_explained_var: 0.9929242134094238
          vf_loss: 0.025250962004065514
    num_agent_steps_sampled: 920430
    num_agent_steps_trained: 920430
    num_steps_sampled: 920430
    num_steps_trained: 920430
  iterations_since_restore: 187
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.581658692185
    ram_util_percent: 6.399202551834131
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10227350008384362
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1829.630601899199
    mean_inference_ms: 2.4333641124587944
    mean_raw_obs_processing_ms: 204.40999159822977
  time_since_restore: 169678.46449685097
  time_this_iter_s: 1355.507779121399
  time_total_s: 514680.35713768005
  timers:
    learn_throughput: 752.559
    learn_time_ms: 2511.432
    load_throughput: 182821.145
    load_time_ms: 10.338
    sample_throughput: 1.672
    sample_time_ms: 1130680.694
    update_time_ms: 27.664
  timestamp: 1633230183
  timesteps_since_restore: 0
  timesteps_total: 920430
  training_iteration: 487
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    487 |           514680 | 920430 |  5.10984 |              7.70716 |              2.46656 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 922320
  custom_metrics: {}
  date: 2021-10-02_20-10-12
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.34631754434083
  episode_reward_mean: 5.200682019489042
  episode_reward_min: 2.3852871932254818
  episodes_this_iter: 270
  episodes_total: 131760
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.470000127796084e-06
          entropy: 7.371840476989746
          entropy_coeff: 1.3097400369588286e-05
          kl: 0.010314597748219967
          model: {}
          policy_loss: -0.16693350672721863
          total_loss: -0.12034884840250015
          vf_explained_var: 0.9939436316490173
          vf_loss: 0.023183269426226616
    num_agent_steps_sampled: 922320
    num_agent_steps_trained: 922320
    num_steps_sampled: 922320
    num_steps_trained: 922320
  iterations_since_restore: 188
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.21862416107383
    ram_util_percent: 6.3999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10227753458026961
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1825.9727036980976
    mean_inference_ms: 2.4326006142176975
    mean_raw_obs_processing_ms: 204.3941188805334
  time_since_restore: 170107.9359047413
  time_this_iter_s: 429.4714078903198
  time_total_s: 515109.8285455704
  timers:
    learn_throughput: 751.789
    learn_time_ms: 2514.003
    load_throughput: 183740.258
    load_time_ms: 10.286
    sample_throughput: 2.282
    sample_time_ms: 828358.692
    update_time_ms: 27.732
  timestamp: 1633230612
  timesteps_since_restore: 0
  timesteps_total: 922320
  training_iteration: 488
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    488 |           515110 | 922320 |  5.20068 |              8.34632 |              2.38529 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 924210
  custom_metrics: {}
  date: 2021-10-02_20-27-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.847156852469412
  episode_reward_mean: 5.0898044160275155
  episode_reward_min: 2.0585416380086015
  episodes_this_iter: 270
  episodes_total: 132030
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.279999863385456e-06
          entropy: 7.42465353012085
          entropy_coeff: 1.209760011988692e-05
          kl: 0.010798283852636814
          model: {}
          policy_loss: -0.165329709649086
          total_loss: -0.11738667637109756
          vf_explained_var: 0.9937013387680054
          vf_loss: 0.02343301847577095
    num_agent_steps_sampled: 924210
    num_agent_steps_trained: 924210
    num_steps_sampled: 924210
    num_steps_trained: 924210
  iterations_since_restore: 189
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.99351276742581
    ram_util_percent: 6.389855072463768
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1022848630388823
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1829.367009372015
    mean_inference_ms: 2.431983578395092
    mean_raw_obs_processing_ms: 204.38499340262922
  time_since_restore: 171151.99485135078
  time_this_iter_s: 1044.058946609497
  time_total_s: 516153.88749217987
  timers:
    learn_throughput: 751.404
    learn_time_ms: 2515.291
    load_throughput: 183675.547
    load_time_ms: 10.29
    sample_throughput: 2.375
    sample_time_ms: 795671.192
    update_time_ms: 28.081
  timestamp: 1633231656
  timesteps_since_restore: 0
  timesteps_total: 924210
  training_iteration: 489
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    489 |           516154 | 924210 |   5.0898 |              7.84716 |              2.05854 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 926100
  custom_metrics: {}
  date: 2021-10-02_20-34-46
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.781742351599867
  episode_reward_mean: 5.144175152650855
  episode_reward_min: 2.3581416429722255
  episodes_this_iter: 270
  episodes_total: 132300
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 7.0900000537221786e-06
          entropy: 7.413764953613281
          entropy_coeff: 1.1097799870185554e-05
          kl: 0.010495921596884727
          model: {}
          policy_loss: -0.173392191529274
          total_loss: -0.12818102538585663
          vf_explained_var: 0.994199812412262
          vf_loss: 0.02138240449130535
    num_agent_steps_sampled: 926100
    num_agent_steps_trained: 926100
    num_steps_sampled: 926100
    num_steps_trained: 926100
  iterations_since_restore: 190
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.63926174496644
    ram_util_percent: 6.391107382550334
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10228884175822373
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1827.6336436225408
    mean_inference_ms: 2.43148337738289
    mean_raw_obs_processing_ms: 204.3767256094774
  time_since_restore: 171581.95591306686
  time_this_iter_s: 429.9610617160797
  time_total_s: 516583.84855389595
  timers:
    learn_throughput: 749.248
    learn_time_ms: 2522.53
    load_throughput: 183634.275
    load_time_ms: 10.292
    sample_throughput: 3.108
    sample_time_ms: 608091.034
    update_time_ms: 28.185
  timestamp: 1633232086
  timesteps_since_restore: 0
  timesteps_total: 926100
  training_iteration: 490
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    490 |           516584 | 926100 |  5.14418 |              7.78174 |              2.35814 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 927990
  custom_metrics: {}
  date: 2021-10-02_20-52-08
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.86103526547794
  episode_reward_mean: 5.120216768707055
  episode_reward_min: 2.264007105789223
  episodes_this_iter: 270
  episodes_total: 132570
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.8999997893115506e-06
          entropy: 7.392616271972656
          entropy_coeff: 1.0097999620484188e-05
          kl: 0.010262307710945606
          model: {}
          policy_loss: -0.17207182943820953
          total_loss: -0.12425822764635086
          vf_explained_var: 0.9934977889060974
          vf_loss: 0.024509426206350327
    num_agent_steps_sampled: 927990
    num_agent_steps_trained: 927990
    num_steps_sampled: 927990
    num_steps_trained: 927990
  iterations_since_restore: 191
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.71085753803596
    ram_util_percent: 6.394260027662516
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10229186245874371
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1827.289223540568
    mean_inference_ms: 2.4308152844147437
    mean_raw_obs_processing_ms: 204.3620682667184
  time_since_restore: 172623.8602836132
  time_this_iter_s: 1041.904370546341
  time_total_s: 517625.7529244423
  timers:
    learn_throughput: 750.208
    learn_time_ms: 2519.3
    load_throughput: 184416.908
    load_time_ms: 10.249
    sample_throughput: 2.737
    sample_time_ms: 690494.672
    update_time_ms: 28.867
  timestamp: 1633233128
  timesteps_since_restore: 0
  timesteps_total: 927990
  training_iteration: 491
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    491 |           517626 | 927990 |  5.12022 |              7.86104 |              2.26401 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 929880
  custom_metrics: {}
  date: 2021-10-02_20-56-19
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.8738573418389715
  episode_reward_mean: 5.0541203689242105
  episode_reward_min: 2.1451171263578854
  episodes_this_iter: 270
  episodes_total: 132840
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.7099999796482734e-06
          entropy: 7.387782096862793
          entropy_coeff: 9.098200280277524e-06
          kl: 0.009608529508113861
          model: {}
          policy_loss: -0.1708315759897232
          total_loss: -0.12524113059043884
          vf_explained_var: 0.9933362603187561
          vf_loss: 0.023768197745084763
    num_agent_steps_sampled: 929880
    num_agent_steps_trained: 929880
    num_steps_sampled: 929880
    num_steps_trained: 929880
  iterations_since_restore: 192
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 41.389367816091955
    ram_util_percent: 6.3999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10229936636537025
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1823.111894132907
    mean_inference_ms: 2.4302659441206034
    mean_raw_obs_processing_ms: 204.3487875415243
  time_since_restore: 172874.27081108093
  time_this_iter_s: 250.41052746772766
  time_total_s: 517876.16345191
  timers:
    learn_throughput: 751.527
    learn_time_ms: 2514.879
    load_throughput: 183706.619
    load_time_ms: 10.288
    sample_throughput: 2.841
    sample_time_ms: 665164.921
    update_time_ms: 29.128
  timestamp: 1633233379
  timesteps_since_restore: 0
  timesteps_total: 929880
  training_iteration: 492
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    492 |           517876 | 929880 |  5.05412 |              7.87386 |              2.14512 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 931770
  custom_metrics: {}
  date: 2021-10-02_21-10-36
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.868501262158399
  episode_reward_mean: 5.178112949664151
  episode_reward_min: 2.1375474762512496
  episodes_this_iter: 270
  episodes_total: 133110
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.520000169984996e-06
          entropy: 7.284407615661621
          entropy_coeff: 8.098400030576158e-06
          kl: 0.01056339405477047
          model: {}
          policy_loss: -0.16962464153766632
          total_loss: -0.12434474378824234
          vf_explained_var: 0.9942874312400818
          vf_loss: 0.021274136379361153
    num_agent_steps_sampled: 931770
    num_agent_steps_trained: 931770
    num_steps_sampled: 931770
    num_steps_trained: 931770
  iterations_since_restore: 193
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.149075630252106
    ram_util_percent: 6.4
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10230894931986559
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1821.4621938651424
    mean_inference_ms: 2.4296552087792156
    mean_raw_obs_processing_ms: 204.3338643052592
  time_since_restore: 173731.99351906776
  time_this_iter_s: 857.7227079868317
  time_total_s: 518733.88615989685
  timers:
    learn_throughput: 750.094
    learn_time_ms: 2519.686
    load_throughput: 183669.589
    load_time_ms: 10.29
    sample_throughput: 2.623
    sample_time_ms: 720461.313
    update_time_ms: 29.319
  timestamp: 1633234236
  timesteps_since_restore: 0
  timesteps_total: 931770
  training_iteration: 493
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    493 |           518734 | 931770 |  5.17811 |               7.8685 |              2.13755 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 933660
  custom_metrics: {}
  date: 2021-10-02_21-20-14
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.490519018094407
  episode_reward_mean: 5.106868402492422
  episode_reward_min: 2.324122694353881
  episodes_this_iter: 270
  episodes_total: 133380
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.329999905574368e-06
          entropy: 7.384463310241699
          entropy_coeff: 7.0985997808747925e-06
          kl: 0.010068769566714764
          model: {}
          policy_loss: -0.16036958992481232
          total_loss: -0.11520399898290634
          vf_explained_var: 0.9940052032470703
          vf_loss: 0.02228006348013878
    num_agent_steps_sampled: 933660
    num_agent_steps_trained: 933660
    num_steps_sampled: 933660
    num_steps_trained: 933660
  iterations_since_restore: 194
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.813607990012486
    ram_util_percent: 6.3999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10231301832583248
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1821.6496871080926
    mean_inference_ms: 2.428908873222525
    mean_raw_obs_processing_ms: 204.31506835422593
  time_since_restore: 174309.31296014786
  time_this_iter_s: 577.3194410800934
  time_total_s: 519311.20560097694
  timers:
    learn_throughput: 750.585
    learn_time_ms: 2518.036
    load_throughput: 183947.469
    load_time_ms: 10.275
    sample_throughput: 2.586
    sample_time_ms: 730882.29
    update_time_ms: 29.467
  timestamp: 1633234814
  timesteps_since_restore: 0
  timesteps_total: 933660
  training_iteration: 494
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    494 |           519311 | 933660 |  5.10687 |              8.49052 |              2.32412 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 935550
  custom_metrics: {}
  date: 2021-10-02_21-36-26
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 8.494081162615165
  episode_reward_mean: 5.127476569107929
  episode_reward_min: 2.28523146841137
  episodes_this_iter: 270
  episodes_total: 133650
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 6.140000095911091e-06
          entropy: 7.201501369476318
          entropy_coeff: 6.0987999859207775e-06
          kl: 0.010329232551157475
          model: {}
          policy_loss: -0.1622520238161087
          total_loss: -0.11634434014558792
          vf_explained_var: 0.9938055872917175
          vf_loss: 0.022420315071940422
    num_agent_steps_sampled: 935550
    num_agent_steps_trained: 935550
    num_steps_sampled: 935550
    num_steps_trained: 935550
  iterations_since_restore: 195
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.788724035608304
    ram_util_percent: 6.3999999999999995
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10235009512294792
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1820.7130305381024
    mean_inference_ms: 2.428329465755818
    mean_raw_obs_processing_ms: 204.2987552899008
  time_since_restore: 175281.46561217308
  time_this_iter_s: 972.1526520252228
  time_total_s: 520283.35825300217
  timers:
    learn_throughput: 754.205
    learn_time_ms: 2505.949
    load_throughput: 183017.836
    load_time_ms: 10.327
    sample_throughput: 2.522
    sample_time_ms: 749275.988
    update_time_ms: 29.61
  timestamp: 1633235786
  timesteps_since_restore: 0
  timesteps_total: 935550
  training_iteration: 495
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.8/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    495 |           520283 | 935550 |  5.12748 |              8.49408 |              2.28523 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 937440
  custom_metrics: {}
  date: 2021-10-02_21-45-09
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.895553534538842
  episode_reward_mean: 5.096378750038206
  episode_reward_min: 2.7795329446239636
  episodes_this_iter: 270
  episodes_total: 133920
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.949999831500463e-06
          entropy: 7.132102966308594
          entropy_coeff: 5.099000190966763e-06
          kl: 0.009771202690899372
          model: {}
          policy_loss: -0.14590319991111755
          total_loss: -0.10082320123910904
          vf_explained_var: 0.9937032461166382
          vf_loss: 0.022856341674923897
    num_agent_steps_sampled: 937440
    num_agent_steps_trained: 937440
    num_steps_sampled: 937440
    num_steps_trained: 937440
  iterations_since_restore: 196
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.37438016528926
    ram_util_percent: 6.36625344352617
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10237782701004502
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1818.0876859009836
    mean_inference_ms: 2.4276168586222946
    mean_raw_obs_processing_ms: 204.28717830809535
  time_since_restore: 175804.6359755993
  time_this_iter_s: 523.1703634262085
  time_total_s: 520806.5286164284
  timers:
    learn_throughput: 754.713
    learn_time_ms: 2504.264
    load_throughput: 183628.32
    load_time_ms: 10.293
    sample_throughput: 2.535
    sample_time_ms: 745492.04
    update_time_ms: 29.838
  timestamp: 1633236309
  timesteps_since_restore: 0
  timesteps_total: 937440
  training_iteration: 496
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    496 |           520807 | 937440 |  5.09638 |              7.89555 |              2.77953 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 939330
  custom_metrics: {}
  date: 2021-10-02_22-02-28
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.816989685234451
  episode_reward_mean: 5.13798007750626
  episode_reward_min: 2.134731813480715
  episodes_this_iter: 270
  episodes_total: 134190
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.760000021837186e-06
          entropy: 7.053107738494873
          entropy_coeff: 4.099199941265397e-06
          kl: 0.00950514804571867
          model: {}
          policy_loss: -0.1764974743127823
          total_loss: -0.13248218595981598
          vf_explained_var: 0.9941110610961914
          vf_loss: 0.022390305995941162
    num_agent_steps_sampled: 939330
    num_agent_steps_trained: 939330
    num_steps_sampled: 939330
    num_steps_trained: 939330
  iterations_since_restore: 197
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.83134535367545
    ram_util_percent: 6.3945214979195555
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10237756517403186
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1816.1880853635619
    mean_inference_ms: 2.4269951579986615
    mean_raw_obs_processing_ms: 204.27131144328632
  time_since_restore: 176843.59563064575
  time_this_iter_s: 1038.959655046463
  time_total_s: 521845.48827147484
  timers:
    learn_throughput: 752.417
    learn_time_ms: 2511.905
    load_throughput: 183065.595
    load_time_ms: 10.324
    sample_throughput: 2.648
    sample_time_ms: 713829.573
    update_time_ms: 29.812
  timestamp: 1633237348
  timesteps_since_restore: 0
  timesteps_total: 939330
  training_iteration: 497
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    497 |           521845 | 939330 |  5.13798 |              7.81699 |              2.13473 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 941220
  custom_metrics: {}
  date: 2021-10-02_22-45-21
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.677011798979926
  episode_reward_mean: 5.087228260177513
  episode_reward_min: 2.0695092635897643
  episodes_this_iter: 270
  episodes_total: 134460
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.570000212173909e-06
          entropy: 7.131440162658691
          entropy_coeff: 3.0993999189377064e-06
          kl: 0.00953635387122631
          model: {}
          policy_loss: -0.14561842381954193
          total_loss: -0.10011275112628937
          vf_explained_var: 0.9934661984443665
          vf_loss: 0.023802785202860832
    num_agent_steps_sampled: 941220
    num_agent_steps_trained: 941220
    num_steps_sampled: 941220
    num_steps_trained: 941220
  iterations_since_restore: 198
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 37.95163819658359
    ram_util_percent: 6.399523942873146
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10239414504434445
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1819.6132579436166
    mean_inference_ms: 2.426291926591684
    mean_raw_obs_processing_ms: 204.26469345501394
  time_since_restore: 179415.96301293373
  time_this_iter_s: 2572.367382287979
  time_total_s: 524417.8556537628
  timers:
    learn_throughput: 753.466
    learn_time_ms: 2508.408
    load_throughput: 183623.215
    load_time_ms: 10.293
    sample_throughput: 2.036
    sample_time_ms: 928122.485
    update_time_ms: 29.906
  timestamp: 1633239921
  timesteps_since_restore: 0
  timesteps_total: 941220
  training_iteration: 498
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    498 |           524418 | 941220 |  5.08723 |              7.67701 |              2.06951 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 943110
  custom_metrics: {}
  date: 2021-10-02_23-04-35
  done: false
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.741695441778017
  episode_reward_mean: 5.1542486334258335
  episode_reward_min: 2.1006357807278624
  episodes_this_iter: 270
  episodes_total: 134730
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.379999947763281e-06
          entropy: 7.179958820343018
          entropy_coeff: 2.099599896610016e-06
          kl: 0.00906536728143692
          model: {}
          policy_loss: -0.15404760837554932
          total_loss: -0.11078471690416336
          vf_explained_var: 0.9939841032028198
          vf_loss: 0.02262594923377037
    num_agent_steps_sampled: 943110
    num_agent_steps_trained: 943110
    num_steps_sampled: 943110
    num_steps_trained: 943110
  iterations_since_restore: 199
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.34529008109794
    ram_util_percent: 6.4
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10238862671020915
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1817.8058809789795
    mean_inference_ms: 2.425575085887988
    mean_raw_obs_processing_ms: 204.2542038585633
  time_since_restore: 180569.87495422363
  time_this_iter_s: 1153.9119412899017
  time_total_s: 525571.7675950527
  timers:
    learn_throughput: 754.314
    learn_time_ms: 2505.589
    load_throughput: 183516.943
    load_time_ms: 10.299
    sample_throughput: 2.013
    sample_time_ms: 939110.471
    update_time_ms: 29.765
  timestamp: 1633241075
  timesteps_since_restore: 0
  timesteps_total: 943110
  training_iteration: 499
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    499 |           525572 | 943110 |  5.15425 |               7.7417 |              2.10064 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_ReservoirEnv_20cd0_00000:
  agent_timesteps_total: 945000
  custom_metrics: {}
  date: 2021-10-02_23-11-15
  done: true
  episode_len_mean: 7.0
  episode_media: {}
  episode_reward_max: 7.79473731487093
  episode_reward_mean: 5.115624609384859
  episode_reward_min: 2.3434291867093258
  episodes_this_iter: 270
  episodes_total: 135000
  experiment_id: 9979c3e90488473a9c13a3f70587ec60
  hostname: sh03-05n57.int
  info:
    learner:
      default_policy:
        learner_stats:
          cur_kl_coeff: 2.278125047683716
          cur_lr: 5.190000138100004e-06
          entropy: 7.137611389160156
          entropy_coeff: 1.0997999879691633e-06
          kl: 0.009590870700776577
          model: {}
          policy_loss: -0.14548376202583313
          total_loss: -0.09980224072933197
          vf_explained_var: 0.9934075474739075
          vf_loss: 0.023840148001909256
    num_agent_steps_sampled: 945000
    num_agent_steps_trained: 945000
    num_steps_sampled: 945000
    num_steps_trained: 945000
  iterations_since_restore: 200
  node_ip: 10.19.5.57
  num_healthy_workers: 135
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.42046678635547
    ram_util_percent: 6.396768402154397
  pid: 2649
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10237960556163235
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1816.3800654723482
    mean_inference_ms: 2.424880974585344
    mean_raw_obs_processing_ms: 204.25050864463736
  time_since_restore: 180970.56631612778
  time_this_iter_s: 400.6913619041443
  time_total_s: 525972.4589569569
  timers:
    learn_throughput: 757.056
    learn_time_ms: 2496.512
    load_throughput: 182954.9
    load_time_ms: 10.33
    sample_throughput: 2.019
    sample_time_ms: 936192.457
    update_time_ms: 30.105
  timestamp: 1633241475
  timesteps_since_restore: 0
  timesteps_total: 945000
  training_iteration: 500
  trial_id: 20cd0_00000
  
== Status ==
Memory usage on this node: 17.7/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 140.0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 RUNNING)
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | RUNNING  | 10.19.5.57:2649 |    500 |           525972 | 945000 |  5.11562 |              7.79474 |              2.34343 |                  7 |
+------------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 10.5/251.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/140 CPUs, 0/0 GPUs, 0.0/1194.33 GiB heap, 0.0/515.85 GiB objects
Result logdir: /scratch/users/nyusuf/logs/PPO
Number of trials: 1/1 (1 TERMINATED)
+------------------------------+------------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                   | status     | loc   |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|------------------------------+------------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_ReservoirEnv_20cd0_00000 | TERMINATED |       |    500 |           525972 | 945000 |  5.11562 |              7.79474 |              2.34343 |                  7 |
+------------------------------+------------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


